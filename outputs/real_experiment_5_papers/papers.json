[
  {
    "entry_id": "http://arxiv.org/abs/2401.17464v2",
    "updated": "2024-02-26T20:26:40+00:00",
    "published": "2024-01-30T21:53:30+00:00",
    "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
    "authors": "['Silin Gao' 'Jane Dwivedi-Yu' 'Ping Yu' 'Xiaoqing Ellen Tan'\n 'Ramakanth Pasunuru' 'Olga Golovneva' 'Koustuv Sinha' 'Asli Celikyilmaz'\n 'Antoine Bosselut' 'Tianlu Wang']",
    "summary": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL']",
    "links": "['http://arxiv.org/abs/2401.17464v2' 'http://arxiv.org/pdf/2401.17464v2']",
    "pdf_url": "http://arxiv.org/pdf/2401.17464v2",
    "arxiv_id": "2401.17464",
    "row_id": 37433,
    "year": 2024,
    "processed_text": "Efficient Tool Use with Chain-of-Abstraction Reasoning To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.",
    "bm25_score": 23.279031840789138
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.01908v1",
    "updated": "2025-02-28T21:30:28+00:00",
    "published": "2025-02-28T21:30:28+00:00",
    "title": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning",
    "authors": "['Jiawei Zhang' 'Shuang Yang' 'Bo Li']",
    "summary": "Large Language Model (LLM) agents equipped with external tools have become\nincreasingly powerful for handling complex tasks such as web shopping,\nautomated email replies, and financial trading. However, these advancements\nalso amplify the risks of adversarial attacks, particularly when LLM agents can\naccess sensitive external functionalities. Moreover, because LLM agents engage\nin extensive reasoning or planning before executing final actions, manipulating\nthem into performing targeted malicious actions or invoking specific tools\nremains a significant challenge. Consequently, directly embedding adversarial\nstrings in malicious instructions or injecting malicious prompts into tool\ninteractions has become less effective against modern LLM agents. In this work,\nwe present UDora, a unified red teaming framework designed for LLM Agents that\ndynamically leverages the agent's own reasoning processes to compel it toward\nmalicious behavior. Specifically, UDora first samples the model's reasoning for\nthe given task, then automatically identifies multiple optimal positions within\nthese reasoning traces to insert targeted perturbations. Subsequently, it uses\nthe modified reasoning as the objective to optimize the adversarial strings. By\niteratively applying this process, the LLM agent will then be induced to\nundertake designated malicious actions or to invoke specific malicious tools.\nOur approach demonstrates superior effectiveness compared to existing methods\nacross three LLM agent datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": "['cs.CR' 'cs.AI' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2503.01908v1' 'http://arxiv.org/pdf/2503.01908v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.01908v1",
    "arxiv_id": "2503.01908",
    "row_id": 83930,
    "year": 2025,
    "processed_text": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning Large Language Model (LLM) agents equipped with external tools have become\nincreasingly powerful for handling complex tasks such as web shopping,\nautomated email replies, and financial trading. However, these advancements\nalso amplify the risks of adversarial attacks, particularly when LLM agents can\naccess sensitive external functionalities. Moreover, because LLM agents engage\nin extensive reasoning or planning before executing final actions, manipulating\nthem into performing targeted malicious actions or invoking specific tools\nremains a significant challenge. Consequently, directly embedding adversarial\nstrings in malicious instructions or injecting malicious prompts into tool\ninteractions has become less effective against modern LLM agents. In this work,\nwe present UDora, a unified red teaming framework designed for LLM Agents that\ndynamically leverages the agent's own reasoning processes to compel it toward\nmalicious behavior. Specifically, UDora first samples the model's reasoning for\nthe given task, then automatically identifies multiple optimal positions within\nthese reasoning traces to insert targeted perturbations. Subsequently, it uses\nthe modified reasoning as the objective to optimize the adversarial strings. By\niteratively applying this process, the LLM agent will then be induced to\nundertake designated malicious actions or to invoke specific malicious tools.\nOur approach demonstrates superior effectiveness compared to existing methods\nacross three LLM agent datasets.",
    "bm25_score": 21.5572072255851
  },
  {
    "entry_id": "http://arxiv.org/abs/2502.19500v1",
    "updated": "2025-02-26T19:04:26+00:00",
    "published": "2025-02-26T19:04:26+00:00",
    "title": "Conversational Planning for Personal Plans",
    "authors": "['Konstantina Christakopoulou' 'Iris Qu' 'John Canny' 'Andrew Goodridge'\n 'Cj Adams' 'Minmin Chen' 'Maja Matari\u0107']",
    "summary": "The language generation and reasoning capabilities of large language models\n(LLMs) have enabled conversational systems with impressive performance in a\nvariety of tasks, from code generation, to composing essays, to passing STEM\nand legal exams, to a new paradigm for knowledge search. Besides those\nshort-term use applications, LLMs are increasingly used to help with real-life\ngoals or tasks that take a long time to complete, involving multiple sessions\nacross days, weeks, months, or even years. Thus to enable conversational\nsystems for long term interactions and tasks, we need language-based agents\nthat can plan for long horizons. Traditionally, such capabilities were\naddressed by reinforcement learning agents with hierarchical planning\ncapabilities. In this work, we explore a novel architecture where the LLM acts\nas the meta-controller deciding the agent's next macro-action, and tool use\naugmented LLM-based option policies execute the selected macro-action. We\ninstantiate this framework for a specific set of macro-actions enabling\nadaptive planning for users' personal plans through conversation and follow-up\nquestions collecting user feedback. We show how this paradigm can be applicable\nin scenarios ranging from tutoring for academic and non-academic tasks to\nconversational coaching for personal health plans.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.HC' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2502.19500v1' 'http://arxiv.org/pdf/2502.19500v1']",
    "pdf_url": "http://arxiv.org/pdf/2502.19500v1",
    "arxiv_id": "2502.19500",
    "row_id": 10786,
    "year": 2025,
    "processed_text": "Conversational Planning for Personal Plans The language generation and reasoning capabilities of large language models\n(LLMs) have enabled conversational systems with impressive performance in a\nvariety of tasks, from code generation, to composing essays, to passing STEM\nand legal exams, to a new paradigm for knowledge search. Besides those\nshort-term use applications, LLMs are increasingly used to help with real-life\ngoals or tasks that take a long time to complete, involving multiple sessions\nacross days, weeks, months, or even years. Thus to enable conversational\nsystems for long term interactions and tasks, we need language-based agents\nthat can plan for long horizons. Traditionally, such capabilities were\naddressed by reinforcement learning agents with hierarchical planning\ncapabilities. In this work, we explore a novel architecture where the LLM acts\nas the meta-controller deciding the agent's next macro-action, and tool use\naugmented LLM-based option policies execute the selected macro-action. We\ninstantiate this framework for a specific set of macro-actions enabling\nadaptive planning for users' personal plans through conversation and follow-up\nquestions collecting user feedback. We show how this paradigm can be applicable\nin scenarios ranging from tutoring for academic and non-academic tasks to\nconversational coaching for personal health plans.",
    "bm25_score": 20.66781025897174
  },
  {
    "entry_id": "http://arxiv.org/abs/2410.20007v1",
    "updated": "2024-10-25T23:32:48+00:00",
    "published": "2024-10-25T23:32:48+00:00",
    "title": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models",
    "authors": "['Danqing Wang' 'Zhuorui Ye' 'Fei Fang' 'Lei Li']",
    "summary": "Enhancing the reasoning capabilities of large language models (LLMs) is\ncrucial for enabling them to tackle complex, multi-step problems. Multi-agent\nframeworks have shown great potential in enhancing LLMs' reasoning\ncapabilities. However, the lack of effective cooperation between LLM agents\nhinders their performance, especially for multi-step reasoning tasks. This\npaper proposes a novel cooperative multi-agent reasoning framework (CoPlanner)\nby separating reasoning steps and assigning distinct duties to different\nagents. CoPlanner consists of two LLM agents: a planning agent and a reasoning\nagent. The planning agent provides high-level strategic hints, while the\nreasoning agent follows these hints and infers answers. By training the\nplanning agent's policy through the interactive reasoning process via Proximal\nPolicy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the\nprevious best method by 9.94\\% on LogiQA and 3.09\\% on BBH. Our results\ndemonstrate that the guidance from the planning agent and the effective\ncooperation between the agents contribute to the superior performance of\nCoPlanner in tackling multi-step reasoning problems.",
    "comment": "Working in progress",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL']",
    "links": "['http://arxiv.org/abs/2410.20007v1' 'http://arxiv.org/pdf/2410.20007v1']",
    "pdf_url": "http://arxiv.org/pdf/2410.20007v1",
    "arxiv_id": "2410.20007",
    "row_id": 7378,
    "year": 2024,
    "processed_text": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models Enhancing the reasoning capabilities of large language models (LLMs) is\ncrucial for enabling them to tackle complex, multi-step problems. Multi-agent\nframeworks have shown great potential in enhancing LLMs' reasoning\ncapabilities. However, the lack of effective cooperation between LLM agents\nhinders their performance, especially for multi-step reasoning tasks. This\npaper proposes a novel cooperative multi-agent reasoning framework (CoPlanner)\nby separating reasoning steps and assigning distinct duties to different\nagents. CoPlanner consists of two LLM agents: a planning agent and a reasoning\nagent. The planning agent provides high-level strategic hints, while the\nreasoning agent follows these hints and infers answers. By training the\nplanning agent's policy through the interactive reasoning process via Proximal\nPolicy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the\nprevious best method by 9.94\\% on LogiQA and 3.09\\% on BBH. Our results\ndemonstrate that the guidance from the planning agent and the effective\ncooperation between the agents contribute to the superior performance of\nCoPlanner in tackling multi-step reasoning problems.",
    "bm25_score": 20.548050306812648
  },
  {
    "entry_id": "http://arxiv.org/abs/2308.06391v1",
    "updated": "2023-08-11T21:17:13+00:00",
    "published": "2023-08-11T21:17:13+00:00",
    "title": "Dynamic Planning with a LLM",
    "authors": "['Gautier Dagan' 'Frank Keller' 'Alex Lascarides']",
    "summary": "While Large Language Models (LLMs) can solve many NLP tasks in zero-shot\nsettings, applications involving embodied agents remain problematic. In\nparticular, complex plans that require multi-step reasoning become difficult\nand too costly as the context window grows. Planning requires understanding the\nlikely effects of one's actions and identifying whether the current environment\nsatisfies the goal state. While symbolic planners find optimal solutions\nquickly, they require a complete and accurate representation of the planning\nproblem, severely limiting their use in practical scenarios. In contrast,\nmodern LLMs cope with noisy observations and high levels of uncertainty when\nreasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a\nneuro-symbolic framework where an LLM works hand-in-hand with a traditional\nplanner to solve an embodied task. Given action-descriptions, LLM-DP solves\nAlfworld faster and more efficiently than a naive LLM ReAct baseline.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.RO']",
    "links": "['http://arxiv.org/abs/2308.06391v1' 'http://arxiv.org/pdf/2308.06391v1']",
    "pdf_url": "http://arxiv.org/pdf/2308.06391v1",
    "arxiv_id": "2308.06391",
    "row_id": 42951,
    "year": 2023,
    "processed_text": "Dynamic Planning with a LLM While Large Language Models (LLMs) can solve many NLP tasks in zero-shot\nsettings, applications involving embodied agents remain problematic. In\nparticular, complex plans that require multi-step reasoning become difficult\nand too costly as the context window grows. Planning requires understanding the\nlikely effects of one's actions and identifying whether the current environment\nsatisfies the goal state. While symbolic planners find optimal solutions\nquickly, they require a complete and accurate representation of the planning\nproblem, severely limiting their use in practical scenarios. In contrast,\nmodern LLMs cope with noisy observations and high levels of uncertainty when\nreasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a\nneuro-symbolic framework where an LLM works hand-in-hand with a traditional\nplanner to solve an embodied task. Given action-descriptions, LLM-DP solves\nAlfworld faster and more efficiently than a naive LLM ReAct baseline.",
    "bm25_score": 20.12675019785235
  }
]