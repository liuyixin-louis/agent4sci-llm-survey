[
  {
    "entry_id": "http://arxiv.org/abs/2401.17464v2",
    "updated": "2024-02-26T20:26:40+00:00",
    "published": "2024-01-30T21:53:30+00:00",
    "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
    "authors": "['Silin Gao' 'Jane Dwivedi-Yu' 'Ping Yu' 'Xiaoqing Ellen Tan'\n 'Ramakanth Pasunuru' 'Olga Golovneva' 'Koustuv Sinha' 'Asli Celikyilmaz'\n 'Antoine Bosselut' 'Tianlu Wang']",
    "summary": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL']",
    "links": "['http://arxiv.org/abs/2401.17464v2' 'http://arxiv.org/pdf/2401.17464v2']",
    "pdf_url": "http://arxiv.org/pdf/2401.17464v2",
    "arxiv_id": "2401.17464",
    "row_id": 37433,
    "year": 2024,
    "processed_text": "Efficient Tool Use with Chain-of-Abstraction Reasoning To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.",
    "bm25_score": 23.279031840789138
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.01908v1",
    "updated": "2025-02-28T21:30:28+00:00",
    "published": "2025-02-28T21:30:28+00:00",
    "title": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning",
    "authors": "['Jiawei Zhang' 'Shuang Yang' 'Bo Li']",
    "summary": "Large Language Model (LLM) agents equipped with external tools have become\nincreasingly powerful for handling complex tasks such as web shopping,\nautomated email replies, and financial trading. However, these advancements\nalso amplify the risks of adversarial attacks, particularly when LLM agents can\naccess sensitive external functionalities. Moreover, because LLM agents engage\nin extensive reasoning or planning before executing final actions, manipulating\nthem into performing targeted malicious actions or invoking specific tools\nremains a significant challenge. Consequently, directly embedding adversarial\nstrings in malicious instructions or injecting malicious prompts into tool\ninteractions has become less effective against modern LLM agents. In this work,\nwe present UDora, a unified red teaming framework designed for LLM Agents that\ndynamically leverages the agent's own reasoning processes to compel it toward\nmalicious behavior. Specifically, UDora first samples the model's reasoning for\nthe given task, then automatically identifies multiple optimal positions within\nthese reasoning traces to insert targeted perturbations. Subsequently, it uses\nthe modified reasoning as the objective to optimize the adversarial strings. By\niteratively applying this process, the LLM agent will then be induced to\nundertake designated malicious actions or to invoke specific malicious tools.\nOur approach demonstrates superior effectiveness compared to existing methods\nacross three LLM agent datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": "['cs.CR' 'cs.AI' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2503.01908v1' 'http://arxiv.org/pdf/2503.01908v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.01908v1",
    "arxiv_id": "2503.01908",
    "row_id": 83930,
    "year": 2025,
    "processed_text": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning Large Language Model (LLM) agents equipped with external tools have become\nincreasingly powerful for handling complex tasks such as web shopping,\nautomated email replies, and financial trading. However, these advancements\nalso amplify the risks of adversarial attacks, particularly when LLM agents can\naccess sensitive external functionalities. Moreover, because LLM agents engage\nin extensive reasoning or planning before executing final actions, manipulating\nthem into performing targeted malicious actions or invoking specific tools\nremains a significant challenge. Consequently, directly embedding adversarial\nstrings in malicious instructions or injecting malicious prompts into tool\ninteractions has become less effective against modern LLM agents. In this work,\nwe present UDora, a unified red teaming framework designed for LLM Agents that\ndynamically leverages the agent's own reasoning processes to compel it toward\nmalicious behavior. Specifically, UDora first samples the model's reasoning for\nthe given task, then automatically identifies multiple optimal positions within\nthese reasoning traces to insert targeted perturbations. Subsequently, it uses\nthe modified reasoning as the objective to optimize the adversarial strings. By\niteratively applying this process, the LLM agent will then be induced to\nundertake designated malicious actions or to invoke specific malicious tools.\nOur approach demonstrates superior effectiveness compared to existing methods\nacross three LLM agent datasets.",
    "bm25_score": 21.5572072255851
  },
  {
    "entry_id": "http://arxiv.org/abs/2502.19500v1",
    "updated": "2025-02-26T19:04:26+00:00",
    "published": "2025-02-26T19:04:26+00:00",
    "title": "Conversational Planning for Personal Plans",
    "authors": "['Konstantina Christakopoulou' 'Iris Qu' 'John Canny' 'Andrew Goodridge'\n 'Cj Adams' 'Minmin Chen' 'Maja Matari\u0107']",
    "summary": "The language generation and reasoning capabilities of large language models\n(LLMs) have enabled conversational systems with impressive performance in a\nvariety of tasks, from code generation, to composing essays, to passing STEM\nand legal exams, to a new paradigm for knowledge search. Besides those\nshort-term use applications, LLMs are increasingly used to help with real-life\ngoals or tasks that take a long time to complete, involving multiple sessions\nacross days, weeks, months, or even years. Thus to enable conversational\nsystems for long term interactions and tasks, we need language-based agents\nthat can plan for long horizons. Traditionally, such capabilities were\naddressed by reinforcement learning agents with hierarchical planning\ncapabilities. In this work, we explore a novel architecture where the LLM acts\nas the meta-controller deciding the agent's next macro-action, and tool use\naugmented LLM-based option policies execute the selected macro-action. We\ninstantiate this framework for a specific set of macro-actions enabling\nadaptive planning for users' personal plans through conversation and follow-up\nquestions collecting user feedback. We show how this paradigm can be applicable\nin scenarios ranging from tutoring for academic and non-academic tasks to\nconversational coaching for personal health plans.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.HC' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2502.19500v1' 'http://arxiv.org/pdf/2502.19500v1']",
    "pdf_url": "http://arxiv.org/pdf/2502.19500v1",
    "arxiv_id": "2502.19500",
    "row_id": 10786,
    "year": 2025,
    "processed_text": "Conversational Planning for Personal Plans The language generation and reasoning capabilities of large language models\n(LLMs) have enabled conversational systems with impressive performance in a\nvariety of tasks, from code generation, to composing essays, to passing STEM\nand legal exams, to a new paradigm for knowledge search. Besides those\nshort-term use applications, LLMs are increasingly used to help with real-life\ngoals or tasks that take a long time to complete, involving multiple sessions\nacross days, weeks, months, or even years. Thus to enable conversational\nsystems for long term interactions and tasks, we need language-based agents\nthat can plan for long horizons. Traditionally, such capabilities were\naddressed by reinforcement learning agents with hierarchical planning\ncapabilities. In this work, we explore a novel architecture where the LLM acts\nas the meta-controller deciding the agent's next macro-action, and tool use\naugmented LLM-based option policies execute the selected macro-action. We\ninstantiate this framework for a specific set of macro-actions enabling\nadaptive planning for users' personal plans through conversation and follow-up\nquestions collecting user feedback. We show how this paradigm can be applicable\nin scenarios ranging from tutoring for academic and non-academic tasks to\nconversational coaching for personal health plans.",
    "bm25_score": 20.66781025897174
  },
  {
    "entry_id": "http://arxiv.org/abs/2410.20007v1",
    "updated": "2024-10-25T23:32:48+00:00",
    "published": "2024-10-25T23:32:48+00:00",
    "title": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models",
    "authors": "['Danqing Wang' 'Zhuorui Ye' 'Fei Fang' 'Lei Li']",
    "summary": "Enhancing the reasoning capabilities of large language models (LLMs) is\ncrucial for enabling them to tackle complex, multi-step problems. Multi-agent\nframeworks have shown great potential in enhancing LLMs' reasoning\ncapabilities. However, the lack of effective cooperation between LLM agents\nhinders their performance, especially for multi-step reasoning tasks. This\npaper proposes a novel cooperative multi-agent reasoning framework (CoPlanner)\nby separating reasoning steps and assigning distinct duties to different\nagents. CoPlanner consists of two LLM agents: a planning agent and a reasoning\nagent. The planning agent provides high-level strategic hints, while the\nreasoning agent follows these hints and infers answers. By training the\nplanning agent's policy through the interactive reasoning process via Proximal\nPolicy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the\nprevious best method by 9.94\\% on LogiQA and 3.09\\% on BBH. Our results\ndemonstrate that the guidance from the planning agent and the effective\ncooperation between the agents contribute to the superior performance of\nCoPlanner in tackling multi-step reasoning problems.",
    "comment": "Working in progress",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL']",
    "links": "['http://arxiv.org/abs/2410.20007v1' 'http://arxiv.org/pdf/2410.20007v1']",
    "pdf_url": "http://arxiv.org/pdf/2410.20007v1",
    "arxiv_id": "2410.20007",
    "row_id": 7378,
    "year": 2024,
    "processed_text": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models Enhancing the reasoning capabilities of large language models (LLMs) is\ncrucial for enabling them to tackle complex, multi-step problems. Multi-agent\nframeworks have shown great potential in enhancing LLMs' reasoning\ncapabilities. However, the lack of effective cooperation between LLM agents\nhinders their performance, especially for multi-step reasoning tasks. This\npaper proposes a novel cooperative multi-agent reasoning framework (CoPlanner)\nby separating reasoning steps and assigning distinct duties to different\nagents. CoPlanner consists of two LLM agents: a planning agent and a reasoning\nagent. The planning agent provides high-level strategic hints, while the\nreasoning agent follows these hints and infers answers. By training the\nplanning agent's policy through the interactive reasoning process via Proximal\nPolicy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the\nprevious best method by 9.94\\% on LogiQA and 3.09\\% on BBH. Our results\ndemonstrate that the guidance from the planning agent and the effective\ncooperation between the agents contribute to the superior performance of\nCoPlanner in tackling multi-step reasoning problems.",
    "bm25_score": 20.548050306812648
  },
  {
    "entry_id": "http://arxiv.org/abs/2308.06391v1",
    "updated": "2023-08-11T21:17:13+00:00",
    "published": "2023-08-11T21:17:13+00:00",
    "title": "Dynamic Planning with a LLM",
    "authors": "['Gautier Dagan' 'Frank Keller' 'Alex Lascarides']",
    "summary": "While Large Language Models (LLMs) can solve many NLP tasks in zero-shot\nsettings, applications involving embodied agents remain problematic. In\nparticular, complex plans that require multi-step reasoning become difficult\nand too costly as the context window grows. Planning requires understanding the\nlikely effects of one's actions and identifying whether the current environment\nsatisfies the goal state. While symbolic planners find optimal solutions\nquickly, they require a complete and accurate representation of the planning\nproblem, severely limiting their use in practical scenarios. In contrast,\nmodern LLMs cope with noisy observations and high levels of uncertainty when\nreasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a\nneuro-symbolic framework where an LLM works hand-in-hand with a traditional\nplanner to solve an embodied task. Given action-descriptions, LLM-DP solves\nAlfworld faster and more efficiently than a naive LLM ReAct baseline.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.RO']",
    "links": "['http://arxiv.org/abs/2308.06391v1' 'http://arxiv.org/pdf/2308.06391v1']",
    "pdf_url": "http://arxiv.org/pdf/2308.06391v1",
    "arxiv_id": "2308.06391",
    "row_id": 42951,
    "year": 2023,
    "processed_text": "Dynamic Planning with a LLM While Large Language Models (LLMs) can solve many NLP tasks in zero-shot\nsettings, applications involving embodied agents remain problematic. In\nparticular, complex plans that require multi-step reasoning become difficult\nand too costly as the context window grows. Planning requires understanding the\nlikely effects of one's actions and identifying whether the current environment\nsatisfies the goal state. While symbolic planners find optimal solutions\nquickly, they require a complete and accurate representation of the planning\nproblem, severely limiting their use in practical scenarios. In contrast,\nmodern LLMs cope with noisy observations and high levels of uncertainty when\nreasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a\nneuro-symbolic framework where an LLM works hand-in-hand with a traditional\nplanner to solve an embodied task. Given action-descriptions, LLM-DP solves\nAlfworld faster and more efficiently than a naive LLM ReAct baseline.",
    "bm25_score": 20.12675019785235
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.10922v1",
    "updated": "2025-05-16T06:54:52+00:00",
    "published": "2025-05-16T06:54:52+00:00",
    "title": "Vaiage: A Multi-Agent Solution to Personalized Travel Planning",
    "authors": "['Binwen Liu' 'Jiexi Ge' 'Jiamin Wang']",
    "summary": "Planning trips is a cognitively intensive task involving conflicting user\npreferences, dynamic external information, and multi-step temporal-spatial\noptimization. Traditional platforms often fall short - they provide static\nresults, lack contextual adaptation, and fail to support real-time interaction\nor intent refinement.\n  Our approach, Vaiage, addresses these challenges through a graph-structured\nmulti-agent framework built around large language models (LLMs) that serve as\nboth goal-conditioned recommenders and sequential planners. LLMs infer user\nintent, suggest personalized destinations and activities, and synthesize\nitineraries that align with contextual constraints such as budget, timing,\ngroup size, and weather. Through natural language interaction, structured tool\nuse, and map-based feedback loops, Vaiage enables adaptive, explainable, and\nend-to-end travel planning grounded in both symbolic reasoning and\nconversational understanding.\n  To evaluate Vaiage, we conducted human-in-the-loop experiments using\nrubric-based GPT-4 assessments and qualitative feedback. The full system\nachieved an average score of 8.5 out of 10, outperforming the no-strategy (7.2)\nand no-external-API (6.8) variants, particularly in feasibility. Qualitative\nanalysis indicated that agent coordination - especially the Strategy and\nInformation Agents - significantly improved itinerary quality by optimizing\ntime use and integrating real-time context. These results demonstrate the\neffectiveness of combining LLM reasoning with symbolic agent coordination in\nopen-ended, real-world planning tasks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": "['cs.MA' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2505.10922v1' 'http://arxiv.org/pdf/2505.10922v1']",
    "pdf_url": "http://arxiv.org/pdf/2505.10922v1",
    "arxiv_id": "2505.10922",
    "row_id": 471859,
    "year": 2025,
    "processed_text": "Vaiage: A Multi-Agent Solution to Personalized Travel Planning Planning trips is a cognitively intensive task involving conflicting user\npreferences, dynamic external information, and multi-step temporal-spatial\noptimization. Traditional platforms often fall short - they provide static\nresults, lack contextual adaptation, and fail to support real-time interaction\nor intent refinement.\n  Our approach, Vaiage, addresses these challenges through a graph-structured\nmulti-agent framework built around large language models (LLMs) that serve as\nboth goal-conditioned recommenders and sequential planners. LLMs infer user\nintent, suggest personalized destinations and activities, and synthesize\nitineraries that align with contextual constraints such as budget, timing,\ngroup size, and weather. Through natural language interaction, structured tool\nuse, and map-based feedback loops, Vaiage enables adaptive, explainable, and\nend-to-end travel planning grounded in both symbolic reasoning and\nconversational understanding.\n  To evaluate Vaiage, we conducted human-in-the-loop experiments using\nrubric-based GPT-4 assessments and qualitative feedback. The full system\nachieved an average score of 8.5 out of 10, outperforming the no-strategy (7.2)\nand no-external-API (6.8) variants, particularly in feasibility. Qualitative\nanalysis indicated that agent coordination - especially the Strategy and\nInformation Agents - significantly improved itinerary quality by optimizing\ntime use and integrating real-time context. These results demonstrate the\neffectiveness of combining LLM reasoning with symbolic agent coordination in\nopen-ended, real-world planning tasks.",
    "bm25_score": 19.833900494961323
  },
  {
    "entry_id": "http://arxiv.org/abs/2411.01643v1",
    "updated": "2024-11-03T17:37:06+00:00",
    "published": "2024-11-03T17:37:06+00:00",
    "title": "EcoAct: Economic Agent Determines When to Register What Action",
    "authors": "['Shaokun Zhang' 'Jieyu Zhang' 'Dujian Ding' 'Mirian Hipolito Garcia'\n 'Ankur Mallick' 'Daniel Madrigal' 'Menglin Xia' 'Victor R\u00fchle'\n 'Qingyun Wu' 'Chi Wang']",
    "summary": "Recent advancements have enabled Large Language Models (LLMs) to function as\nagents that can perform actions using external tools. This requires\nregistering, i.e., integrating tool information into the LLM context prior to\ntaking actions. Current methods indiscriminately incorporate all candidate\ntools into the agent's context and retain them across multiple reasoning steps.\nThis process remains opaque to LLM agents and is not integrated into their\nreasoning procedures, leading to inefficiencies due to increased context length\nfrom irrelevant tools. To address this, we introduce EcoAct, a tool using\nalgorithm that allows LLMs to selectively register tools as needed, optimizing\ncontext use. By integrating the tool registration process into the reasoning\nprocedure, EcoAct reduces computational costs by over 50% in multiple steps\nreasoning tasks while maintaining performance, as demonstrated through\nextensive experiments. Moreover, it can be plugged into any reasoning pipeline\nwith only minor modifications to the prompt, making it applicable to LLM agents\nnow and future.",
    "comment": "16 pages, 10 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL']",
    "links": "['http://arxiv.org/abs/2411.01643v1' 'http://arxiv.org/pdf/2411.01643v1']",
    "pdf_url": "http://arxiv.org/pdf/2411.01643v1",
    "arxiv_id": "2411.01643",
    "row_id": 9045,
    "year": 2024,
    "processed_text": "EcoAct: Economic Agent Determines When to Register What Action Recent advancements have enabled Large Language Models (LLMs) to function as\nagents that can perform actions using external tools. This requires\nregistering, i.e., integrating tool information into the LLM context prior to\ntaking actions. Current methods indiscriminately incorporate all candidate\ntools into the agent's context and retain them across multiple reasoning steps.\nThis process remains opaque to LLM agents and is not integrated into their\nreasoning procedures, leading to inefficiencies due to increased context length\nfrom irrelevant tools. To address this, we introduce EcoAct, a tool using\nalgorithm that allows LLMs to selectively register tools as needed, optimizing\ncontext use. By integrating the tool registration process into the reasoning\nprocedure, EcoAct reduces computational costs by over 50% in multiple steps\nreasoning tasks while maintaining performance, as demonstrated through\nextensive experiments. Moreover, it can be plugged into any reasoning pipeline\nwith only minor modifications to the prompt, making it applicable to LLM agents\nnow and future.",
    "bm25_score": 19.582771968408313
  },
  {
    "entry_id": "http://arxiv.org/abs/2404.17833v1",
    "updated": "2024-04-27T08:56:45+00:00",
    "published": "2024-04-27T08:56:45+00:00",
    "title": "Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs",
    "authors": "['Zhenlan Ji' 'Daoyuan Wu' 'Pingchuan Ma' 'Zongjie Li' 'Shuai Wang']",
    "summary": "Agents based on large language models (LLMs) have demonstrated effectiveness\nin solving a wide range of tasks by integrating LLMs with key modules such as\nplanning, memory, and tool usage. Increasingly, customers are adopting LLM\nagents across a variety of commercial applications critical to reliability,\nincluding support for mental well-being, chemical synthesis, and software\ndevelopment. Nevertheless, our observations and daily use of LLM agents\nindicate that they are prone to making erroneous plans, especially when the\ntasks are complex and require long-term planning.\n  In this paper, we propose PDoctor, a novel and automated approach to testing\nLLM agents and understanding their erroneous planning. As the first work in\nthis direction, we formulate the detection of erroneous planning as a\nconstraint satisfiability problem: an LLM agent's plan is considered erroneous\nif its execution violates the constraints derived from the user inputs. To this\nend, PDoctor first defines a domain-specific language (DSL) for user queries\nand synthesizes varying inputs with the assistance of the Z3 constraint solver.\nThese synthesized inputs are natural language paragraphs that specify the\nrequirements for completing a series of tasks. Then, PDoctor derives\nconstraints from these requirements to form a testing oracle. We evaluate\nPDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5\nand GPT-4). The results show that PDoctor can effectively detect diverse errors\nin agent planning and provide insights and error characteristics that are\nvaluable to both agent developers and users. We conclude by discussing\npotential alternative designs and directions to extend PDoctor.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.PL']",
    "links": "['http://arxiv.org/abs/2404.17833v1' 'http://arxiv.org/pdf/2404.17833v1']",
    "pdf_url": "http://arxiv.org/pdf/2404.17833v1",
    "arxiv_id": "2404.17833",
    "row_id": 1062,
    "year": 2024,
    "processed_text": "Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs Agents based on large language models (LLMs) have demonstrated effectiveness\nin solving a wide range of tasks by integrating LLMs with key modules such as\nplanning, memory, and tool usage. Increasingly, customers are adopting LLM\nagents across a variety of commercial applications critical to reliability,\nincluding support for mental well-being, chemical synthesis, and software\ndevelopment. Nevertheless, our observations and daily use of LLM agents\nindicate that they are prone to making erroneous plans, especially when the\ntasks are complex and require long-term planning.\n  In this paper, we propose PDoctor, a novel and automated approach to testing\nLLM agents and understanding their erroneous planning. As the first work in\nthis direction, we formulate the detection of erroneous planning as a\nconstraint satisfiability problem: an LLM agent's plan is considered erroneous\nif its execution violates the constraints derived from the user inputs. To this\nend, PDoctor first defines a domain-specific language (DSL) for user queries\nand synthesizes varying inputs with the assistance of the Z3 constraint solver.\nThese synthesized inputs are natural language paragraphs that specify the\nrequirements for completing a series of tasks. Then, PDoctor derives\nconstraints from these requirements to form a testing oracle. We evaluate\nPDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5\nand GPT-4). The results show that PDoctor can effectively detect diverse errors\nin agent planning and provide insights and error characteristics that are\nvaluable to both agent developers and users. We conclude by discussing\npotential alternative designs and directions to extend PDoctor.",
    "bm25_score": 19.385457276936418
  },
  {
    "entry_id": "http://arxiv.org/abs/2412.13682v1",
    "updated": "2024-12-18T10:10:12+00:00",
    "published": "2024-12-18T10:10:12+00:00",
    "title": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese Travel Planning",
    "authors": "['Jie-Jing Shao' 'Xiao-Wen Yang' 'Bo-Wen Zhang' 'Baizhi Chen' 'Wen-Da Wei'\n 'Lan-Zhe Guo' 'Yu-feng Li']",
    "summary": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios.",
    "comment": "Webpage: https://www.lamda.nju.edu.cn/shaojj/chinatravel",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL']",
    "links": "['http://arxiv.org/abs/2412.13682v1' 'http://arxiv.org/pdf/2412.13682v1']",
    "pdf_url": "http://arxiv.org/pdf/2412.13682v1",
    "arxiv_id": "2412.13682",
    "row_id": 9676,
    "year": 2024,
    "processed_text": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese Travel Planning Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios.",
    "bm25_score": 19.353746265431248
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.01622v4",
    "updated": "2024-10-23T15:02:57+00:00",
    "published": "2024-02-02T18:39:51+00:00",
    "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
    "authors": "['Jian Xie' 'Kai Zhang' 'Jiangjie Chen' 'Tinghui Zhu' 'Renze Lou'\n 'Yuandong Tian' 'Yanghua Xiao' 'Yu Su']",
    "summary": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
    "comment": "ICML 2024 (Spotlight)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL']",
    "links": "['http://arxiv.org/abs/2402.01622v4' 'http://arxiv.org/pdf/2402.01622v4']",
    "pdf_url": "http://arxiv.org/pdf/2402.01622v4",
    "arxiv_id": "2402.01622",
    "row_id": 37281,
    "year": 2024,
    "processed_text": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
    "bm25_score": 19.312104284850818
  }
]