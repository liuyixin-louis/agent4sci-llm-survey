[
  {
    "entry_id": "http://arxiv.org/abs/2502.01390v1",
    "updated": "2025-02-03T14:23:22+00:00",
    "published": "2025-02-03T14:23:22+00:00",
    "title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant",
    "authors": "['Gaole He' 'Gianluca Demartini' 'Ujwal Gadiraju']",
    "summary": "Since the explosion in popularity of ChatGPT, large language models (LLMs)\nhave continued to impact our everyday lives. Equipped with external tools that\nare designed for a specific purpose (e.g., for flight booking or an alarm\nclock), LLM agents exercise an increasing capability to assist humans in their\ndaily work. Although LLM agents have shown a promising blueprint as daily\nassistants, there is a limited understanding of how they can provide daily\nassistance based on planning and sequential decision making capabilities. We\ndraw inspiration from recent work that has highlighted the value of\n'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks.\nWe conducted an empirical study (N = 248) of LLM agents as daily assistants in\nsix commonly occurring tasks with different levels of risk typically associated\nwith them (e.g., flight ticket booking and credit card payments). To ensure\nuser agency and control over the LLM agent, we adopted LLM agents in a\nplan-then-execute manner, wherein the agents conducted step-wise planning and\nstep-by-step execution in a simulation environment. We analyzed how user\ninvolvement at each stage affects their trust and collaborative team\nperformance. Our findings demonstrate that LLM agents can be a double-edged\nsword -- (1) they can work well when a high-quality plan and necessary user\ninvolvement in execution are available, and (2) users can easily mistrust the\nLLM agents with plans that seem plausible. We synthesized key insights for\nusing LLM agents as daily assistants to calibrate user trust and achieve better\noverall task outcomes. Our work has important implications for the future\ndesign of daily assistants and human-AI collaboration with LLM agents.",
    "comment": "conditionally accepted to CHI 2025",
    "journal_ref": null,
    "doi": "10.1145/3706598.3713218",
    "primary_category": "cs.HC",
    "categories": "['cs.HC' 'cs.CL']",
    "links": "['http://dx.doi.org/10.1145/3706598.3713218'\n 'http://arxiv.org/abs/2502.01390v1' 'http://arxiv.org/pdf/2502.01390v1']",
    "pdf_url": "http://arxiv.org/pdf/2502.01390v1",
    "arxiv_id": "2502.01390",
    "row_id": 230264,
    "year": 2025,
    "processed_text": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant Since the explosion in popularity of ChatGPT, large language models (LLMs)\nhave continued to impact our everyday lives. Equipped with external tools that\nare designed for a specific purpose (e.g., for flight booking or an alarm\nclock), LLM agents exercise an increasing capability to assist humans in their\ndaily work. Although LLM agents have shown a promising blueprint as daily\nassistants, there is a limited understanding of how they can provide daily\nassistance based on planning and sequential decision making capabilities. We\ndraw inspiration from recent work that has highlighted the value of\n'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks.\nWe conducted an empirical study (N = 248) of LLM agents as daily assistants in\nsix commonly occurring tasks with different levels of risk typically associated\nwith them (e.g., flight ticket booking and credit card payments). To ensure\nuser agency and control over the LLM agent, we adopted LLM agents in a\nplan-then-execute manner, wherein the agents conducted step-wise planning and\nstep-by-step execution in a simulation environment. We analyzed how user\ninvolvement at each stage affects their trust and collaborative team\nperformance. Our findings demonstrate that LLM agents can be a double-edged\nsword -- (1) they can work well when a high-quality plan and necessary user\ninvolvement in execution are available, and (2) users can easily mistrust the\nLLM agents with plans that seem plausible. We synthesized key insights for\nusing LLM agents as daily assistants to calibrate user trust and achieve better\noverall task outcomes. Our work has important implications for the future\ndesign of daily assistants and human-AI collaboration with LLM agents.",
    "bm25_score": 10.851289472377129
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04559v4",
    "updated": "2024-11-01T16:10:41+00:00",
    "published": "2024-02-07T03:37:19+00:00",
    "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
    "authors": "['Chengxing Xie' 'Canyu Chen' 'Feiran Jia' 'Ziyu Ye' 'Shiyang Lai'\n 'Kai Shu' 'Jindong Gu' 'Adel Bibi' 'Ziniu Hu' 'David Jurgens'\n 'James Evans' 'Philip Torr' 'Bernard Ghanem' 'Guohao Li']",
    "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
    "comment": "Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.HC']",
    "links": "['http://arxiv.org/abs/2402.04559v4' 'http://arxiv.org/pdf/2402.04559v4']",
    "pdf_url": "http://arxiv.org/pdf/2402.04559v4",
    "arxiv_id": "2402.04559",
    "row_id": 2020,
    "year": 2024,
    "processed_text": "Can Large Language Model Agents Simulate Human Trust Behavior? Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
    "bm25_score": 10.84366068597188
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.01637v1",
    "updated": "2024-06-02T16:25:26+00:00",
    "published": "2024-06-02T16:25:26+00:00",
    "title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities",
    "authors": "['Richard Fang' 'Rohan Bindu' 'Akul Gupta' 'Qiusi Zhan' 'Daniel Kang']",
    "summary": "LLM agents have become increasingly sophisticated, especially in the realm of\ncybersecurity. Researchers have shown that LLM agents can exploit real-world\nvulnerabilities when given a description of the vulnerability and toy\ncapture-the-flag problems. However, these agents still perform poorly on\nreal-world vulnerabilities that are unknown to the agent ahead of time\n(zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world,\nzero-day vulnerabilities. Prior agents struggle with exploring many different\nvulnerabilities and long-range planning when used alone. To resolve this, we\nintroduce HPTSA, a system of agents with a planning agent that can launch\nsubagents. The planning agent explores the system and determines which\nsubagents to call, resolving long-term planning issues when trying different\nvulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and\nshow that our team of agents improve over prior work by up to 4.5$\\times$.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": "['cs.MA' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2406.01637v1' 'http://arxiv.org/pdf/2406.01637v1']",
    "pdf_url": "http://arxiv.org/pdf/2406.01637v1",
    "arxiv_id": "2406.01637",
    "row_id": 322366,
    "year": 2024,
    "processed_text": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities LLM agents have become increasingly sophisticated, especially in the realm of\ncybersecurity. Researchers have shown that LLM agents can exploit real-world\nvulnerabilities when given a description of the vulnerability and toy\ncapture-the-flag problems. However, these agents still perform poorly on\nreal-world vulnerabilities that are unknown to the agent ahead of time\n(zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world,\nzero-day vulnerabilities. Prior agents struggle with exploring many different\nvulnerabilities and long-range planning when used alone. To resolve this, we\nintroduce HPTSA, a system of agents with a planning agent that can launch\nsubagents. The planning agent explores the system and determines which\nsubagents to call, resolving long-term planning issues when trying different\nvulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and\nshow that our team of agents improve over prior work by up to 4.5$\\times$.",
    "bm25_score": 10.8220419454651
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.19213v1",
    "updated": "2025-03-24T23:39:44+00:00",
    "published": "2025-03-24T23:39:44+00:00",
    "title": "A Survey of Large Language Model Agents for Question Answering",
    "authors": "['Murong Yue']",
    "summary": "This paper surveys the development of large language model (LLM)-based agents\nfor question answering (QA). Traditional agents face significant limitations,\nincluding substantial data requirements and difficulty in generalizing to new\nenvironments. LLM-based agents address these challenges by leveraging LLMs as\ntheir core reasoning engine. These agents achieve superior QA results compared\nto traditional QA pipelines and naive LLM QA systems by enabling interaction\nwith external environments. We systematically review the design of LLM agents\nin the context of QA tasks, organizing our discussion across key stages:\nplanning, question understanding, information retrieval, and answer generation.\nAdditionally, this paper identifies ongoing challenges and explores future\nresearch directions to enhance the performance of LLM agent QA systems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI' 'cs.HC']",
    "links": "['http://arxiv.org/abs/2503.19213v1' 'http://arxiv.org/pdf/2503.19213v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.19213v1",
    "arxiv_id": "2503.19213",
    "row_id": 57360,
    "year": 2025,
    "processed_text": "A Survey of Large Language Model Agents for Question Answering This paper surveys the development of large language model (LLM)-based agents\nfor question answering (QA). Traditional agents face significant limitations,\nincluding substantial data requirements and difficulty in generalizing to new\nenvironments. LLM-based agents address these challenges by leveraging LLMs as\ntheir core reasoning engine. These agents achieve superior QA results compared\nto traditional QA pipelines and naive LLM QA systems by enabling interaction\nwith external environments. We systematically review the design of LLM agents\nin the context of QA tasks, organizing our discussion across key stages:\nplanning, question understanding, information retrieval, and answer generation.\nAdditionally, this paper identifies ongoing challenges and explores future\nresearch directions to enhance the performance of LLM agent QA systems.",
    "bm25_score": 10.509595567029404
  },
  {
    "entry_id": "http://arxiv.org/abs/2403.12482v2",
    "updated": "2024-05-23T06:29:00+00:00",
    "published": "2024-03-19T06:39:47+00:00",
    "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
    "authors": "['Xudong Guo' 'Kaixuan Huang' 'Jiale Liu' 'Wenhui Fan' 'Natalia V\u00e9lez'\n 'Qingyun Wu' 'Huazheng Wang' 'Thomas L. Griffiths' 'Mengdi Wang']",
    "summary": "Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.CY' 'cs.MA']",
    "links": "['http://arxiv.org/abs/2403.12482v2' 'http://arxiv.org/pdf/2403.12482v2']",
    "pdf_url": "http://arxiv.org/pdf/2403.12482v2",
    "arxiv_id": "2403.12482",
    "row_id": 1490,
    "year": 2024,
    "processed_text": "Embodied LLM Agents Learn to Cooperate in Organized Teams Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
    "bm25_score": 10.465627248418183
  },
  {
    "entry_id": "http://arxiv.org/abs/2409.10372v3",
    "updated": "2024-10-30T16:45:15+00:00",
    "published": "2024-09-16T15:15:51+00:00",
    "title": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation",
    "authors": "['Qiliang Chen' 'Sepehr Ilami' 'Nunzio Lore' 'Babak Heydari']",
    "summary": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.CY' 'cs.GT']",
    "links": "['http://arxiv.org/abs/2409.10372v3' 'http://arxiv.org/pdf/2409.10372v3']",
    "pdf_url": "http://arxiv.org/pdf/2409.10372v3",
    "arxiv_id": "2409.10372",
    "row_id": 8152,
    "year": 2024,
    "processed_text": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
    "bm25_score": 10.429110429674427
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06596v1",
    "updated": "2024-02-09T18:19:25+00:00",
    "published": "2024-02-09T18:19:25+00:00",
    "title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment",
    "authors": "['Mingzhe Xing' 'Rongkai Zhang' 'Hui Xue' 'Qi Chen' 'Fan Yang' 'Zhen Xiao']",
    "summary": "Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.HC' 'cs.SE']",
    "links": "['http://arxiv.org/abs/2402.06596v1' 'http://arxiv.org/pdf/2402.06596v1']",
    "pdf_url": "http://arxiv.org/pdf/2402.06596v1",
    "arxiv_id": "2402.06596",
    "row_id": 1976,
    "year": 2024,
    "processed_text": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.",
    "bm25_score": 10.423962205197373
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.03007v1",
    "updated": "2024-06-05T07:14:28+00:00",
    "published": "2024-06-05T07:14:28+00:00",
    "title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents",
    "authors": "['Yifei Wang' 'Dizhan Xue' 'Shengjie Zhang' 'Shengsheng Qian']",
    "summary": "With the prosperity of large language models (LLMs), powerful LLM-based\nintelligent agents have been developed to provide customized services with a\nset of user-defined tools. State-of-the-art methods for constructing LLM agents\nadopt trained LLMs and further fine-tune them on data for the agent task.\nHowever, we show that such methods are vulnerable to our proposed backdoor\nattacks named BadAgent on various agent tasks, where a backdoor can be embedded\nby fine-tuning on the backdoor data. At test time, the attacker can manipulate\nthe deployed LLM agents to execute harmful operations by showing the trigger in\nthe agent input or environment. To our surprise, our proposed attack methods\nare extremely robust even after fine-tuning on trustworthy data. Though\nbackdoor attacks have been studied extensively in natural language processing,\nto the best of our knowledge, we could be the first to study them on LLM agents\nthat are more dangerous due to the permission to use external tools. Our work\ndemonstrates the clear risk of constructing LLM agents based on untrusted LLMs\nor data. Our code is public at https://github.com/DPamK/BadAgent",
    "comment": "Accepted by ACL 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI' 'cs.CR' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2406.03007v1' 'http://arxiv.org/pdf/2406.03007v1']",
    "pdf_url": "http://arxiv.org/pdf/2406.03007v1",
    "arxiv_id": "2406.03007",
    "row_id": 32069,
    "year": 2024,
    "processed_text": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents With the prosperity of large language models (LLMs), powerful LLM-based\nintelligent agents have been developed to provide customized services with a\nset of user-defined tools. State-of-the-art methods for constructing LLM agents\nadopt trained LLMs and further fine-tune them on data for the agent task.\nHowever, we show that such methods are vulnerable to our proposed backdoor\nattacks named BadAgent on various agent tasks, where a backdoor can be embedded\nby fine-tuning on the backdoor data. At test time, the attacker can manipulate\nthe deployed LLM agents to execute harmful operations by showing the trigger in\nthe agent input or environment. To our surprise, our proposed attack methods\nare extremely robust even after fine-tuning on trustworthy data. Though\nbackdoor attacks have been studied extensively in natural language processing,\nto the best of our knowledge, we could be the first to study them on LLM agents\nthat are more dangerous due to the permission to use external tools. Our work\ndemonstrates the clear risk of constructing LLM agents based on untrusted LLMs\nor data. Our code is public at https://github.com/DPamK/BadAgent",
    "bm25_score": 10.423872763811385
  },
  {
    "entry_id": "http://arxiv.org/abs/2410.12481v1",
    "updated": "2024-10-16T11:59:27+00:00",
    "published": "2024-10-16T11:59:27+00:00",
    "title": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling",
    "authors": "['Loris Gaven' 'Clement Romac' 'Thomas Carta' 'Sylvain Lamprier'\n 'Olivier Sigaud' 'Pierre-Yves Oudeyer']",
    "summary": "The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": "['cs.LG' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2410.12481v1' 'http://arxiv.org/pdf/2410.12481v1']",
    "pdf_url": "http://arxiv.org/pdf/2410.12481v1",
    "arxiv_id": "2410.12481",
    "row_id": 294884,
    "year": 2024,
    "processed_text": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments.",
    "bm25_score": 10.365938331371575
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.03961v1",
    "updated": "2025-05-06T20:23:25+00:00",
    "published": "2025-05-06T20:23:25+00:00",
    "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete",
    "authors": "['Gerrit Gro\u00dfmann' 'Larisa Ivanova' 'Sai Leela Poduru'\n 'Mohaddeseh Tabrizian' 'Islam Mesabah' 'David A. Selby'\n 'Sebastian J. Vollmer']",
    "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment.",
    "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.MA' 'I.2.11; I.2.7; I.6; J.4']",
    "links": "['http://arxiv.org/abs/2505.03961v1' 'http://arxiv.org/pdf/2505.03961v1']",
    "pdf_url": "http://arxiv.org/pdf/2505.03961v1",
    "arxiv_id": "2505.03961",
    "row_id": 11854,
    "year": 2025,
    "processed_text": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment.",
    "bm25_score": 10.31770003957593
  },
  {
    "entry_id": "http://arxiv.org/abs/2407.01231v1",
    "updated": "2024-07-01T12:22:46+00:00",
    "published": "2024-07-01T12:22:46+00:00",
    "title": "MIRAI: Evaluating LLM Agents for Event Forecasting",
    "authors": "['Chenchen Ye' 'Ziniu Hu' 'Yihe Deng' 'Zijie Huang' 'Mingyu Derek Ma'\n 'Yanqiao Zhu' 'Wei Wang']",
    "summary": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents\nto autonomously collect world information, over which to conduct reasoning to\nsolve complex problems. Given this capability, increasing interests have been\nput into employing LLM agents for predicting international events, which can\ninfluence decision-making and shape policy development on an international\nscale. Despite such a growing interest, there is a lack of a rigorous benchmark\nof LLM agents' forecasting capability and reliability. To address this gap, we\nintroduce MIRAI, a novel benchmark designed to systematically evaluate LLM\nagents as temporal forecasters in the context of international events. Our\nbenchmark features an agentic environment with tools for accessing an extensive\ndatabase of historical, structured events and textual news articles. We refine\nthe GDELT event database with careful cleaning and parsing to curate a series\nof relational prediction tasks with varying forecasting horizons, assessing LLM\nagents' abilities from short-term to long-term forecasting. We further\nimplement APIs to enable LLM agents to utilize different tools via a code-based\ninterface. In summary, MIRAI comprehensively evaluates the agents' capabilities\nin three dimensions: 1) autonomously source and integrate critical information\nfrom large global databases; 2) write codes using domain-specific APIs and\nlibraries for tool-use; and 3) jointly reason over historical knowledge from\ndiverse formats and time to accurately predict future events. Through\ncomprehensive benchmarking, we aim to establish a reliable framework for\nassessing the capabilities of LLM agents in forecasting international events,\nthereby contributing to the development of more accurate and trustworthy models\nfor international relation analysis.",
    "comment": "66 pages, 8 figures, 6 tables; Website: https://mirai-llm.github.io/",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2407.01231v1' 'http://arxiv.org/pdf/2407.01231v1']",
    "pdf_url": "http://arxiv.org/pdf/2407.01231v1",
    "arxiv_id": "2407.01231",
    "row_id": 30465,
    "year": 2024,
    "processed_text": "MIRAI: Evaluating LLM Agents for Event Forecasting Recent advancements in Large Language Models (LLMs) have empowered LLM agents\nto autonomously collect world information, over which to conduct reasoning to\nsolve complex problems. Given this capability, increasing interests have been\nput into employing LLM agents for predicting international events, which can\ninfluence decision-making and shape policy development on an international\nscale. Despite such a growing interest, there is a lack of a rigorous benchmark\nof LLM agents' forecasting capability and reliability. To address this gap, we\nintroduce MIRAI, a novel benchmark designed to systematically evaluate LLM\nagents as temporal forecasters in the context of international events. Our\nbenchmark features an agentic environment with tools for accessing an extensive\ndatabase of historical, structured events and textual news articles. We refine\nthe GDELT event database with careful cleaning and parsing to curate a series\nof relational prediction tasks with varying forecasting horizons, assessing LLM\nagents' abilities from short-term to long-term forecasting. We further\nimplement APIs to enable LLM agents to utilize different tools via a code-based\ninterface. In summary, MIRAI comprehensively evaluates the agents' capabilities\nin three dimensions: 1) autonomously source and integrate critical information\nfrom large global databases; 2) write codes using domain-specific APIs and\nlibraries for tool-use; and 3) jointly reason over historical knowledge from\ndiverse formats and time to accurately predict future events. Through\ncomprehensive benchmarking, we aim to establish a reliable framework for\nassessing the capabilities of LLM agents in forecasting international events,\nthereby contributing to the development of more accurate and trustworthy models\nfor international relation analysis.",
    "bm25_score": 10.301503829871013
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.18825v1",
    "updated": "2025-03-24T16:06:04+00:00",
    "published": "2025-03-24T16:06:04+00:00",
    "title": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments",
    "authors": "['Sara Fish' 'Julia Shephard' 'Minkai Li' 'Ran I. Shorrer'\n 'Yannai A. Gonczarowski']",
    "summary": "We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.GT']",
    "links": "['http://arxiv.org/abs/2503.18825v1' 'http://arxiv.org/pdf/2503.18825v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.18825v1",
    "arxiv_id": "2503.18825",
    "row_id": 11214,
    "year": 2025,
    "processed_text": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy.",
    "bm25_score": 10.282521742964668
  },
  {
    "entry_id": "http://arxiv.org/abs/2412.10270v1",
    "updated": "2024-12-13T16:45:49+00:00",
    "published": "2024-12-13T16:45:49+00:00",
    "title": "Cultural Evolution of Cooperation among LLM Agents",
    "authors": "['Aron Vallinder' 'Edward Hughes']",
    "summary": "Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society.",
    "comment": "15 pages, 6 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": "['cs.MA' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2412.10270v1' 'http://arxiv.org/pdf/2412.10270v1']",
    "pdf_url": "http://arxiv.org/pdf/2412.10270v1",
    "arxiv_id": "2412.10270",
    "row_id": 325125,
    "year": 2024,
    "processed_text": "Cultural Evolution of Cooperation among LLM Agents Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society.",
    "bm25_score": 10.267324017747404
  },
  {
    "entry_id": "http://arxiv.org/abs/2403.05307v1",
    "updated": "2024-03-08T13:34:20+00:00",
    "published": "2024-03-08T13:34:20+00:00",
    "title": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents",
    "authors": "['Jinyang Li' 'Nan Huo' 'Yan Gao' 'Jiayi Shi' 'Yingxiu Zhao' 'Ge Qu'\n 'Yurong Wu' 'Chenhao Ma' 'Jian-Guang Lou' 'Reynold Cheng']",
    "summary": "Interactive Data Analysis, the collaboration between humans and LLM agents,\nenables real-time data exploration for informed decision-making. The challenges\nand costs of collecting realistic interactive logs for data analysis hinder the\nquantitative evaluation of Large Language Model (LLM) agents in this task. To\nmitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate\nLLM agents on interactive data analysis. Tapilot-Crossing contains 1024\ninteractions, covering 4 practical scenarios: Normal, Action, Private, and\nPrivate Action. Notably, Tapilot-Crossing is constructed by an economical\nmulti-agent environment, Decision Company, with few human efforts. We evaluate\npopular and advanced LLM agents in Tapilot-Crossing, which underscores the\nchallenges of interactive data analysis. Furthermore, we propose Adaptive\nInteraction Reflection (AIR), a self-generated reflection strategy that guides\nLLM agents to learn from successful history. Experiments demonstrate that Air\ncan evolve LLMs into effective interactive data analysis agents, achieving a\nrelative performance improvement of up to 44.5%.",
    "comment": "30 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2403.05307v1' 'http://arxiv.org/pdf/2403.05307v1']",
    "pdf_url": "http://arxiv.org/pdf/2403.05307v1",
    "arxiv_id": "2403.05307",
    "row_id": 1570,
    "year": 2024,
    "processed_text": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents Interactive Data Analysis, the collaboration between humans and LLM agents,\nenables real-time data exploration for informed decision-making. The challenges\nand costs of collecting realistic interactive logs for data analysis hinder the\nquantitative evaluation of Large Language Model (LLM) agents in this task. To\nmitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate\nLLM agents on interactive data analysis. Tapilot-Crossing contains 1024\ninteractions, covering 4 practical scenarios: Normal, Action, Private, and\nPrivate Action. Notably, Tapilot-Crossing is constructed by an economical\nmulti-agent environment, Decision Company, with few human efforts. We evaluate\npopular and advanced LLM agents in Tapilot-Crossing, which underscores the\nchallenges of interactive data analysis. Furthermore, we propose Adaptive\nInteraction Reflection (AIR), a self-generated reflection strategy that guides\nLLM agents to learn from successful history. Experiments demonstrate that Air\ncan evolve LLMs into effective interactive data analysis agents, achieving a\nrelative performance improvement of up to 44.5%.",
    "bm25_score": 10.247587450606837
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.01908v1",
    "updated": "2025-02-28T21:30:28+00:00",
    "published": "2025-02-28T21:30:28+00:00",
    "title": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning",
    "authors": "['Jiawei Zhang' 'Shuang Yang' 'Bo Li']",
    "summary": "Large Language Model (LLM) agents equipped with external tools have become\nincreasingly powerful for handling complex tasks such as web shopping,\nautomated email replies, and financial trading. However, these advancements\nalso amplify the risks of adversarial attacks, particularly when LLM agents can\naccess sensitive external functionalities. Moreover, because LLM agents engage\nin extensive reasoning or planning before executing final actions, manipulating\nthem into performing targeted malicious actions or invoking specific tools\nremains a significant challenge. Consequently, directly embedding adversarial\nstrings in malicious instructions or injecting malicious prompts into tool\ninteractions has become less effective against modern LLM agents. In this work,\nwe present UDora, a unified red teaming framework designed for LLM Agents that\ndynamically leverages the agent's own reasoning processes to compel it toward\nmalicious behavior. Specifically, UDora first samples the model's reasoning for\nthe given task, then automatically identifies multiple optimal positions within\nthese reasoning traces to insert targeted perturbations. Subsequently, it uses\nthe modified reasoning as the objective to optimize the adversarial strings. By\niteratively applying this process, the LLM agent will then be induced to\nundertake designated malicious actions or to invoke specific malicious tools.\nOur approach demonstrates superior effectiveness compared to existing methods\nacross three LLM agent datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": "['cs.CR' 'cs.AI' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2503.01908v1' 'http://arxiv.org/pdf/2503.01908v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.01908v1",
    "arxiv_id": "2503.01908",
    "row_id": 83930,
    "year": 2025,
    "processed_text": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning Large Language Model (LLM) agents equipped with external tools have become\nincreasingly powerful for handling complex tasks such as web shopping,\nautomated email replies, and financial trading. However, these advancements\nalso amplify the risks of adversarial attacks, particularly when LLM agents can\naccess sensitive external functionalities. Moreover, because LLM agents engage\nin extensive reasoning or planning before executing final actions, manipulating\nthem into performing targeted malicious actions or invoking specific tools\nremains a significant challenge. Consequently, directly embedding adversarial\nstrings in malicious instructions or injecting malicious prompts into tool\ninteractions has become less effective against modern LLM agents. In this work,\nwe present UDora, a unified red teaming framework designed for LLM Agents that\ndynamically leverages the agent's own reasoning processes to compel it toward\nmalicious behavior. Specifically, UDora first samples the model's reasoning for\nthe given task, then automatically identifies multiple optimal positions within\nthese reasoning traces to insert targeted perturbations. Subsequently, it uses\nthe modified reasoning as the objective to optimize the adversarial strings. By\niteratively applying this process, the LLM agent will then be induced to\nundertake designated malicious actions or to invoke specific malicious tools.\nOur approach demonstrates superior effectiveness compared to existing methods\nacross three LLM agent datasets.",
    "bm25_score": 10.240553200525351
  },
  {
    "entry_id": "http://arxiv.org/abs/2409.09345v1",
    "updated": "2024-09-14T07:32:49+00:00",
    "published": "2024-09-14T07:32:49+00:00",
    "title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models",
    "authors": "['Yuanzhao Zhai' 'Tingkai Yang' 'Kele Xu' 'Feng Dawei' 'Cheng Yang'\n 'Bo Ding' 'Huaimin Wang']",
    "summary": "Agents significantly enhance the capabilities of standalone Large Language\nModels (LLMs) by perceiving environments, making decisions, and executing\nactions. However, LLM agents still face challenges in tasks that require\nmultiple decision-making steps. Estimating the value of actions in specific\ntasks is difficult when intermediate actions are neither appropriately rewarded\nnor penalized. In this paper, we propose leveraging a task-relevant Q-value\nmodel to guide action selection. Specifically, we first collect decision-making\ntrajectories annotated with step-level Q values via Monte Carlo Tree Search\n(MCTS) and construct preference data. We then use another LLM to fit these\npreferences through step-level Direct Policy Optimization (DPO), which serves\nas the Q-value model. During inference, at each decision-making step, LLM\nagents select the action with the highest Q value before interacting with the\nenvironment. We apply our method to various open-source and API-based LLM\nagents, demonstrating that Q-value models significantly improve their\nperformance. Notably, the performance of the agent built with\nPhi-3-mini-4k-instruct improved by 103% on WebShop and 75% on HotPotQA when\nenhanced with Q-value models, even surpassing GPT-4o-mini. Additionally,\nQ-value models offer several advantages, such as generalization to different\nLLM agents and seamless integration with existing prompting strategies.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2409.09345v1' 'http://arxiv.org/pdf/2409.09345v1']",
    "pdf_url": "http://arxiv.org/pdf/2409.09345v1",
    "arxiv_id": "2409.09345",
    "row_id": 8161,
    "year": 2024,
    "processed_text": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models Agents significantly enhance the capabilities of standalone Large Language\nModels (LLMs) by perceiving environments, making decisions, and executing\nactions. However, LLM agents still face challenges in tasks that require\nmultiple decision-making steps. Estimating the value of actions in specific\ntasks is difficult when intermediate actions are neither appropriately rewarded\nnor penalized. In this paper, we propose leveraging a task-relevant Q-value\nmodel to guide action selection. Specifically, we first collect decision-making\ntrajectories annotated with step-level Q values via Monte Carlo Tree Search\n(MCTS) and construct preference data. We then use another LLM to fit these\npreferences through step-level Direct Policy Optimization (DPO), which serves\nas the Q-value model. During inference, at each decision-making step, LLM\nagents select the action with the highest Q value before interacting with the\nenvironment. We apply our method to various open-source and API-based LLM\nagents, demonstrating that Q-value models significantly improve their\nperformance. Notably, the performance of the agent built with\nPhi-3-mini-4k-instruct improved by 103% on WebShop and 75% on HotPotQA when\nenhanced with Q-value models, even surpassing GPT-4o-mini. Additionally,\nQ-value models offer several advantages, such as generalization to different\nLLM agents and seamless integration with existing prompting strategies.",
    "bm25_score": 10.234411292526264
  },
  {
    "entry_id": "http://arxiv.org/abs/2405.14751v1",
    "updated": "2024-05-23T16:17:44+00:00",
    "published": "2024-05-23T16:17:44+00:00",
    "title": "AGILE: A Novel Framework of LLM Agents",
    "authors": "['Peiyuan Feng' 'Yichen He' 'Guanhua Huang' 'Yuan Lin' 'Hanchong Zhang'\n 'Yuchen Zhang' 'Hang Li']",
    "summary": "We introduce a novel framework of LLM agents named AGILE (AGent that\nInteracts and Learns from Environments) designed to perform complex\nconversational tasks with users, leveraging LLMs, memory, tools, and\ninteractions with experts. The agent's abilities include not only conversation\nbut also reflection, utilization of tools, and consultation with experts. We\nformulate the construction of such an LLM agent as a reinforcement learning\nproblem, in which the LLM serves as the policy model. We fine-tune the LLM\nusing labeled data of actions and the PPO algorithm. We focus on question\nanswering and release a dataset for agents called ProductQA, comprising\nchallenging questions in online shopping. Our extensive experiments on\nProductQA and MedMCQA show that AGILE agents based on 13B and 7B LLMs trained\nwith PPO can outperform GPT-4 agents. Our ablation study highlights the\nindispensability of memory, tools, consultation, reflection, and reinforcement\nlearning in achieving the agent's strong performance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": "['cs.LG']",
    "links": "['http://arxiv.org/abs/2405.14751v1' 'http://arxiv.org/pdf/2405.14751v1']",
    "pdf_url": "http://arxiv.org/pdf/2405.14751v1",
    "arxiv_id": "2405.14751",
    "row_id": 284084,
    "year": 2024,
    "processed_text": "AGILE: A Novel Framework of LLM Agents We introduce a novel framework of LLM agents named AGILE (AGent that\nInteracts and Learns from Environments) designed to perform complex\nconversational tasks with users, leveraging LLMs, memory, tools, and\ninteractions with experts. The agent's abilities include not only conversation\nbut also reflection, utilization of tools, and consultation with experts. We\nformulate the construction of such an LLM agent as a reinforcement learning\nproblem, in which the LLM serves as the policy model. We fine-tune the LLM\nusing labeled data of actions and the PPO algorithm. We focus on question\nanswering and release a dataset for agents called ProductQA, comprising\nchallenging questions in online shopping. Our extensive experiments on\nProductQA and MedMCQA show that AGILE agents based on 13B and 7B LLMs trained\nwith PPO can outperform GPT-4 agents. Our ablation study highlights the\nindispensability of memory, tools, consultation, reflection, and reinforcement\nlearning in achieving the agent's strong performance.",
    "bm25_score": 10.19999184326223
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.11942v3",
    "updated": "2025-05-30T02:28:21+00:00",
    "published": "2025-05-17T10:09:11+00:00",
    "title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners",
    "authors": "['Junhao Zheng' 'Xidi Cai' 'Qiuke Li' 'Duzhen Zhang' 'ZhongZhi Li'\n 'Yingying Zhang' 'Le Song' 'Qianli Ma']",
    "summary": "Lifelong learning is essential for intelligent agents operating in dynamic\nenvironments. Current large language model (LLM)-based agents, however, remain\nstateless and unable to accumulate or transfer knowledge over time. Existing\nbenchmarks treat agents as static systems and fail to evaluate lifelong\nlearning capabilities. We present LifelongAgentBench, the first unified\nbenchmark designed to systematically assess the lifelong learning ability of\nLLM agents. It provides skill-grounded, interdependent tasks across three\ninteractive environments, Database, Operating System, and Knowledge Graph, with\nautomatic label verification, reproducibility, and modular extensibility.\nExtensive experiments reveal that conventional experience replay has limited\neffectiveness for LLM agents due to irrelevant information and context length\nconstraints. We further introduce a group self-consistency mechanism that\nsignificantly improves lifelong learning performance. We hope\nLifelongAgentBench will advance the development of adaptive, memory-capable LLM\nagents.",
    "comment": "Project Page: https://caixd-220529.github.io/LifelongAgentBench/",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2505.11942v3' 'http://arxiv.org/pdf/2505.11942v3']",
    "pdf_url": "http://arxiv.org/pdf/2505.11942v3",
    "arxiv_id": "2505.11942",
    "row_id": 460521,
    "year": 2025,
    "processed_text": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners Lifelong learning is essential for intelligent agents operating in dynamic\nenvironments. Current large language model (LLM)-based agents, however, remain\nstateless and unable to accumulate or transfer knowledge over time. Existing\nbenchmarks treat agents as static systems and fail to evaluate lifelong\nlearning capabilities. We present LifelongAgentBench, the first unified\nbenchmark designed to systematically assess the lifelong learning ability of\nLLM agents. It provides skill-grounded, interdependent tasks across three\ninteractive environments, Database, Operating System, and Knowledge Graph, with\nautomatic label verification, reproducibility, and modular extensibility.\nExtensive experiments reveal that conventional experience replay has limited\neffectiveness for LLM agents due to irrelevant information and context length\nconstraints. We further introduce a group self-consistency mechanism that\nsignificantly improves lifelong learning performance. We hope\nLifelongAgentBench will advance the development of adaptive, memory-capable LLM\nagents.",
    "bm25_score": 10.194895509586726
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.22809v1",
    "updated": "2025-05-28T19:34:36+00:00",
    "published": "2025-05-28T19:34:36+00:00",
    "title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay",
    "authors": "['Andrew Zhu' 'Evan Osgood' 'Chris Callison-Burch']",
    "summary": "Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents.",
    "comment": "8 pages, 5 figures. In submission at EMNLP 2025",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI' 'cs.HC']",
    "links": "['http://arxiv.org/abs/2505.22809v1' 'http://arxiv.org/pdf/2505.22809v1']",
    "pdf_url": "http://arxiv.org/pdf/2505.22809v1",
    "arxiv_id": "2505.22809",
    "row_id": 461909,
    "year": 2025,
    "processed_text": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents.",
    "bm25_score": 10.109292325829367
  },
  {
    "entry_id": "http://arxiv.org/abs/2502.07709v2",
    "updated": "2025-02-12T08:52:52+00:00",
    "published": "2025-02-11T17:08:00+00:00",
    "title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces",
    "authors": "['Loris Gaven' 'Thomas Carta' 'Cl\u00e9ment Romac' 'C\u00e9dric Colas'\n 'Sylvain Lamprier' 'Olivier Sigaud' 'Pierre-Yves Oudeyer']",
    "summary": "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2502.07709v2' 'http://arxiv.org/pdf/2502.07709v2']",
    "pdf_url": "http://arxiv.org/pdf/2502.07709v2",
    "arxiv_id": "2502.07709",
    "row_id": 10310,
    "year": 2025,
    "processed_text": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
    "bm25_score": 10.080097893896468
  }
]