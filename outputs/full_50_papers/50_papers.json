[
  {
    "entry_id": "http://arxiv.org/abs/2502.01390v1",
    "updated": "2025-02-03T14:23:22+00:00",
    "published": "2025-02-03T14:23:22+00:00",
    "title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant",
    "authors": "['Gaole He' 'Gianluca Demartini' 'Ujwal Gadiraju']",
    "summary": "Since the explosion in popularity of ChatGPT, large language models (LLMs)\nhave continued to impact our everyday lives. Equipped with external tools that\nare designed for a specific purpose (e.g., for flight booking or an alarm\nclock), LLM agents exercise an increasing capability to assist humans in their\ndaily work. Although LLM agents have shown a promising blueprint as daily\nassistants, there is a limited understanding of how they can provide daily\nassistance based on planning and sequential decision making capabilities. We\ndraw inspiration from recent work that has highlighted the value of\n'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks.\nWe conducted an empirical study (N = 248) of LLM agents as daily assistants in\nsix commonly occurring tasks with different levels of risk typically associated\nwith them (e.g., flight ticket booking and credit card payments). To ensure\nuser agency and control over the LLM agent, we adopted LLM agents in a\nplan-then-execute manner, wherein the agents conducted step-wise planning and\nstep-by-step execution in a simulation environment. We analyzed how user\ninvolvement at each stage affects their trust and collaborative team\nperformance. Our findings demonstrate that LLM agents can be a double-edged\nsword -- (1) they can work well when a high-quality plan and necessary user\ninvolvement in execution are available, and (2) users can easily mistrust the\nLLM agents with plans that seem plausible. We synthesized key insights for\nusing LLM agents as daily assistants to calibrate user trust and achieve better\noverall task outcomes. Our work has important implications for the future\ndesign of daily assistants and human-AI collaboration with LLM agents.",
    "comment": "conditionally accepted to CHI 2025",
    "journal_ref": null,
    "doi": "10.1145/3706598.3713218",
    "primary_category": "cs.HC",
    "categories": "['cs.HC' 'cs.CL']",
    "links": "['http://dx.doi.org/10.1145/3706598.3713218'\n 'http://arxiv.org/abs/2502.01390v1' 'http://arxiv.org/pdf/2502.01390v1']",
    "pdf_url": "http://arxiv.org/pdf/2502.01390v1",
    "arxiv_id": "2502.01390",
    "row_id": 230264,
    "year": 2025,
    "processed_text": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant Since the explosion in popularity of ChatGPT, large language models (LLMs)\nhave continued to impact our everyday lives. Equipped with external tools that\nare designed for a specific purpose (e.g., for flight booking or an alarm\nclock), LLM agents exercise an increasing capability to assist humans in their\ndaily work. Although LLM agents have shown a promising blueprint as daily\nassistants, there is a limited understanding of how they can provide daily\nassistance based on planning and sequential decision making capabilities. We\ndraw inspiration from recent work that has highlighted the value of\n'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks.\nWe conducted an empirical study (N = 248) of LLM agents as daily assistants in\nsix commonly occurring tasks with different levels of risk typically associated\nwith them (e.g., flight ticket booking and credit card payments). To ensure\nuser agency and control over the LLM agent, we adopted LLM agents in a\nplan-then-execute manner, wherein the agents conducted step-wise planning and\nstep-by-step execution in a simulation environment. We analyzed how user\ninvolvement at each stage affects their trust and collaborative team\nperformance. Our findings demonstrate that LLM agents can be a double-edged\nsword -- (1) they can work well when a high-quality plan and necessary user\ninvolvement in execution are available, and (2) users can easily mistrust the\nLLM agents with plans that seem plausible. We synthesized key insights for\nusing LLM agents as daily assistants to calibrate user trust and achieve better\noverall task outcomes. Our work has important implications for the future\ndesign of daily assistants and human-AI collaboration with LLM agents.",
    "bm25_score": 10.851289472377129
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04559v4",
    "updated": "2024-11-01T16:10:41+00:00",
    "published": "2024-02-07T03:37:19+00:00",
    "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
    "authors": "['Chengxing Xie' 'Canyu Chen' 'Feiran Jia' 'Ziyu Ye' 'Shiyang Lai'\n 'Kai Shu' 'Jindong Gu' 'Adel Bibi' 'Ziniu Hu' 'David Jurgens'\n 'James Evans' 'Philip Torr' 'Bernard Ghanem' 'Guohao Li']",
    "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
    "comment": "Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.HC']",
    "links": "['http://arxiv.org/abs/2402.04559v4' 'http://arxiv.org/pdf/2402.04559v4']",
    "pdf_url": "http://arxiv.org/pdf/2402.04559v4",
    "arxiv_id": "2402.04559",
    "row_id": 2020,
    "year": 2024,
    "processed_text": "Can Large Language Model Agents Simulate Human Trust Behavior? Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
    "bm25_score": 10.84366068597188
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.01637v1",
    "updated": "2024-06-02T16:25:26+00:00",
    "published": "2024-06-02T16:25:26+00:00",
    "title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities",
    "authors": "['Richard Fang' 'Rohan Bindu' 'Akul Gupta' 'Qiusi Zhan' 'Daniel Kang']",
    "summary": "LLM agents have become increasingly sophisticated, especially in the realm of\ncybersecurity. Researchers have shown that LLM agents can exploit real-world\nvulnerabilities when given a description of the vulnerability and toy\ncapture-the-flag problems. However, these agents still perform poorly on\nreal-world vulnerabilities that are unknown to the agent ahead of time\n(zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world,\nzero-day vulnerabilities. Prior agents struggle with exploring many different\nvulnerabilities and long-range planning when used alone. To resolve this, we\nintroduce HPTSA, a system of agents with a planning agent that can launch\nsubagents. The planning agent explores the system and determines which\nsubagents to call, resolving long-term planning issues when trying different\nvulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and\nshow that our team of agents improve over prior work by up to 4.5$\\times$.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": "['cs.MA' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2406.01637v1' 'http://arxiv.org/pdf/2406.01637v1']",
    "pdf_url": "http://arxiv.org/pdf/2406.01637v1",
    "arxiv_id": "2406.01637",
    "row_id": 322366,
    "year": 2024,
    "processed_text": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities LLM agents have become increasingly sophisticated, especially in the realm of\ncybersecurity. Researchers have shown that LLM agents can exploit real-world\nvulnerabilities when given a description of the vulnerability and toy\ncapture-the-flag problems. However, these agents still perform poorly on\nreal-world vulnerabilities that are unknown to the agent ahead of time\n(zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world,\nzero-day vulnerabilities. Prior agents struggle with exploring many different\nvulnerabilities and long-range planning when used alone. To resolve this, we\nintroduce HPTSA, a system of agents with a planning agent that can launch\nsubagents. The planning agent explores the system and determines which\nsubagents to call, resolving long-term planning issues when trying different\nvulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and\nshow that our team of agents improve over prior work by up to 4.5$\\times$.",
    "bm25_score": 10.8220419454651
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.19213v1",
    "updated": "2025-03-24T23:39:44+00:00",
    "published": "2025-03-24T23:39:44+00:00",
    "title": "A Survey of Large Language Model Agents for Question Answering",
    "authors": "['Murong Yue']",
    "summary": "This paper surveys the development of large language model (LLM)-based agents\nfor question answering (QA). Traditional agents face significant limitations,\nincluding substantial data requirements and difficulty in generalizing to new\nenvironments. LLM-based agents address these challenges by leveraging LLMs as\ntheir core reasoning engine. These agents achieve superior QA results compared\nto traditional QA pipelines and naive LLM QA systems by enabling interaction\nwith external environments. We systematically review the design of LLM agents\nin the context of QA tasks, organizing our discussion across key stages:\nplanning, question understanding, information retrieval, and answer generation.\nAdditionally, this paper identifies ongoing challenges and explores future\nresearch directions to enhance the performance of LLM agent QA systems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI' 'cs.HC']",
    "links": "['http://arxiv.org/abs/2503.19213v1' 'http://arxiv.org/pdf/2503.19213v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.19213v1",
    "arxiv_id": "2503.19213",
    "row_id": 57360,
    "year": 2025,
    "processed_text": "A Survey of Large Language Model Agents for Question Answering This paper surveys the development of large language model (LLM)-based agents\nfor question answering (QA). Traditional agents face significant limitations,\nincluding substantial data requirements and difficulty in generalizing to new\nenvironments. LLM-based agents address these challenges by leveraging LLMs as\ntheir core reasoning engine. These agents achieve superior QA results compared\nto traditional QA pipelines and naive LLM QA systems by enabling interaction\nwith external environments. We systematically review the design of LLM agents\nin the context of QA tasks, organizing our discussion across key stages:\nplanning, question understanding, information retrieval, and answer generation.\nAdditionally, this paper identifies ongoing challenges and explores future\nresearch directions to enhance the performance of LLM agent QA systems.",
    "bm25_score": 10.509595567029404
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.07010v1",
    "updated": "2025-03-10T07:47:27+00:00",
    "published": "2025-03-10T07:47:27+00:00",
    "title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation",
    "authors": "['Kaiyuan Liu' 'Youcheng Pan' 'Jing Li' 'Daojing He' 'Yang Xiang'\n 'Yexing Du' 'Tianrun Gao']",
    "summary": "Recently, LLM agents have made rapid progress in improving their programming\ncapabilities. However, existing benchmarks lack the ability to automatically\nevaluate from users' perspective, and also lack the explainability of the\nresults of LLM agents' code generation capabilities. Thus, we introduce\nProjectEval, a new benchmark for LLM agents project-level code generation's\nautomated evaluation by simulating user interaction. ProjectEval is constructed\nby LLM with human reviewing. It has three different level inputs of natural\nlanguages or code skeletons. ProjectEval can evaluate the generated projects by\nuser interaction simulation for execution, and by code similarity through\nexisting objective indicators. Through ProjectEval, we find that systematic\nengineering project code, overall understanding of the project and\ncomprehensive analysis capability are the keys for LLM agents to achieve\npractical projects. Our findings and benchmark provide valuable insights for\ndeveloping more effective programming agents that can be deployed in future\nreal-world production.",
    "comment": "17 pages (9 Appendix pages), 4 figures, 7 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": "['cs.SE' 'cs.CL']",
    "links": "['http://arxiv.org/abs/2503.07010v1' 'http://arxiv.org/pdf/2503.07010v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.07010v1",
    "arxiv_id": "2503.07010",
    "row_id": 417331,
    "year": 2025,
    "processed_text": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation Recently, LLM agents have made rapid progress in improving their programming\ncapabilities. However, existing benchmarks lack the ability to automatically\nevaluate from users' perspective, and also lack the explainability of the\nresults of LLM agents' code generation capabilities. Thus, we introduce\nProjectEval, a new benchmark for LLM agents project-level code generation's\nautomated evaluation by simulating user interaction. ProjectEval is constructed\nby LLM with human reviewing. It has three different level inputs of natural\nlanguages or code skeletons. ProjectEval can evaluate the generated projects by\nuser interaction simulation for execution, and by code similarity through\nexisting objective indicators. Through ProjectEval, we find that systematic\nengineering project code, overall understanding of the project and\ncomprehensive analysis capability are the keys for LLM agents to achieve\npractical projects. Our findings and benchmark provide valuable insights for\ndeveloping more effective programming agents that can be deployed in future\nreal-world production.",
    "bm25_score": 10.475701699223457
  },
  {
    "entry_id": "http://arxiv.org/abs/2409.14913v2",
    "updated": "2024-09-25T08:52:49+00:00",
    "published": "2024-09-23T11:08:04+00:00",
    "title": "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents",
    "authors": "['Peter M\u00fchlbacher' 'Nikos I. Bosse' 'Lawrence Phillips']",
    "summary": "We present initial results of a forthcoming benchmark for evaluating LLM\nagents on white-collar tasks of economic value. We evaluate agents on\nreal-world \"messy\" open-web research tasks of the type that are routine in\nfinance and consulting. In doing so, we lay the groundwork for an LLM agent\nevaluation suite where good performance directly corresponds to a large\neconomic and societal impact. We built and tested several agent architectures\nwith o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.\nOn average, LLM agents powered by Claude-3.5 Sonnet and o1-preview\nsubstantially outperformed agents using GPT-4o, with agents based on Llama 3.1\n(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct\narchitecture with the ability to delegate subtasks to subagents performed best.\nIn addition to quantitative evaluations, we qualitatively assessed the\nperformance of the LLM agents by inspecting their traces and reflecting on\ntheir observations. Our evaluation represents the first in-depth assessment of\nagents' abilities to conduct challenging, economically valuable analyst-style\nresearch on the real open web.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.IR' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2409.14913v2' 'http://arxiv.org/pdf/2409.14913v2']",
    "pdf_url": "http://arxiv.org/pdf/2409.14913v2",
    "arxiv_id": "2409.14913",
    "row_id": 48817,
    "year": 2024,
    "processed_text": "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents We present initial results of a forthcoming benchmark for evaluating LLM\nagents on white-collar tasks of economic value. We evaluate agents on\nreal-world \"messy\" open-web research tasks of the type that are routine in\nfinance and consulting. In doing so, we lay the groundwork for an LLM agent\nevaluation suite where good performance directly corresponds to a large\neconomic and societal impact. We built and tested several agent architectures\nwith o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.\nOn average, LLM agents powered by Claude-3.5 Sonnet and o1-preview\nsubstantially outperformed agents using GPT-4o, with agents based on Llama 3.1\n(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct\narchitecture with the ability to delegate subtasks to subagents performed best.\nIn addition to quantitative evaluations, we qualitatively assessed the\nperformance of the LLM agents by inspecting their traces and reflecting on\ntheir observations. Our evaluation represents the first in-depth assessment of\nagents' abilities to conduct challenging, economically valuable analyst-style\nresearch on the real open web.",
    "bm25_score": 10.468543837137386
  },
  {
    "entry_id": "http://arxiv.org/abs/2403.12482v2",
    "updated": "2024-05-23T06:29:00+00:00",
    "published": "2024-03-19T06:39:47+00:00",
    "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
    "authors": "['Xudong Guo' 'Kaixuan Huang' 'Jiale Liu' 'Wenhui Fan' 'Natalia V\u00e9lez'\n 'Qingyun Wu' 'Huazheng Wang' 'Thomas L. Griffiths' 'Mengdi Wang']",
    "summary": "Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.CY' 'cs.MA']",
    "links": "['http://arxiv.org/abs/2403.12482v2' 'http://arxiv.org/pdf/2403.12482v2']",
    "pdf_url": "http://arxiv.org/pdf/2403.12482v2",
    "arxiv_id": "2403.12482",
    "row_id": 1490,
    "year": 2024,
    "processed_text": "Embodied LLM Agents Learn to Cooperate in Organized Teams Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
    "bm25_score": 10.465627248418183
  },
  {
    "entry_id": "http://arxiv.org/abs/2409.10372v3",
    "updated": "2024-10-30T16:45:15+00:00",
    "published": "2024-09-16T15:15:51+00:00",
    "title": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation",
    "authors": "['Qiliang Chen' 'Sepehr Ilami' 'Nunzio Lore' 'Babak Heydari']",
    "summary": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.CY' 'cs.GT']",
    "links": "['http://arxiv.org/abs/2409.10372v3' 'http://arxiv.org/pdf/2409.10372v3']",
    "pdf_url": "http://arxiv.org/pdf/2409.10372v3",
    "arxiv_id": "2409.10372",
    "row_id": 8152,
    "year": 2024,
    "processed_text": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
    "bm25_score": 10.429110429674427
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06596v1",
    "updated": "2024-02-09T18:19:25+00:00",
    "published": "2024-02-09T18:19:25+00:00",
    "title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment",
    "authors": "['Mingzhe Xing' 'Rongkai Zhang' 'Hui Xue' 'Qi Chen' 'Fan Yang' 'Zhen Xiao']",
    "summary": "Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.HC' 'cs.SE']",
    "links": "['http://arxiv.org/abs/2402.06596v1' 'http://arxiv.org/pdf/2402.06596v1']",
    "pdf_url": "http://arxiv.org/pdf/2402.06596v1",
    "arxiv_id": "2402.06596",
    "row_id": 1976,
    "year": 2024,
    "processed_text": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.",
    "bm25_score": 10.423962205197373
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.03007v1",
    "updated": "2024-06-05T07:14:28+00:00",
    "published": "2024-06-05T07:14:28+00:00",
    "title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents",
    "authors": "['Yifei Wang' 'Dizhan Xue' 'Shengjie Zhang' 'Shengsheng Qian']",
    "summary": "With the prosperity of large language models (LLMs), powerful LLM-based\nintelligent agents have been developed to provide customized services with a\nset of user-defined tools. State-of-the-art methods for constructing LLM agents\nadopt trained LLMs and further fine-tune them on data for the agent task.\nHowever, we show that such methods are vulnerable to our proposed backdoor\nattacks named BadAgent on various agent tasks, where a backdoor can be embedded\nby fine-tuning on the backdoor data. At test time, the attacker can manipulate\nthe deployed LLM agents to execute harmful operations by showing the trigger in\nthe agent input or environment. To our surprise, our proposed attack methods\nare extremely robust even after fine-tuning on trustworthy data. Though\nbackdoor attacks have been studied extensively in natural language processing,\nto the best of our knowledge, we could be the first to study them on LLM agents\nthat are more dangerous due to the permission to use external tools. Our work\ndemonstrates the clear risk of constructing LLM agents based on untrusted LLMs\nor data. Our code is public at https://github.com/DPamK/BadAgent",
    "comment": "Accepted by ACL 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI' 'cs.CR' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2406.03007v1' 'http://arxiv.org/pdf/2406.03007v1']",
    "pdf_url": "http://arxiv.org/pdf/2406.03007v1",
    "arxiv_id": "2406.03007",
    "row_id": 32069,
    "year": 2024,
    "processed_text": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents With the prosperity of large language models (LLMs), powerful LLM-based\nintelligent agents have been developed to provide customized services with a\nset of user-defined tools. State-of-the-art methods for constructing LLM agents\nadopt trained LLMs and further fine-tune them on data for the agent task.\nHowever, we show that such methods are vulnerable to our proposed backdoor\nattacks named BadAgent on various agent tasks, where a backdoor can be embedded\nby fine-tuning on the backdoor data. At test time, the attacker can manipulate\nthe deployed LLM agents to execute harmful operations by showing the trigger in\nthe agent input or environment. To our surprise, our proposed attack methods\nare extremely robust even after fine-tuning on trustworthy data. Though\nbackdoor attacks have been studied extensively in natural language processing,\nto the best of our knowledge, we could be the first to study them on LLM agents\nthat are more dangerous due to the permission to use external tools. Our work\ndemonstrates the clear risk of constructing LLM agents based on untrusted LLMs\nor data. Our code is public at https://github.com/DPamK/BadAgent",
    "bm25_score": 10.423872763811385
  },
  {
    "entry_id": "http://arxiv.org/abs/2410.12481v1",
    "updated": "2024-10-16T11:59:27+00:00",
    "published": "2024-10-16T11:59:27+00:00",
    "title": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling",
    "authors": "['Loris Gaven' 'Clement Romac' 'Thomas Carta' 'Sylvain Lamprier'\n 'Olivier Sigaud' 'Pierre-Yves Oudeyer']",
    "summary": "The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": "['cs.LG' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2410.12481v1' 'http://arxiv.org/pdf/2410.12481v1']",
    "pdf_url": "http://arxiv.org/pdf/2410.12481v1",
    "arxiv_id": "2410.12481",
    "row_id": 294884,
    "year": 2024,
    "processed_text": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments.",
    "bm25_score": 10.365938331371575
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.03961v1",
    "updated": "2025-05-06T20:23:25+00:00",
    "published": "2025-05-06T20:23:25+00:00",
    "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete",
    "authors": "['Gerrit Gro\u00dfmann' 'Larisa Ivanova' 'Sai Leela Poduru'\n 'Mohaddeseh Tabrizian' 'Islam Mesabah' 'David A. Selby'\n 'Sebastian J. Vollmer']",
    "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment.",
    "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.MA' 'I.2.11; I.2.7; I.6; J.4']",
    "links": "['http://arxiv.org/abs/2505.03961v1' 'http://arxiv.org/pdf/2505.03961v1']",
    "pdf_url": "http://arxiv.org/pdf/2505.03961v1",
    "arxiv_id": "2505.03961",
    "row_id": 11854,
    "year": 2025,
    "processed_text": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment.",
    "bm25_score": 10.31770003957593
  },
  {
    "entry_id": "http://arxiv.org/abs/2407.01231v1",
    "updated": "2024-07-01T12:22:46+00:00",
    "published": "2024-07-01T12:22:46+00:00",
    "title": "MIRAI: Evaluating LLM Agents for Event Forecasting",
    "authors": "['Chenchen Ye' 'Ziniu Hu' 'Yihe Deng' 'Zijie Huang' 'Mingyu Derek Ma'\n 'Yanqiao Zhu' 'Wei Wang']",
    "summary": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents\nto autonomously collect world information, over which to conduct reasoning to\nsolve complex problems. Given this capability, increasing interests have been\nput into employing LLM agents for predicting international events, which can\ninfluence decision-making and shape policy development on an international\nscale. Despite such a growing interest, there is a lack of a rigorous benchmark\nof LLM agents' forecasting capability and reliability. To address this gap, we\nintroduce MIRAI, a novel benchmark designed to systematically evaluate LLM\nagents as temporal forecasters in the context of international events. Our\nbenchmark features an agentic environment with tools for accessing an extensive\ndatabase of historical, structured events and textual news articles. We refine\nthe GDELT event database with careful cleaning and parsing to curate a series\nof relational prediction tasks with varying forecasting horizons, assessing LLM\nagents' abilities from short-term to long-term forecasting. We further\nimplement APIs to enable LLM agents to utilize different tools via a code-based\ninterface. In summary, MIRAI comprehensively evaluates the agents' capabilities\nin three dimensions: 1) autonomously source and integrate critical information\nfrom large global databases; 2) write codes using domain-specific APIs and\nlibraries for tool-use; and 3) jointly reason over historical knowledge from\ndiverse formats and time to accurately predict future events. Through\ncomprehensive benchmarking, we aim to establish a reliable framework for\nassessing the capabilities of LLM agents in forecasting international events,\nthereby contributing to the development of more accurate and trustworthy models\nfor international relation analysis.",
    "comment": "66 pages, 8 figures, 6 tables; Website: https://mirai-llm.github.io/",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2407.01231v1' 'http://arxiv.org/pdf/2407.01231v1']",
    "pdf_url": "http://arxiv.org/pdf/2407.01231v1",
    "arxiv_id": "2407.01231",
    "row_id": 30465,
    "year": 2024,
    "processed_text": "MIRAI: Evaluating LLM Agents for Event Forecasting Recent advancements in Large Language Models (LLMs) have empowered LLM agents\nto autonomously collect world information, over which to conduct reasoning to\nsolve complex problems. Given this capability, increasing interests have been\nput into employing LLM agents for predicting international events, which can\ninfluence decision-making and shape policy development on an international\nscale. Despite such a growing interest, there is a lack of a rigorous benchmark\nof LLM agents' forecasting capability and reliability. To address this gap, we\nintroduce MIRAI, a novel benchmark designed to systematically evaluate LLM\nagents as temporal forecasters in the context of international events. Our\nbenchmark features an agentic environment with tools for accessing an extensive\ndatabase of historical, structured events and textual news articles. We refine\nthe GDELT event database with careful cleaning and parsing to curate a series\nof relational prediction tasks with varying forecasting horizons, assessing LLM\nagents' abilities from short-term to long-term forecasting. We further\nimplement APIs to enable LLM agents to utilize different tools via a code-based\ninterface. In summary, MIRAI comprehensively evaluates the agents' capabilities\nin three dimensions: 1) autonomously source and integrate critical information\nfrom large global databases; 2) write codes using domain-specific APIs and\nlibraries for tool-use; and 3) jointly reason over historical knowledge from\ndiverse formats and time to accurately predict future events. Through\ncomprehensive benchmarking, we aim to establish a reliable framework for\nassessing the capabilities of LLM agents in forecasting international events,\nthereby contributing to the development of more accurate and trustworthy models\nfor international relation analysis.",
    "bm25_score": 10.301503829871013
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.18825v1",
    "updated": "2025-03-24T16:06:04+00:00",
    "published": "2025-03-24T16:06:04+00:00",
    "title": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments",
    "authors": "['Sara Fish' 'Julia Shephard' 'Minkai Li' 'Ran I. Shorrer'\n 'Yannai A. Gonczarowski']",
    "summary": "We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.GT']",
    "links": "['http://arxiv.org/abs/2503.18825v1' 'http://arxiv.org/pdf/2503.18825v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.18825v1",
    "arxiv_id": "2503.18825",
    "row_id": 11214,
    "year": 2025,
    "processed_text": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy.",
    "bm25_score": 10.282521742964668
  },
  {
    "entry_id": "http://arxiv.org/abs/2412.10270v1",
    "updated": "2024-12-13T16:45:49+00:00",
    "published": "2024-12-13T16:45:49+00:00",
    "title": "Cultural Evolution of Cooperation among LLM Agents",
    "authors": "['Aron Vallinder' 'Edward Hughes']",
    "summary": "Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society.",
    "comment": "15 pages, 6 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": "['cs.MA' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2412.10270v1' 'http://arxiv.org/pdf/2412.10270v1']",
    "pdf_url": "http://arxiv.org/pdf/2412.10270v1",
    "arxiv_id": "2412.10270",
    "row_id": 325125,
    "year": 2024,
    "processed_text": "Cultural Evolution of Cooperation among LLM Agents Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society.",
    "bm25_score": 10.267324017747404
  },
  {
    "entry_id": "http://arxiv.org/abs/2403.05307v1",
    "updated": "2024-03-08T13:34:20+00:00",
    "published": "2024-03-08T13:34:20+00:00",
    "title": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents",
    "authors": "['Jinyang Li' 'Nan Huo' 'Yan Gao' 'Jiayi Shi' 'Yingxiu Zhao' 'Ge Qu'\n 'Yurong Wu' 'Chenhao Ma' 'Jian-Guang Lou' 'Reynold Cheng']",
    "summary": "Interactive Data Analysis, the collaboration between humans and LLM agents,\nenables real-time data exploration for informed decision-making. The challenges\nand costs of collecting realistic interactive logs for data analysis hinder the\nquantitative evaluation of Large Language Model (LLM) agents in this task. To\nmitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate\nLLM agents on interactive data analysis. Tapilot-Crossing contains 1024\ninteractions, covering 4 practical scenarios: Normal, Action, Private, and\nPrivate Action. Notably, Tapilot-Crossing is constructed by an economical\nmulti-agent environment, Decision Company, with few human efforts. We evaluate\npopular and advanced LLM agents in Tapilot-Crossing, which underscores the\nchallenges of interactive data analysis. Furthermore, we propose Adaptive\nInteraction Reflection (AIR), a self-generated reflection strategy that guides\nLLM agents to learn from successful history. Experiments demonstrate that Air\ncan evolve LLMs into effective interactive data analysis agents, achieving a\nrelative performance improvement of up to 44.5%.",
    "comment": "30 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2403.05307v1' 'http://arxiv.org/pdf/2403.05307v1']",
    "pdf_url": "http://arxiv.org/pdf/2403.05307v1",
    "arxiv_id": "2403.05307",
    "row_id": 1570,
    "year": 2024,
    "processed_text": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents Interactive Data Analysis, the collaboration between humans and LLM agents,\nenables real-time data exploration for informed decision-making. The challenges\nand costs of collecting realistic interactive logs for data analysis hinder the\nquantitative evaluation of Large Language Model (LLM) agents in this task. To\nmitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate\nLLM agents on interactive data analysis. Tapilot-Crossing contains 1024\ninteractions, covering 4 practical scenarios: Normal, Action, Private, and\nPrivate Action. Notably, Tapilot-Crossing is constructed by an economical\nmulti-agent environment, Decision Company, with few human efforts. We evaluate\npopular and advanced LLM agents in Tapilot-Crossing, which underscores the\nchallenges of interactive data analysis. Furthermore, we propose Adaptive\nInteraction Reflection (AIR), a self-generated reflection strategy that guides\nLLM agents to learn from successful history. Experiments demonstrate that Air\ncan evolve LLMs into effective interactive data analysis agents, achieving a\nrelative performance improvement of up to 44.5%.",
    "bm25_score": 10.247587450606837
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.01908v1",
    "updated": "2025-02-28T21:30:28+00:00",
    "published": "2025-02-28T21:30:28+00:00",
    "title": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning",
    "authors": "['Jiawei Zhang' 'Shuang Yang' 'Bo Li']",
    "summary": "Large Language Model (LLM) agents equipped with external tools have become\nincreasingly powerful for handling complex tasks such as web shopping,\nautomated email replies, and financial trading. However, these advancements\nalso amplify the risks of adversarial attacks, particularly when LLM agents can\naccess sensitive external functionalities. Moreover, because LLM agents engage\nin extensive reasoning or planning before executing final actions, manipulating\nthem into performing targeted malicious actions or invoking specific tools\nremains a significant challenge. Consequently, directly embedding adversarial\nstrings in malicious instructions or injecting malicious prompts into tool\ninteractions has become less effective against modern LLM agents. In this work,\nwe present UDora, a unified red teaming framework designed for LLM Agents that\ndynamically leverages the agent's own reasoning processes to compel it toward\nmalicious behavior. Specifically, UDora first samples the model's reasoning for\nthe given task, then automatically identifies multiple optimal positions within\nthese reasoning traces to insert targeted perturbations. Subsequently, it uses\nthe modified reasoning as the objective to optimize the adversarial strings. By\niteratively applying this process, the LLM agent will then be induced to\nundertake designated malicious actions or to invoke specific malicious tools.\nOur approach demonstrates superior effectiveness compared to existing methods\nacross three LLM agent datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": "['cs.CR' 'cs.AI' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2503.01908v1' 'http://arxiv.org/pdf/2503.01908v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.01908v1",
    "arxiv_id": "2503.01908",
    "row_id": 83930,
    "year": 2025,
    "processed_text": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning Large Language Model (LLM) agents equipped with external tools have become\nincreasingly powerful for handling complex tasks such as web shopping,\nautomated email replies, and financial trading. However, these advancements\nalso amplify the risks of adversarial attacks, particularly when LLM agents can\naccess sensitive external functionalities. Moreover, because LLM agents engage\nin extensive reasoning or planning before executing final actions, manipulating\nthem into performing targeted malicious actions or invoking specific tools\nremains a significant challenge. Consequently, directly embedding adversarial\nstrings in malicious instructions or injecting malicious prompts into tool\ninteractions has become less effective against modern LLM agents. In this work,\nwe present UDora, a unified red teaming framework designed for LLM Agents that\ndynamically leverages the agent's own reasoning processes to compel it toward\nmalicious behavior. Specifically, UDora first samples the model's reasoning for\nthe given task, then automatically identifies multiple optimal positions within\nthese reasoning traces to insert targeted perturbations. Subsequently, it uses\nthe modified reasoning as the objective to optimize the adversarial strings. By\niteratively applying this process, the LLM agent will then be induced to\nundertake designated malicious actions or to invoke specific malicious tools.\nOur approach demonstrates superior effectiveness compared to existing methods\nacross three LLM agent datasets.",
    "bm25_score": 10.240553200525351
  },
  {
    "entry_id": "http://arxiv.org/abs/2409.09345v1",
    "updated": "2024-09-14T07:32:49+00:00",
    "published": "2024-09-14T07:32:49+00:00",
    "title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models",
    "authors": "['Yuanzhao Zhai' 'Tingkai Yang' 'Kele Xu' 'Feng Dawei' 'Cheng Yang'\n 'Bo Ding' 'Huaimin Wang']",
    "summary": "Agents significantly enhance the capabilities of standalone Large Language\nModels (LLMs) by perceiving environments, making decisions, and executing\nactions. However, LLM agents still face challenges in tasks that require\nmultiple decision-making steps. Estimating the value of actions in specific\ntasks is difficult when intermediate actions are neither appropriately rewarded\nnor penalized. In this paper, we propose leveraging a task-relevant Q-value\nmodel to guide action selection. Specifically, we first collect decision-making\ntrajectories annotated with step-level Q values via Monte Carlo Tree Search\n(MCTS) and construct preference data. We then use another LLM to fit these\npreferences through step-level Direct Policy Optimization (DPO), which serves\nas the Q-value model. During inference, at each decision-making step, LLM\nagents select the action with the highest Q value before interacting with the\nenvironment. We apply our method to various open-source and API-based LLM\nagents, demonstrating that Q-value models significantly improve their\nperformance. Notably, the performance of the agent built with\nPhi-3-mini-4k-instruct improved by 103% on WebShop and 75% on HotPotQA when\nenhanced with Q-value models, even surpassing GPT-4o-mini. Additionally,\nQ-value models offer several advantages, such as generalization to different\nLLM agents and seamless integration with existing prompting strategies.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2409.09345v1' 'http://arxiv.org/pdf/2409.09345v1']",
    "pdf_url": "http://arxiv.org/pdf/2409.09345v1",
    "arxiv_id": "2409.09345",
    "row_id": 8161,
    "year": 2024,
    "processed_text": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models Agents significantly enhance the capabilities of standalone Large Language\nModels (LLMs) by perceiving environments, making decisions, and executing\nactions. However, LLM agents still face challenges in tasks that require\nmultiple decision-making steps. Estimating the value of actions in specific\ntasks is difficult when intermediate actions are neither appropriately rewarded\nnor penalized. In this paper, we propose leveraging a task-relevant Q-value\nmodel to guide action selection. Specifically, we first collect decision-making\ntrajectories annotated with step-level Q values via Monte Carlo Tree Search\n(MCTS) and construct preference data. We then use another LLM to fit these\npreferences through step-level Direct Policy Optimization (DPO), which serves\nas the Q-value model. During inference, at each decision-making step, LLM\nagents select the action with the highest Q value before interacting with the\nenvironment. We apply our method to various open-source and API-based LLM\nagents, demonstrating that Q-value models significantly improve their\nperformance. Notably, the performance of the agent built with\nPhi-3-mini-4k-instruct improved by 103% on WebShop and 75% on HotPotQA when\nenhanced with Q-value models, even surpassing GPT-4o-mini. Additionally,\nQ-value models offer several advantages, such as generalization to different\nLLM agents and seamless integration with existing prompting strategies.",
    "bm25_score": 10.234411292526264
  },
  {
    "entry_id": "http://arxiv.org/abs/2309.07870v3",
    "updated": "2023-12-12T04:47:21+00:00",
    "published": "2023-09-14T17:18:25+00:00",
    "title": "Agents: An Open-source Framework for Autonomous Language Agents",
    "authors": "['Wangchunshu Zhou' 'Yuchen Eleanor Jiang' 'Long Li' 'Jialong Wu'\n 'Tiannan Wang' 'Shi Qiu' 'Jintian Zhang' 'Jing Chen' 'Ruipu Wu'\n 'Shuai Wang' 'Shiding Zhu' 'Jiyu Chen' 'Wentao Zhang' 'Xiangru Tang'\n 'Ningyu Zhang' 'Huajun Chen' 'Peng Cui' 'Mrinmaya Sachan']",
    "summary": "Recent advances on large language models (LLMs) enable researchers and\ndevelopers to build autonomous language agents that can automatically solve\nvarious tasks and interact with environments, humans, and other agents using\nnatural language interfaces. We consider language agents as a promising\ndirection towards artificial general intelligence and release Agents, an\nopen-source library with the goal of opening up these advances to a wider\nnon-specialist audience. Agents is carefully engineered to support important\nfeatures including planning, memory, tool usage, multi-agent communication, and\nfine-grained symbolic control. Agents is user-friendly as it enables\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\nautonomous language agents without much coding. The library is also\nresearch-friendly as its modularized design makes it easily extensible for\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.",
    "comment": "Code available at https://github.com/aiwaves-cn/agents",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL']",
    "links": "['http://arxiv.org/abs/2309.07870v3' 'http://arxiv.org/pdf/2309.07870v3']",
    "pdf_url": "http://arxiv.org/pdf/2309.07870v3",
    "arxiv_id": "2309.07870",
    "row_id": 42234,
    "year": 2023,
    "processed_text": "Agents: An Open-source Framework for Autonomous Language Agents Recent advances on large language models (LLMs) enable researchers and\ndevelopers to build autonomous language agents that can automatically solve\nvarious tasks and interact with environments, humans, and other agents using\nnatural language interfaces. We consider language agents as a promising\ndirection towards artificial general intelligence and release Agents, an\nopen-source library with the goal of opening up these advances to a wider\nnon-specialist audience. Agents is carefully engineered to support important\nfeatures including planning, memory, tool usage, multi-agent communication, and\nfine-grained symbolic control. Agents is user-friendly as it enables\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\nautonomous language agents without much coding. The library is also\nresearch-friendly as its modularized design makes it easily extensible for\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.",
    "bm25_score": 8.666531546032655
  },
  {
    "entry_id": "http://arxiv.org/abs/2410.10020v1",
    "updated": "2024-10-13T21:45:16+00:00",
    "published": "2024-10-13T21:45:16+00:00",
    "title": "Adaptive Reasoning and Acting in Medical Language Agents",
    "authors": "['Abhishek Dutta' 'Yen-Che Hsiao']",
    "summary": "This paper presents an innovative large language model (LLM) agent framework\nfor enhancing diagnostic accuracy in simulated clinical environments using the\nAgentClinic benchmark. The proposed automatic correction enables doctor agents\nto iteratively refine their reasoning and actions following incorrect\ndiagnoses, fostering improved decision-making over time. Experiments show that\nthe implementation of the adaptive LLM-based doctor agents achieve correct\ndiagnoses through dynamic interactions with simulated patients. The evaluations\nhighlight the capacity of autonomous agents to adapt and improve in complex\nmedical scenarios. Future enhancements will focus on refining the algorithm and\nexpanding its applicability across a wider range of tasks and different large\nlanguage models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2410.10020v1' 'http://arxiv.org/pdf/2410.10020v1']",
    "pdf_url": "http://arxiv.org/pdf/2410.10020v1",
    "arxiv_id": "2410.10020",
    "row_id": 7697,
    "year": 2024,
    "processed_text": "Adaptive Reasoning and Acting in Medical Language Agents This paper presents an innovative large language model (LLM) agent framework\nfor enhancing diagnostic accuracy in simulated clinical environments using the\nAgentClinic benchmark. The proposed automatic correction enables doctor agents\nto iteratively refine their reasoning and actions following incorrect\ndiagnoses, fostering improved decision-making over time. Experiments show that\nthe implementation of the adaptive LLM-based doctor agents achieve correct\ndiagnoses through dynamic interactions with simulated patients. The evaluations\nhighlight the capacity of autonomous agents to adapt and improve in complex\nmedical scenarios. Future enhancements will focus on refining the algorithm and\nexpanding its applicability across a wider range of tasks and different large\nlanguage models.",
    "bm25_score": 8.558542526428852
  },
  {
    "entry_id": "http://arxiv.org/abs/2310.18940v3",
    "updated": "2024-02-20T01:21:23+00:00",
    "published": "2023-10-29T09:02:57+00:00",
    "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game",
    "authors": "['Zelai Xu' 'Chao Yu' 'Fei Fang' 'Yu Wang' 'Yi Wu']",
    "summary": "Agents built with large language models (LLMs) have shown great potential\nacross a wide range of domains. However, in complex decision-making tasks, pure\nLLM-based agents tend to exhibit intrinsic bias in their choice of actions,\nwhich is inherited from the model's training data and results in suboptimal\nperformance. To develop strategic language agents, i.e., agents that generate\nflexible language actions and possess strong decision-making abilities, we\npropose a novel framework that powers LLM-based agents with reinforcement\nlearning (RL). We consider Werewolf, a popular social deduction game, as a\nchallenging testbed that emphasizes versatile communication and strategic\ngameplay. To mitigate the intrinsic bias in language actions, our agents use an\nLLM to perform deductive reasoning and generate a diverse set of action\ncandidates. Then an RL policy trained to optimize the decision-making ability\nchooses an action from the candidates to play in the game. Extensive\nexperiments show that our agents overcome the intrinsic bias and outperform\nexisting LLM-based agents in the Werewolf game. We also conduct human-agent\nexperiments and find that our agents achieve human-level performance and\ndemonstrate strong strategic play.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.LG' 'cs.MA']",
    "links": "['http://arxiv.org/abs/2310.18940v3' 'http://arxiv.org/pdf/2310.18940v3']",
    "pdf_url": "http://arxiv.org/pdf/2310.18940v3",
    "arxiv_id": "2310.18940",
    "row_id": 3016,
    "year": 2024,
    "processed_text": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game Agents built with large language models (LLMs) have shown great potential\nacross a wide range of domains. However, in complex decision-making tasks, pure\nLLM-based agents tend to exhibit intrinsic bias in their choice of actions,\nwhich is inherited from the model's training data and results in suboptimal\nperformance. To develop strategic language agents, i.e., agents that generate\nflexible language actions and possess strong decision-making abilities, we\npropose a novel framework that powers LLM-based agents with reinforcement\nlearning (RL). We consider Werewolf, a popular social deduction game, as a\nchallenging testbed that emphasizes versatile communication and strategic\ngameplay. To mitigate the intrinsic bias in language actions, our agents use an\nLLM to perform deductive reasoning and generate a diverse set of action\ncandidates. Then an RL policy trained to optimize the decision-making ability\nchooses an action from the candidates to play in the game. Extensive\nexperiments show that our agents overcome the intrinsic bias and outperform\nexisting LLM-based agents in the Werewolf game. We also conduct human-agent\nexperiments and find that our agents achieve human-level performance and\ndemonstrate strong strategic play.",
    "bm25_score": 8.308191806629823
  },
  {
    "entry_id": "http://arxiv.org/abs/2309.02427v3",
    "updated": "2024-03-15T15:44:11+00:00",
    "published": "2023-09-05T17:56:20+00:00",
    "title": "Cognitive Architectures for Language Agents",
    "authors": "['Theodore R. Sumers' 'Shunyu Yao' 'Karthik Narasimhan'\n 'Thomas L. Griffiths']",
    "summary": "Recent efforts have augmented large language models (LLMs) with external\nresources (e.g., the Internet) or internal control flows (e.g., prompt\nchaining) for tasks requiring grounding or reasoning, leading to a new class of\nlanguage agents. While these agents have achieved substantial empirical\nsuccess, we lack a systematic framework to organize existing agents and plan\nfuture developments. In this paper, we draw on the rich history of cognitive\nscience and symbolic artificial intelligence to propose Cognitive Architectures\nfor Language Agents (CoALA). CoALA describes a language agent with modular\nmemory components, a structured action space to interact with internal memory\nand external environments, and a generalized decision-making process to choose\nactions. We use CoALA to retrospectively survey and organize a large body of\nrecent work, and prospectively identify actionable directions towards more\ncapable agents. Taken together, CoALA contextualizes today's language agents\nwithin the broader history of AI and outlines a path towards language-based\ngeneral intelligence.",
    "comment": "v3 is TMLR camera ready version. 19 pages of main content, 5 figures.\n  The first two authors contributed equally, order decided by coin flip. A\n  CoALA-based repo of recent work on language agents:\n  https://github.com/ysymyth/awesome-language-agents",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.LG' 'cs.SC']",
    "links": "['http://arxiv.org/abs/2309.02427v3' 'http://arxiv.org/pdf/2309.02427v3']",
    "pdf_url": "http://arxiv.org/pdf/2309.02427v3",
    "arxiv_id": "2309.02427",
    "row_id": 3583,
    "year": 2024,
    "processed_text": "Cognitive Architectures for Language Agents Recent efforts have augmented large language models (LLMs) with external\nresources (e.g., the Internet) or internal control flows (e.g., prompt\nchaining) for tasks requiring grounding or reasoning, leading to a new class of\nlanguage agents. While these agents have achieved substantial empirical\nsuccess, we lack a systematic framework to organize existing agents and plan\nfuture developments. In this paper, we draw on the rich history of cognitive\nscience and symbolic artificial intelligence to propose Cognitive Architectures\nfor Language Agents (CoALA). CoALA describes a language agent with modular\nmemory components, a structured action space to interact with internal memory\nand external environments, and a generalized decision-making process to choose\nactions. We use CoALA to retrospectively survey and organize a large body of\nrecent work, and prospectively identify actionable directions towards more\ncapable agents. Taken together, CoALA contextualizes today's language agents\nwithin the broader history of AI and outlines a path towards language-based\ngeneral intelligence.",
    "bm25_score": 8.27735067053922
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.07320v1",
    "updated": "2025-03-10T13:37:36+00:00",
    "published": "2025-03-10T13:37:36+00:00",
    "title": "Experimental Exploration: Investigating Cooperative Interaction Behavior Between Humans and Large Language Model Agents",
    "authors": "['Guanxuan Jiang' 'Yuyang Wang' 'Pan Hui']",
    "summary": "With the rise of large language models (LLMs), AI agents as autonomous\ndecision-makers present significant opportunities and challenges for human-AI\ncooperation. While many studies have explored human cooperation with AI as\ntools, the role of LLM-augmented autonomous agents in competitive-cooperative\ninteractions remains under-examined. This study investigates human cooperative\nbehavior by engaging 30 participants who interacted with LLM agents exhibiting\ndifferent characteristics (purported human, purported rule-based AI agent, and\nLLM agent) in repeated Prisoner's Dilemma games. Findings show significant\ndifferences in cooperative behavior based on the agents' purported\ncharacteristics and the interaction effect of participants' genders and\npurported characteristics. We also analyzed human response patterns, including\ngame completion time, proactive favorable behavior, and acceptance of repair\nefforts. These insights offer a new perspective on human interactions with LLM\nagents in competitive cooperation contexts, such as virtual avatars or future\nphysical entities. The study underscores the importance of understanding human\nbiases toward AI agents and how observed behaviors can influence future\nhuman-AI cooperation dynamics.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": "['cs.HC' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2503.07320v1' 'http://arxiv.org/pdf/2503.07320v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.07320v1",
    "arxiv_id": "2503.07320",
    "row_id": 230549,
    "year": 2025,
    "processed_text": "Experimental Exploration: Investigating Cooperative Interaction Behavior Between Humans and Large Language Model Agents With the rise of large language models (LLMs), AI agents as autonomous\ndecision-makers present significant opportunities and challenges for human-AI\ncooperation. While many studies have explored human cooperation with AI as\ntools, the role of LLM-augmented autonomous agents in competitive-cooperative\ninteractions remains under-examined. This study investigates human cooperative\nbehavior by engaging 30 participants who interacted with LLM agents exhibiting\ndifferent characteristics (purported human, purported rule-based AI agent, and\nLLM agent) in repeated Prisoner's Dilemma games. Findings show significant\ndifferences in cooperative behavior based on the agents' purported\ncharacteristics and the interaction effect of participants' genders and\npurported characteristics. We also analyzed human response patterns, including\ngame completion time, proactive favorable behavior, and acceptance of repair\nefforts. These insights offer a new perspective on human interactions with LLM\nagents in competitive cooperation contexts, such as virtual avatars or future\nphysical entities. The study underscores the importance of understanding human\nbiases toward AI agents and how observed behaviors can influence future\nhuman-AI cooperation dynamics.",
    "bm25_score": 8.251104031104425
  },
  {
    "entry_id": "http://arxiv.org/abs/2308.02151v3",
    "updated": "2024-05-05T05:04:49+00:00",
    "published": "2023-08-04T06:14:23+00:00",
    "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
    "authors": "['Weiran Yao' 'Shelby Heinecke' 'Juan Carlos Niebles' 'Zhiwei Liu'\n 'Yihao Feng' 'Le Xue' 'Rithesh Murthy' 'Zeyuan Chen' 'Jianguo Zhang'\n 'Devansh Arpit' 'Ran Xu' 'Phil Mui' 'Huan Wang' 'Caiming Xiong'\n 'Silvio Savarese']",
    "summary": "Recent months have seen the emergence of a powerful new trend in which large\nlanguage models (LLMs) are augmented to become autonomous language agents\ncapable of performing objective oriented multi-step tasks on their own, rather\nthan merely responding to queries from human users. Most existing language\nagents, however, are not optimized using environment-specific rewards. Although\nsome agents enable iterative refinement through verbal feedback, they do not\nreason and plan in ways that are compatible with gradient-based learning from\nrewards. This paper introduces a principled framework for reinforcing large\nlanguage agents by learning a retrospective model, which automatically tunes\nthe language agent prompts from environment feedback through policy gradient.\nSpecifically, our proposed agent architecture learns from rewards across\nmultiple environments and tasks, for fine-tuning a pre-trained language model\nwhich refines the language agent prompt by summarizing the root cause of prior\nfailed attempts and proposing action plans. Experimental results on various\ntasks demonstrate that the language agents improve over time and that our\napproach considerably outperforms baselines that do not properly leverage\ngradients from the environment. This demonstrates that using policy gradient\noptimization to improve language agents, for which we believe our work is one\nof the first, seems promising and can be applied to optimize other models in\nthe agent architecture to enhance agent performances over time.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2308.02151v3' 'http://arxiv.org/pdf/2308.02151v3']",
    "pdf_url": "http://arxiv.org/pdf/2308.02151v3",
    "arxiv_id": "2308.02151",
    "row_id": 43109,
    "year": 2024,
    "processed_text": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization Recent months have seen the emergence of a powerful new trend in which large\nlanguage models (LLMs) are augmented to become autonomous language agents\ncapable of performing objective oriented multi-step tasks on their own, rather\nthan merely responding to queries from human users. Most existing language\nagents, however, are not optimized using environment-specific rewards. Although\nsome agents enable iterative refinement through verbal feedback, they do not\nreason and plan in ways that are compatible with gradient-based learning from\nrewards. This paper introduces a principled framework for reinforcing large\nlanguage agents by learning a retrospective model, which automatically tunes\nthe language agent prompts from environment feedback through policy gradient.\nSpecifically, our proposed agent architecture learns from rewards across\nmultiple environments and tasks, for fine-tuning a pre-trained language model\nwhich refines the language agent prompt by summarizing the root cause of prior\nfailed attempts and proposing action plans. Experimental results on various\ntasks demonstrate that the language agents improve over time and that our\napproach considerably outperforms baselines that do not properly leverage\ngradients from the environment. This demonstrates that using policy gradient\noptimization to improve language agents, for which we believe our work is one\nof the first, seems promising and can be applied to optimize other models in\nthe agent architecture to enhance agent performances over time.",
    "bm25_score": 8.23858445021379
  },
  {
    "entry_id": "http://arxiv.org/abs/2401.03428v1",
    "updated": "2024-01-07T09:08:24+00:00",
    "published": "2024-01-07T09:08:24+00:00",
    "title": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",
    "authors": "['Yuheng Cheng' 'Ceyao Zhang' 'Zhengwen Zhang' 'Xiangrui Meng'\n 'Sirui Hong' 'Wenhao Li' 'Zihao Wang' 'Zekai Wang' 'Feng Yin'\n 'Junhua Zhao' 'Xiuqiang He']",
    "summary": "Intelligent agents stand out as a potential path toward artificial general\nintelligence (AGI). Thus, researchers have dedicated significant effort to\ndiverse implementations for them. Benefiting from recent progress in large\nlanguage models (LLMs), LLM-based agents that use universal natural language as\nan interface exhibit robust generalization capabilities across various\napplications -- from serving as autonomous general-purpose task assistants to\napplications in coding, social, and economic domains, LLM-based agents offer\nextensive exploration opportunities. This paper surveys current research to\nprovide an in-depth overview of LLM-based intelligent agents within\nsingle-agent and multi-agent systems. It covers their definitions, research\nframeworks, and foundational components such as their composition, cognitive\nand planning methods, tool utilization, and responses to environmental\nfeedback. We also delve into the mechanisms of deploying LLM-based agents in\nmulti-agent systems, including multi-role collaboration, message passing, and\nstrategies to alleviate communication issues between agents. The discussions\nalso shed light on popular datasets and application scenarios. We conclude by\nenvisioning prospects for LLM-based agents, considering the evolving landscape\nof AI and natural language processing.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.MA']",
    "links": "['http://arxiv.org/abs/2401.03428v1' 'http://arxiv.org/pdf/2401.03428v1']",
    "pdf_url": "http://arxiv.org/pdf/2401.03428v1",
    "arxiv_id": "2401.03428",
    "row_id": 2275,
    "year": 2024,
    "processed_text": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects Intelligent agents stand out as a potential path toward artificial general\nintelligence (AGI). Thus, researchers have dedicated significant effort to\ndiverse implementations for them. Benefiting from recent progress in large\nlanguage models (LLMs), LLM-based agents that use universal natural language as\nan interface exhibit robust generalization capabilities across various\napplications -- from serving as autonomous general-purpose task assistants to\napplications in coding, social, and economic domains, LLM-based agents offer\nextensive exploration opportunities. This paper surveys current research to\nprovide an in-depth overview of LLM-based intelligent agents within\nsingle-agent and multi-agent systems. It covers their definitions, research\nframeworks, and foundational components such as their composition, cognitive\nand planning methods, tool utilization, and responses to environmental\nfeedback. We also delve into the mechanisms of deploying LLM-based agents in\nmulti-agent systems, including multi-role collaboration, message passing, and\nstrategies to alleviate communication issues between agents. The discussions\nalso shed light on popular datasets and application scenarios. We conclude by\nenvisioning prospects for LLM-based agents, considering the evolving landscape\nof AI and natural language processing.",
    "bm25_score": 8.209292120097373
  },
  {
    "entry_id": "http://arxiv.org/abs/2403.03141v1",
    "updated": "2024-03-05T17:26:41+00:00",
    "published": "2024-03-05T17:26:41+00:00",
    "title": "Language Guided Exploration for RL Agents in Text Environments",
    "authors": "['Hitesh Golchha' 'Sahil Yerawar' 'Dhruvesh Patel' 'Soham Dan'\n 'Keerthiram Murugesan']",
    "summary": "Real-world sequential decision making is characterized by sparse rewards and\nlarge decision spaces, posing significant difficulty for experiential learning\nsystems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large\nLanguage Models (LLMs), with a wealth of world knowledge, can help RL agents\nlearn quickly and adapt to distribution shifts. In this work, we introduce\nLanguage Guided Exploration (LGE) framework, which uses a pre-trained language\nmodel (called GUIDE ) to provide decision-level guidance to an RL agent (called\nEXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging\ntext environment, LGE outperforms vanilla RL agents significantly and also\noutperforms other sophisticated methods like Behaviour Cloning and Text\nDecision Transformer.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL']",
    "links": "['http://arxiv.org/abs/2403.03141v1' 'http://arxiv.org/pdf/2403.03141v1']",
    "pdf_url": "http://arxiv.org/pdf/2403.03141v1",
    "arxiv_id": "2403.03141",
    "row_id": 35664,
    "year": 2024,
    "processed_text": "Language Guided Exploration for RL Agents in Text Environments Real-world sequential decision making is characterized by sparse rewards and\nlarge decision spaces, posing significant difficulty for experiential learning\nsystems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large\nLanguage Models (LLMs), with a wealth of world knowledge, can help RL agents\nlearn quickly and adapt to distribution shifts. In this work, we introduce\nLanguage Guided Exploration (LGE) framework, which uses a pre-trained language\nmodel (called GUIDE ) to provide decision-level guidance to an RL agent (called\nEXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging\ntext environment, LGE outperforms vanilla RL agents significantly and also\noutperforms other sophisticated methods like Behaviour Cloning and Text\nDecision Transformer.",
    "bm25_score": 8.186555219556935
  },
  {
    "entry_id": "http://arxiv.org/abs/2310.10634v1",
    "updated": "2023-10-16T17:54:53+00:00",
    "published": "2023-10-16T17:54:53+00:00",
    "title": "OpenAgents: An Open Platform for Language Agents in the Wild",
    "authors": "['Tianbao Xie' 'Fan Zhou' 'Zhoujun Cheng' 'Peng Shi' 'Luoxuan Weng'\n 'Yitao Liu' 'Toh Jing Hua' 'Junning Zhao' 'Qian Liu' 'Che Liu'\n 'Leo Z. Liu' 'Yiheng Xu' 'Hongjin Su' 'Dongchan Shin' 'Caiming Xiong'\n 'Tao Yu']",
    "summary": "Language agents show potential in being capable of utilizing natural language\nfor varied and intricate tasks in diverse environments, particularly when built\nupon large language models (LLMs). Current language agent frameworks aim to\nfacilitate the construction of proof-of-concept language agents while\nneglecting the non-expert user access to agents and paying little attention to\napplication-level designs. We present OpenAgents, an open platform for using\nand hosting language agents in the wild of everyday life. OpenAgents includes\nthree agents: (1) Data Agent for data analysis with Python/SQL and data tools;\n(2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web\nbrowsing. OpenAgents enables general users to interact with agent\nfunctionalities through a web user interface optimized for swift responses and\ncommon failures while offering developers and researchers a seamless deployment\nexperience on local setups, providing a foundation for crafting innovative\nlanguage agents and facilitating real-world evaluations. We elucidate the\nchallenges and opportunities, aspiring to set a foundation for future research\nand development of real-world language agents.",
    "comment": "34 pages, 8 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2310.10634v1' 'http://arxiv.org/pdf/2310.10634v1']",
    "pdf_url": "http://arxiv.org/pdf/2310.10634v1",
    "arxiv_id": "2310.10634",
    "row_id": 41066,
    "year": 2023,
    "processed_text": "OpenAgents: An Open Platform for Language Agents in the Wild Language agents show potential in being capable of utilizing natural language\nfor varied and intricate tasks in diverse environments, particularly when built\nupon large language models (LLMs). Current language agent frameworks aim to\nfacilitate the construction of proof-of-concept language agents while\nneglecting the non-expert user access to agents and paying little attention to\napplication-level designs. We present OpenAgents, an open platform for using\nand hosting language agents in the wild of everyday life. OpenAgents includes\nthree agents: (1) Data Agent for data analysis with Python/SQL and data tools;\n(2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web\nbrowsing. OpenAgents enables general users to interact with agent\nfunctionalities through a web user interface optimized for swift responses and\ncommon failures while offering developers and researchers a seamless deployment\nexperience on local setups, providing a foundation for crafting innovative\nlanguage agents and facilitating real-world evaluations. We elucidate the\nchallenges and opportunities, aspiring to set a foundation for future research\nand development of real-world language agents.",
    "bm25_score": 8.11318416787385
  },
  {
    "entry_id": "http://arxiv.org/abs/2409.02977v1",
    "updated": "2024-09-04T15:59:41+00:00",
    "published": "2024-09-04T15:59:41+00:00",
    "title": "Large Language Model-Based Agents for Software Engineering: A Survey",
    "authors": "['Junwei Liu' 'Kaixin Wang' 'Yixuan Chen' 'Xin Peng' 'Zhenpeng Chen'\n 'Lingming Zhang' 'Yiling Lou']",
    "summary": "The recent advance in Large Language Models (LLMs) has shaped a new paradigm\nof AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based\nagents substantially extend the versatility and expertise of LLMs by enhancing\nLLMs with the capabilities of perceiving and utilizing external resources and\ntools. To date, LLM-based agents have been applied and shown remarkable\neffectiveness in Software Engineering (SE). The synergy between multiple agents\nand human interaction brings further promise in tackling complex real-world SE\nproblems. In this work, we present a comprehensive and systematic survey on\nLLM-based agents for SE. We collect 106 papers and categorize them from two\nperspectives, i.e., the SE and agent perspectives. In addition, we discuss open\nchallenges and future directions in this critical domain. The repository of\nthis survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": "['cs.SE' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2409.02977v1' 'http://arxiv.org/pdf/2409.02977v1']",
    "pdf_url": "http://arxiv.org/pdf/2409.02977v1",
    "arxiv_id": "2409.02977",
    "row_id": 415955,
    "year": 2024,
    "processed_text": "Large Language Model-Based Agents for Software Engineering: A Survey The recent advance in Large Language Models (LLMs) has shaped a new paradigm\nof AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based\nagents substantially extend the versatility and expertise of LLMs by enhancing\nLLMs with the capabilities of perceiving and utilizing external resources and\ntools. To date, LLM-based agents have been applied and shown remarkable\neffectiveness in Software Engineering (SE). The synergy between multiple agents\nand human interaction brings further promise in tackling complex real-world SE\nproblems. In this work, we present a comprehensive and systematic survey on\nLLM-based agents for SE. We collect 106 papers and categorize them from two\nperspectives, i.e., the SE and agent perspectives. In addition, we discuss open\nchallenges and future directions in this critical domain. The repository of\nthis survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.",
    "bm25_score": 8.11023615843401
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.18532v1",
    "updated": "2024-06-26T17:59:18+00:00",
    "published": "2024-06-26T17:59:18+00:00",
    "title": "Symbolic Learning Enables Self-Evolving Agents",
    "authors": "['Wangchunshu Zhou' 'Yixin Ou' 'Shengwei Ding' 'Long Li' 'Jialong Wu'\n 'Tiannan Wang' 'Jiamin Chen' 'Shuai Wang' 'Xiaohua Xu' 'Ningyu Zhang'\n 'Huajun Chen' 'Yuchen Eleanor Jiang']",
    "summary": "The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".",
    "comment": "Code available at https://github.com/aiwaves-cn/agents",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2406.18532v1' 'http://arxiv.org/pdf/2406.18532v1']",
    "pdf_url": "http://arxiv.org/pdf/2406.18532v1",
    "arxiv_id": "2406.18532",
    "row_id": 30653,
    "year": 2024,
    "processed_text": "Symbolic Learning Enables Self-Evolving Agents The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".",
    "bm25_score": 8.093571610067801
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.01893v2",
    "updated": "2024-06-21T14:54:46+00:00",
    "published": "2024-06-04T01:57:37+00:00",
    "title": "Large Language Model-Enabled Multi-Agent Manufacturing Systems",
    "authors": "['Jonghan Lim' 'Birgit Vogel-Heuser' 'Ilya Kovalenko']",
    "summary": "Traditional manufacturing faces challenges adapting to dynamic environments\nand quickly responding to manufacturing changes. The use of multi-agent systems\nhas improved adaptability and coordination but requires further advancements in\nrapid human instruction comprehension, operational adaptability, and\ncoordination through natural language integration. Large language models like\nGPT-3.5 and GPT-4 enhance multi-agent manufacturing systems by enabling agents\nto communicate in natural language and interpret human instructions for\ndecision-making. This research introduces a novel framework where large\nlanguage models enhance the capabilities of agents in manufacturing, making\nthem more adaptable, and capable of processing context-specific instructions. A\ncase study demonstrates the practical application of this framework, showing\nhow agents can effectively communicate, understand tasks, and execute\nmanufacturing processes, including precise G-code allocation among agents. The\nfindings highlight the importance of continuous large language model\nintegration into multi-agent manufacturing systems and the development of\nsophisticated agent communication protocols for a more flexible manufacturing\nsystem.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": "['cs.MA' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2406.01893v2' 'http://arxiv.org/pdf/2406.01893v2']",
    "pdf_url": "http://arxiv.org/pdf/2406.01893v2",
    "arxiv_id": "2406.01893",
    "row_id": 322362,
    "year": 2024,
    "processed_text": "Large Language Model-Enabled Multi-Agent Manufacturing Systems Traditional manufacturing faces challenges adapting to dynamic environments\nand quickly responding to manufacturing changes. The use of multi-agent systems\nhas improved adaptability and coordination but requires further advancements in\nrapid human instruction comprehension, operational adaptability, and\ncoordination through natural language integration. Large language models like\nGPT-3.5 and GPT-4 enhance multi-agent manufacturing systems by enabling agents\nto communicate in natural language and interpret human instructions for\ndecision-making. This research introduces a novel framework where large\nlanguage models enhance the capabilities of agents in manufacturing, making\nthem more adaptable, and capable of processing context-specific instructions. A\ncase study demonstrates the practical application of this framework, showing\nhow agents can effectively communicate, understand tasks, and execute\nmanufacturing processes, including precise G-code allocation among agents. The\nfindings highlight the importance of continuous large language model\nintegration into multi-agent manufacturing systems and the development of\nsophisticated agent communication protocols for a more flexible manufacturing\nsystem.",
    "bm25_score": 8.09133272350154
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.01622v4",
    "updated": "2024-10-23T15:02:57+00:00",
    "published": "2024-02-02T18:39:51+00:00",
    "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
    "authors": "['Jian Xie' 'Kai Zhang' 'Jiangjie Chen' 'Tinghui Zhu' 'Renze Lou'\n 'Yuandong Tian' 'Yanghua Xiao' 'Yu Su']",
    "summary": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
    "comment": "ICML 2024 (Spotlight)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL']",
    "links": "['http://arxiv.org/abs/2402.01622v4' 'http://arxiv.org/pdf/2402.01622v4']",
    "pdf_url": "http://arxiv.org/pdf/2402.01622v4",
    "arxiv_id": "2402.01622",
    "row_id": 37281,
    "year": 2024,
    "processed_text": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
    "bm25_score": 8.056932296277246
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.04151v1",
    "updated": "2024-06-06T15:15:41+00:00",
    "published": "2024-06-06T15:15:41+00:00",
    "title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments",
    "authors": "['Zhiheng Xi' 'Yiwen Ding' 'Wenxiang Chen' 'Boyang Hong' 'Honglin Guo'\n 'Junzhe Wang' 'Dingwen Yang' 'Chenyang Liao' 'Xin Guo' 'Wei He'\n 'Songyang Gao' 'Lu Chen' 'Rui Zheng' 'Yicheng Zou' 'Tao Gui' 'Qi Zhang'\n 'Xipeng Qiu' 'Xuanjing Huang' 'Zuxuan Wu' 'Yu-Gang Jiang']",
    "summary": "Building generalist agents that can handle diverse tasks and evolve\nthemselves across different environments is a long-term goal in the AI\ncommunity. Large language models (LLMs) are considered a promising foundation\nto build such agents due to their generalized capabilities. Current approaches\neither have LLM-based agents imitate expert-provided trajectories step-by-step,\nrequiring human supervision, which is hard to scale and limits environmental\nexploration; or they let agents explore and learn in isolated environments,\nresulting in specialist agents with limited generalization. In this paper, we\ntake the first step towards building generally-capable LLM-based agents with\nself-evolution ability. We identify a trinity of ingredients: 1) diverse\nenvironments for agent exploration and learning, 2) a trajectory set to equip\nagents with basic capabilities and prior knowledge, and 3) an effective and\nscalable evolution method. We propose AgentGym, a new framework featuring a\nvariety of environments and tasks for broad, real-time, uni-format, and\nconcurrent agent exploration. AgentGym also includes a database with expanded\ninstructions, a benchmark suite, and high-quality trajectories across\nenvironments. Next, we propose a novel method, AgentEvol, to investigate the\npotential of agent self-evolution beyond previously seen data across tasks and\nenvironments. Experimental results show that the evolved agents can achieve\nresults comparable to SOTA models. We release the AgentGym suite, including the\nplatform, dataset, benchmark, checkpoints, and algorithm implementations. The\nAgentGym suite is available on https://github.com/WooooDyy/AgentGym.",
    "comment": "Project site: https://agentgym.github.io",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL']",
    "links": "['http://arxiv.org/abs/2406.04151v1' 'http://arxiv.org/pdf/2406.04151v1']",
    "pdf_url": "http://arxiv.org/pdf/2406.04151v1",
    "arxiv_id": "2406.04151",
    "row_id": 564,
    "year": 2024,
    "processed_text": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments Building generalist agents that can handle diverse tasks and evolve\nthemselves across different environments is a long-term goal in the AI\ncommunity. Large language models (LLMs) are considered a promising foundation\nto build such agents due to their generalized capabilities. Current approaches\neither have LLM-based agents imitate expert-provided trajectories step-by-step,\nrequiring human supervision, which is hard to scale and limits environmental\nexploration; or they let agents explore and learn in isolated environments,\nresulting in specialist agents with limited generalization. In this paper, we\ntake the first step towards building generally-capable LLM-based agents with\nself-evolution ability. We identify a trinity of ingredients: 1) diverse\nenvironments for agent exploration and learning, 2) a trajectory set to equip\nagents with basic capabilities and prior knowledge, and 3) an effective and\nscalable evolution method. We propose AgentGym, a new framework featuring a\nvariety of environments and tasks for broad, real-time, uni-format, and\nconcurrent agent exploration. AgentGym also includes a database with expanded\ninstructions, a benchmark suite, and high-quality trajectories across\nenvironments. Next, we propose a novel method, AgentEvol, to investigate the\npotential of agent self-evolution beyond previously seen data across tasks and\nenvironments. Experimental results show that the evolved agents can achieve\nresults comparable to SOTA models. We release the AgentGym suite, including the\nplatform, dataset, benchmark, checkpoints, and algorithm implementations. The\nAgentGym suite is available on https://github.com/WooooDyy/AgentGym.",
    "bm25_score": 8.032136436828473
  },
  {
    "entry_id": "http://arxiv.org/abs/2412.03920v1",
    "updated": "2024-12-05T06:46:46+00:00",
    "published": "2024-12-05T06:46:46+00:00",
    "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios",
    "authors": "['Xiachong Feng' 'Longxu Dou' 'Ella Li' 'Qinghao Wang' 'Haochuan Wang'\n 'Yu Guo' 'Chang Ma' 'Lingpeng Kong']",
    "summary": "Game-theoretic scenarios have become pivotal in evaluating the social\nintelligence of Large Language Model (LLM)-based social agents. While numerous\nstudies have explored these agents in such settings, there is a lack of a\ncomprehensive survey summarizing the current progress. To address this gap, we\nsystematically review existing research on LLM-based social agents within\ngame-theoretic scenarios. Our survey organizes the findings into three core\ncomponents: Game Framework, Social Agent, and Evaluation Protocol. The game\nframework encompasses diverse game scenarios, ranging from choice-focusing to\ncommunication-focusing games. The social agent part explores agents'\npreferences, beliefs, and reasoning abilities. The evaluation protocol covers\nboth game-agnostic and game-specific metrics for assessing agent performance.\nBy reflecting on the current research and identifying future research\ndirections, this survey provides insights to advance the development and\nevaluation of social agents in game-theoretic scenarios.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2412.03920v1' 'http://arxiv.org/pdf/2412.03920v1']",
    "pdf_url": "http://arxiv.org/pdf/2412.03920v1",
    "arxiv_id": "2412.03920",
    "row_id": 52309,
    "year": 2024,
    "processed_text": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios Game-theoretic scenarios have become pivotal in evaluating the social\nintelligence of Large Language Model (LLM)-based social agents. While numerous\nstudies have explored these agents in such settings, there is a lack of a\ncomprehensive survey summarizing the current progress. To address this gap, we\nsystematically review existing research on LLM-based social agents within\ngame-theoretic scenarios. Our survey organizes the findings into three core\ncomponents: Game Framework, Social Agent, and Evaluation Protocol. The game\nframework encompasses diverse game scenarios, ranging from choice-focusing to\ncommunication-focusing games. The social agent part explores agents'\npreferences, beliefs, and reasoning abilities. The evaluation protocol covers\nboth game-agnostic and game-specific metrics for assessing agent performance.\nBy reflecting on the current research and identifying future research\ndirections, this survey provides insights to advance the development and\nevaluation of social agents in game-theoretic scenarios.",
    "bm25_score": 8.025928426332056
  },
  {
    "entry_id": "http://arxiv.org/abs/2404.02831v2",
    "updated": "2024-07-24T20:31:52+00:00",
    "published": "2024-04-03T16:08:01+00:00",
    "title": "Empowering Biomedical Discovery with AI Agents",
    "authors": "['Shanghua Gao' 'Ada Fang' 'Yepeng Huang' 'Valentina Giunchiglia'\n 'Ayush Noori' 'Jonathan Richard Schwarz' 'Yasha Ektefaie' 'Jovana Kondic'\n 'Marinka Zitnik']",
    "summary": "We envision \"AI scientists\" as systems capable of skeptical learning and\nreasoning that empower biomedical research through collaborative agents that\nintegrate AI models and biomedical tools with experimental platforms. Rather\nthan taking humans out of the discovery process, biomedical AI agents combine\nhuman creativity and expertise with AI's ability to analyze large datasets,\nnavigate hypothesis spaces, and execute repetitive tasks. AI agents are poised\nto be proficient in various tasks, planning discovery workflows and performing\nself-assessment to identify and mitigate gaps in their knowledge. These agents\nuse large language models and generative models to feature structured memory\nfor continual learning and use machine learning tools to incorporate scientific\nknowledge, biological principles, and theories. AI agents can impact areas\nranging from virtual cell simulation, programmable control of phenotypes, and\nthe design of cellular circuits to developing new therapies.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2404.02831v2' 'http://arxiv.org/pdf/2404.02831v2']",
    "pdf_url": "http://arxiv.org/pdf/2404.02831v2",
    "arxiv_id": "2404.02831",
    "row_id": 1308,
    "year": 2024,
    "processed_text": "Empowering Biomedical Discovery with AI Agents We envision \"AI scientists\" as systems capable of skeptical learning and\nreasoning that empower biomedical research through collaborative agents that\nintegrate AI models and biomedical tools with experimental platforms. Rather\nthan taking humans out of the discovery process, biomedical AI agents combine\nhuman creativity and expertise with AI's ability to analyze large datasets,\nnavigate hypothesis spaces, and execute repetitive tasks. AI agents are poised\nto be proficient in various tasks, planning discovery workflows and performing\nself-assessment to identify and mitigate gaps in their knowledge. These agents\nuse large language models and generative models to feature structured memory\nfor continual learning and use machine learning tools to incorporate scientific\nknowledge, biological principles, and theories. AI agents can impact areas\nranging from virtual cell simulation, programmable control of phenotypes, and\nthe design of cellular circuits to developing new therapies.",
    "bm25_score": 8.022085529203688
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.13820v1",
    "updated": "2025-05-20T02:01:55+00:00",
    "published": "2025-05-20T02:01:55+00:00",
    "title": "Structured Agent Distillation for Large Language Model",
    "authors": "['Jun Liu' 'Zhenglun Kong' 'Peiyan Dong' 'Changdi Yang' 'Tianqi Li'\n 'Hao Tang' 'Geng Yuan' 'Wei Niu' 'Wenbin Zhang' 'Pu Zhao' 'Xue Lin'\n 'Dong Huang' 'Yanzhi Wang']",
    "summary": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": "['cs.LG' 'cs.AI' 'cs.CL']",
    "links": "['http://arxiv.org/abs/2505.13820v1' 'http://arxiv.org/pdf/2505.13820v1']",
    "pdf_url": "http://arxiv.org/pdf/2505.13820v1",
    "arxiv_id": "2505.13820",
    "row_id": 470807,
    "year": 2025,
    "processed_text": "Structured Agent Distillation for Large Language Model Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.",
    "bm25_score": 8.015685417731149
  },
  {
    "entry_id": "http://arxiv.org/abs/2412.14222v1",
    "updated": "2024-12-18T15:03:26+00:00",
    "published": "2024-12-18T15:03:26+00:00",
    "title": "A Survey on Large Language Model-based Agents for Statistics and Data Science",
    "authors": "['Maojun Sun' 'Ruijian Han' 'Binyan Jiang' 'Houduo Qi' 'Defeng Sun'\n 'Yancheng Yuan' 'Jian Huang']",
    "summary": "In recent years, data science agents powered by Large Language Models (LLMs),\nknown as \"data agents,\" have shown significant potential to transform the\ntraditional data analysis paradigm. This survey provides an overview of the\nevolution, capabilities, and applications of LLM-based data agents,\nhighlighting their role in simplifying complex data tasks and lowering the\nentry barrier for users without related expertise. We explore current trends in\nthe design of LLM-based frameworks, detailing essential features such as\nplanning, reasoning, reflection, multi-agent collaboration, user interface,\nknowledge integration, and system design, which enable agents to address\ndata-centric problems with minimal human intervention. Furthermore, we analyze\nseveral case studies to demonstrate the practical applications of various data\nagents in real-world scenarios. Finally, we identify key challenges and propose\nfuture research directions to advance the development of data agents into\nintelligent statistical analysis software.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.LG' 'stat.OT']",
    "links": "['http://arxiv.org/abs/2412.14222v1' 'http://arxiv.org/pdf/2412.14222v1']",
    "pdf_url": "http://arxiv.org/pdf/2412.14222v1",
    "arxiv_id": "2412.14222",
    "row_id": 10587,
    "year": 2024,
    "processed_text": "A Survey on Large Language Model-based Agents for Statistics and Data Science In recent years, data science agents powered by Large Language Models (LLMs),\nknown as \"data agents,\" have shown significant potential to transform the\ntraditional data analysis paradigm. This survey provides an overview of the\nevolution, capabilities, and applications of LLM-based data agents,\nhighlighting their role in simplifying complex data tasks and lowering the\nentry barrier for users without related expertise. We explore current trends in\nthe design of LLM-based frameworks, detailing essential features such as\nplanning, reasoning, reflection, multi-agent collaboration, user interface,\nknowledge integration, and system design, which enable agents to address\ndata-centric problems with minimal human intervention. Furthermore, we analyze\nseveral case studies to demonstrate the practical applications of various data\nagents in real-world scenarios. Finally, we identify key challenges and propose\nfuture research directions to advance the development of data agents into\nintelligent statistical analysis software.",
    "bm25_score": 8.013184555396396
  },
  {
    "entry_id": "http://arxiv.org/abs/2409.03793v2",
    "updated": "2024-09-13T08:14:36+00:00",
    "published": "2024-09-03T10:14:51+00:00",
    "title": "Safeguarding AI Agents: Developing and Analyzing Safety Architectures",
    "authors": "['Ishaan Domkundwar' 'Mukunda N S' 'Ishaan Bhola']",
    "summary": "AI agents, specifically powered by large language models, have demonstrated\nexceptional capabilities in various applications where precision and efficacy\nare necessary. However, these agents come with inherent risks, including the\npotential for unsafe or biased actions, vulnerability to adversarial attacks,\nlack of transparency, and tendency to generate hallucinations. As AI agents\nbecome more prevalent in critical sectors of the industry, the implementation\nof effective safety protocols becomes increasingly important. This paper\naddresses the critical need for safety measures in AI systems, especially ones\nthat collaborate with human teams. We propose and evaluate three frameworks to\nenhance safety protocols in AI agent systems: an LLM-powered input-output\nfilter, a safety agent integrated within the system, and a hierarchical\ndelegation-based system with embedded safety checks. Our methodology involves\nimplementing these frameworks and testing them against a set of unsafe agentic\nuse cases, providing a comprehensive evaluation of their effectiveness in\nmitigating risks associated with AI agent deployment. We conclude that these\nframeworks can significantly strengthen the safety and security of AI agent\nsystems, minimizing potential harmful actions or outputs. Our work contributes\nto the ongoing effort to create safe and reliable AI applications, particularly\nin automated operations, and provides a foundation for developing robust\nguardrails to ensure the responsible use of AI agents in real-world\napplications.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": "['cs.CR' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2409.03793v2' 'http://arxiv.org/pdf/2409.03793v2']",
    "pdf_url": "http://arxiv.org/pdf/2409.03793v2",
    "arxiv_id": "2409.03793",
    "row_id": 81842,
    "year": 2024,
    "processed_text": "Safeguarding AI Agents: Developing and Analyzing Safety Architectures AI agents, specifically powered by large language models, have demonstrated\nexceptional capabilities in various applications where precision and efficacy\nare necessary. However, these agents come with inherent risks, including the\npotential for unsafe or biased actions, vulnerability to adversarial attacks,\nlack of transparency, and tendency to generate hallucinations. As AI agents\nbecome more prevalent in critical sectors of the industry, the implementation\nof effective safety protocols becomes increasingly important. This paper\naddresses the critical need for safety measures in AI systems, especially ones\nthat collaborate with human teams. We propose and evaluate three frameworks to\nenhance safety protocols in AI agent systems: an LLM-powered input-output\nfilter, a safety agent integrated within the system, and a hierarchical\ndelegation-based system with embedded safety checks. Our methodology involves\nimplementing these frameworks and testing them against a set of unsafe agentic\nuse cases, providing a comprehensive evaluation of their effectiveness in\nmitigating risks associated with AI agent deployment. We conclude that these\nframeworks can significantly strengthen the safety and security of AI agent\nsystems, minimizing potential harmful actions or outputs. Our work contributes\nto the ongoing effort to create safe and reliable AI applications, particularly\nin automated operations, and provides a foundation for developing robust\nguardrails to ensure the responsible use of AI agents in real-world\napplications.",
    "bm25_score": 12.091632422400824
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.18829v1",
    "updated": "2025-05-24T18:56:00+00:00",
    "published": "2025-05-24T18:56:00+00:00",
    "title": "LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS",
    "authors": "['Kai Mei' 'Xi Zhu' 'Hang Gao' 'Shuhang Lin' 'Yongfeng Zhang']",
    "summary": "We present AIOS 1.0, a novel platform designed to advance computer-use agent\n(CUA) capabilities through environmental contextualization. While existing\napproaches primarily focus on building more powerful agent frameworks or\nenhancing agent models, we identify a fundamental limitation: the semantic\ndisconnect between how language models understand the world and how computer\ninterfaces are structured. AIOS 1.0 addresses this challenge by transforming\ncomputers into contextual environments that language models can natively\ncomprehend, implementing a Model Context Protocol (MCP) server architecture to\nabstract computer states and actions. This approach effectively decouples\ninterface complexity from decision complexity, enabling agents to reason more\neffectively about computing environments. To demonstrate our platform's\neffectiveness, we introduce LiteCUA, a lightweight computer-use agent built on\nAIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark,\noutperforming several specialized agent frameworks despite its simple\narchitecture. Our results suggest that contextualizing computer environments\nfor language models represents a promising direction for developing more\ncapable computer-use agents and advancing toward AI that can interact with\ndigital systems. The source code of LiteCUA is available at\nhttps://github.com/agiresearch/LiteCUA, and it is also integrated into the AIOS\nmain branch as part of AIOS at https://github.com/agiresearch/AIOS.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.HC' 'cs.OS']",
    "links": "['http://arxiv.org/abs/2505.18829v1' 'http://arxiv.org/pdf/2505.18829v1']",
    "pdf_url": "http://arxiv.org/pdf/2505.18829v1",
    "arxiv_id": "2505.18829",
    "row_id": 460228,
    "year": 2025,
    "processed_text": "LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS We present AIOS 1.0, a novel platform designed to advance computer-use agent\n(CUA) capabilities through environmental contextualization. While existing\napproaches primarily focus on building more powerful agent frameworks or\nenhancing agent models, we identify a fundamental limitation: the semantic\ndisconnect between how language models understand the world and how computer\ninterfaces are structured. AIOS 1.0 addresses this challenge by transforming\ncomputers into contextual environments that language models can natively\ncomprehend, implementing a Model Context Protocol (MCP) server architecture to\nabstract computer states and actions. This approach effectively decouples\ninterface complexity from decision complexity, enabling agents to reason more\neffectively about computing environments. To demonstrate our platform's\neffectiveness, we introduce LiteCUA, a lightweight computer-use agent built on\nAIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark,\noutperforming several specialized agent frameworks despite its simple\narchitecture. Our results suggest that contextualizing computer environments\nfor language models represents a promising direction for developing more\ncapable computer-use agents and advancing toward AI that can interact with\ndigital systems. The source code of LiteCUA is available at\nhttps://github.com/agiresearch/LiteCUA, and it is also integrated into the AIOS\nmain branch as part of AIOS at https://github.com/agiresearch/AIOS.",
    "bm25_score": 11.994963380365862
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.20127v1",
    "updated": "2025-05-26T15:26:07+00:00",
    "published": "2025-05-26T15:26:07+00:00",
    "title": "Agentic AI Process Observability: Discovering Behavioral Variability",
    "authors": "['Fabiana Fournier' 'Lior Limonad' 'Yuval David']",
    "summary": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions.",
    "comment": "12 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2505.20127v1' 'http://arxiv.org/pdf/2505.20127v1']",
    "pdf_url": "http://arxiv.org/pdf/2505.20127v1",
    "arxiv_id": "2505.20127",
    "row_id": 460141,
    "year": 2025,
    "processed_text": "Agentic AI Process Observability: Discovering Behavioral Variability AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions.",
    "bm25_score": 11.934743912875145
  },
  {
    "entry_id": "http://arxiv.org/abs/2504.14427v1",
    "updated": "2025-04-20T00:02:56+00:00",
    "published": "2025-04-20T00:02:56+00:00",
    "title": "Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework",
    "authors": "['Spencer Lin' 'Miru Jun' 'Basem Rizk' 'Karen Shieh' 'Scott Fisher'\n 'Sharon Mozgai']",
    "summary": "This case study presents our user-centered design model for Socially\nIntelligent Agent (SIA) development frameworks through our experience\ndeveloping Estuary, an open source multimodal framework for building\nlow-latency real-time socially interactive agents. We leverage the Rapid\nAssessment Process (RAP) to collect the thoughts of leading researchers in the\nfield of SIAs regarding the current state of the art for SIA development as\nwell as their evaluation of how well Estuary may potentially address current\nresearch gaps. We achieve this through a series of end-user interviews\nconducted by a fellow researcher in the community. We hope that the findings of\nour work will not only assist the continued development of Estuary but also\nguide the development of other future frameworks and technologies for SIAs.",
    "comment": null,
    "journal_ref": null,
    "doi": "10.1145/3706599.3707399",
    "primary_category": "cs.HC",
    "categories": "['cs.HC' 'cs.AI']",
    "links": "['http://dx.doi.org/10.1145/3706599.3707399'\n 'http://arxiv.org/abs/2504.14427v1' 'http://arxiv.org/pdf/2504.14427v1']",
    "pdf_url": "http://arxiv.org/pdf/2504.14427v1",
    "arxiv_id": "2504.14427",
    "row_id": 231396,
    "year": 2025,
    "processed_text": "Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework This case study presents our user-centered design model for Socially\nIntelligent Agent (SIA) development frameworks through our experience\ndeveloping Estuary, an open source multimodal framework for building\nlow-latency real-time socially interactive agents. We leverage the Rapid\nAssessment Process (RAP) to collect the thoughts of leading researchers in the\nfield of SIAs regarding the current state of the art for SIA development as\nwell as their evaluation of how well Estuary may potentially address current\nresearch gaps. We achieve this through a series of end-user interviews\nconducted by a fellow researcher in the community. We hope that the findings of\nour work will not only assist the continued development of Estuary but also\nguide the development of other future frameworks and technologies for SIAs.",
    "bm25_score": 11.289933029585455
  },
  {
    "entry_id": "http://arxiv.org/abs/2305.17246v2",
    "updated": "2023-08-18T11:32:44+00:00",
    "published": "2023-05-26T20:19:09+00:00",
    "title": "NASimEmu: Network Attack Simulator & Emulator for Training Agents Generalizing to Novel Scenarios",
    "authors": "['Jarom\u00edr Janisch' 'Tom\u00e1\u0161 Pevn\u00fd' 'Viliam Lis\u00fd']",
    "summary": "Current frameworks for training offensive penetration testing agents with\ndeep reinforcement learning struggle to produce agents that perform well in\nreal-world scenarios, due to the reality gap in simulation-based frameworks and\nthe lack of scalability in emulation-based frameworks. Additionally, existing\nframeworks often use an unrealistic metric that measures the agents'\nperformance on the training data. NASimEmu, a new framework introduced in this\npaper, addresses these issues by providing both a simulator and an emulator\nwith a shared interface. This approach allows agents to be trained in\nsimulation and deployed in the emulator, thus verifying the realism of the used\nabstraction. Our framework promotes the development of general agents that can\ntransfer to novel scenarios unseen during their training. For the simulation\npart, we adopt an existing simulator NASim and enhance its realism. The\nemulator is implemented with industry-level tools, such as Vagrant, VirtualBox,\nand Metasploit. Experiments demonstrate that a simulation-trained agent can be\ndeployed in emulation, and we show how to use the framework to train a general\nagent that transfers into novel, structurally different scenarios. NASimEmu is\navailable as open-source.",
    "comment": "NASimEmu is available at https://github.com/jaromiru/NASimEmu and the\n  baseline agents at https://github.com/jaromiru/NASimEmu-agents",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": "['cs.CR' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2305.17246v2' 'http://arxiv.org/pdf/2305.17246v2']",
    "pdf_url": "http://arxiv.org/pdf/2305.17246v2",
    "arxiv_id": "2305.17246",
    "row_id": 63242,
    "year": 2023,
    "processed_text": "NASimEmu: Network Attack Simulator & Emulator for Training Agents Generalizing to Novel Scenarios Current frameworks for training offensive penetration testing agents with\ndeep reinforcement learning struggle to produce agents that perform well in\nreal-world scenarios, due to the reality gap in simulation-based frameworks and\nthe lack of scalability in emulation-based frameworks. Additionally, existing\nframeworks often use an unrealistic metric that measures the agents'\nperformance on the training data. NASimEmu, a new framework introduced in this\npaper, addresses these issues by providing both a simulator and an emulator\nwith a shared interface. This approach allows agents to be trained in\nsimulation and deployed in the emulator, thus verifying the realism of the used\nabstraction. Our framework promotes the development of general agents that can\ntransfer to novel scenarios unseen during their training. For the simulation\npart, we adopt an existing simulator NASim and enhance its realism. The\nemulator is implemented with industry-level tools, such as Vagrant, VirtualBox,\nand Metasploit. Experiments demonstrate that a simulation-trained agent can be\ndeployed in emulation, and we show how to use the framework to train a general\nagent that transfers into novel, structurally different scenarios. NASimEmu is\navailable as open-source.",
    "bm25_score": 11.221435043077676
  },
  {
    "entry_id": "http://arxiv.org/abs/2501.07054v1",
    "updated": "2025-01-13T04:28:40+00:00",
    "published": "2025-01-13T04:28:40+00:00",
    "title": "PoAct: Policy and Action Dual-Control Agent for Generalized Applications",
    "authors": "['Guozhi Yuan' 'Youfeng Liu' 'Jingli Yang' 'Wei Jia' 'Kai Lin'\n 'Yansong Gao' 'Shan He' 'Zilin Ding' 'Haitao Li']",
    "summary": "Based on their superior comprehension and reasoning capabilities, Large\nLanguage Model (LLM) driven agent frameworks have achieved significant success\nin numerous complex reasoning tasks. ReAct-like agents can solve various\nintricate problems step-by-step through progressive planning and tool calls,\niteratively optimizing new steps based on environmental feedback. However, as\nthe planning capabilities of LLMs improve, the actions invoked by tool calls in\nReAct-like frameworks often misalign with complex planning and challenging data\norganization. Code Action addresses these issues while also introducing the\nchallenges of a more complex action space and more difficult action\norganization. To leverage Code Action and tackle the challenges of its\ncomplexity, this paper proposes Policy and Action Dual-Control Agent (PoAct)\nfor generalized applications. The aim is to achieve higher-quality code actions\nand more accurate reasoning paths by dynamically switching reasoning policies\nand modifying the action space. Experimental results on the Agent Benchmark for\nboth legal and generic scenarios demonstrate the superior reasoning\ncapabilities and reduced token consumption of our approach in complex tasks. On\nthe LegalAgentBench, our method shows a 20 percent improvement over the\nbaseline while requiring fewer tokens. We conducted experiments and analyses on\nthe GPT-4o and GLM-4 series models, demonstrating the significant potential and\nscalability of our approach to solve complex problems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2501.07054v1' 'http://arxiv.org/pdf/2501.07054v1']",
    "pdf_url": "http://arxiv.org/pdf/2501.07054v1",
    "arxiv_id": "2501.07054",
    "row_id": 9969,
    "year": 2025,
    "processed_text": "PoAct: Policy and Action Dual-Control Agent for Generalized Applications Based on their superior comprehension and reasoning capabilities, Large\nLanguage Model (LLM) driven agent frameworks have achieved significant success\nin numerous complex reasoning tasks. ReAct-like agents can solve various\nintricate problems step-by-step through progressive planning and tool calls,\niteratively optimizing new steps based on environmental feedback. However, as\nthe planning capabilities of LLMs improve, the actions invoked by tool calls in\nReAct-like frameworks often misalign with complex planning and challenging data\norganization. Code Action addresses these issues while also introducing the\nchallenges of a more complex action space and more difficult action\norganization. To leverage Code Action and tackle the challenges of its\ncomplexity, this paper proposes Policy and Action Dual-Control Agent (PoAct)\nfor generalized applications. The aim is to achieve higher-quality code actions\nand more accurate reasoning paths by dynamically switching reasoning policies\nand modifying the action space. Experimental results on the Agent Benchmark for\nboth legal and generic scenarios demonstrate the superior reasoning\ncapabilities and reduced token consumption of our approach in complex tasks. On\nthe LegalAgentBench, our method shows a 20 percent improvement over the\nbaseline while requiring fewer tokens. We conducted experiments and analyses on\nthe GPT-4o and GLM-4 series models, demonstrating the significant potential and\nscalability of our approach to solve complex problems.",
    "bm25_score": 11.155487893646058
  },
  {
    "entry_id": "http://arxiv.org/abs/2412.12475v1",
    "updated": "2024-12-17T02:22:24+00:00",
    "published": "2024-12-17T02:22:24+00:00",
    "title": "RareAgents: Autonomous Multi-disciplinary Team for Rare Disease Diagnosis and Treatment",
    "authors": "['Xuanzhong Chen' 'Ye Jin' 'Xiaohao Mao' 'Lun Wang' 'Shuyang Zhang'\n 'Ting Chen']",
    "summary": "Rare diseases, despite their low individual incidence, collectively impact\naround 300 million people worldwide due to the huge number of diseases. The\ncomplexity of symptoms and the shortage of specialized doctors with relevant\nexperience make diagnosing and treating rare diseases more challenging than\ncommon diseases. Recently, agents powered by large language models (LLMs) have\ndemonstrated notable improvements across various domains. In the medical field,\nsome agent methods have outperformed direct prompts in question-answering tasks\nfrom medical exams. However, current agent frameworks lack adaptation for\nreal-world clinical scenarios, especially those involving the intricate demands\nof rare diseases. To address these challenges, we present RareAgents, the first\nmulti-disciplinary team of LLM-based agents tailored to the complex clinical\ncontext of rare diseases. RareAgents integrates advanced planning capabilities,\nmemory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B\nas the base model. Experimental results show that RareAgents surpasses\nstate-of-the-art domain-specific models, GPT-4o, and existing agent frameworks\nin both differential diagnosis and medication recommendation for rare diseases.\nFurthermore, we contribute a novel dataset, MIMIC-IV-Ext-Rare, derived from\nMIMIC-IV, to support further advancements in this field.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2412.12475v1' 'http://arxiv.org/pdf/2412.12475v1']",
    "pdf_url": "http://arxiv.org/pdf/2412.12475v1",
    "arxiv_id": "2412.12475",
    "row_id": 52780,
    "year": 2024,
    "processed_text": "RareAgents: Autonomous Multi-disciplinary Team for Rare Disease Diagnosis and Treatment Rare diseases, despite their low individual incidence, collectively impact\naround 300 million people worldwide due to the huge number of diseases. The\ncomplexity of symptoms and the shortage of specialized doctors with relevant\nexperience make diagnosing and treating rare diseases more challenging than\ncommon diseases. Recently, agents powered by large language models (LLMs) have\ndemonstrated notable improvements across various domains. In the medical field,\nsome agent methods have outperformed direct prompts in question-answering tasks\nfrom medical exams. However, current agent frameworks lack adaptation for\nreal-world clinical scenarios, especially those involving the intricate demands\nof rare diseases. To address these challenges, we present RareAgents, the first\nmulti-disciplinary team of LLM-based agents tailored to the complex clinical\ncontext of rare diseases. RareAgents integrates advanced planning capabilities,\nmemory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B\nas the base model. Experimental results show that RareAgents surpasses\nstate-of-the-art domain-specific models, GPT-4o, and existing agent frameworks\nin both differential diagnosis and medication recommendation for rare diseases.\nFurthermore, we contribute a novel dataset, MIMIC-IV-Ext-Rare, derived from\nMIMIC-IV, to support further advancements in this field.",
    "bm25_score": 11.138804869990421
  },
  {
    "entry_id": "http://arxiv.org/abs/2407.07061v2",
    "updated": "2024-07-10T15:57:21+00:00",
    "published": "2024-07-09T17:33:24+00:00",
    "title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence",
    "authors": "['Weize Chen' 'Ziming You' 'Ran Li' 'Yitong Guan' 'Chen Qian'\n 'Chenyang Zhao' 'Cheng Yang' 'Ruobing Xie' 'Zhiyuan Liu' 'Maosong Sun']",
    "summary": "The rapid advancement of large language models (LLMs) has paved the way for\nthe development of highly capable autonomous agents. However, existing\nmulti-agent frameworks often struggle with integrating diverse capable\nthird-party agents due to reliance on agents defined within their own\necosystems. They also face challenges in simulating distributed environments,\nas most frameworks are limited to single-device setups. Furthermore, these\nframeworks often rely on hard-coded communication pipelines, limiting their\nadaptability to dynamic task requirements. Inspired by the concept of the\nInternet, we propose the Internet of Agents (IoA), a novel framework that\naddresses these limitations by providing a flexible and scalable platform for\nLLM-based multi-agent collaboration. IoA introduces an agent integration\nprotocol, an instant-messaging-like architecture design, and dynamic mechanisms\nfor agent teaming and conversation flow control. Through extensive experiments\non general assistant tasks, embodied AI tasks, and retrieval-augmented\ngeneration benchmarks, we demonstrate that IoA consistently outperforms\nstate-of-the-art baselines, showcasing its ability to facilitate effective\ncollaboration among heterogeneous agents. IoA represents a step towards linking\ndiverse agents in an Internet-like environment, where agents can seamlessly\ncollaborate to achieve greater intelligence and capabilities. Our codebase has\nbeen released at \\url{https://github.com/OpenBMB/IoA}.",
    "comment": "work in progress",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL']",
    "links": "['http://arxiv.org/abs/2407.07061v2' 'http://arxiv.org/pdf/2407.07061v2']",
    "pdf_url": "http://arxiv.org/pdf/2407.07061v2",
    "arxiv_id": "2407.07061",
    "row_id": 30103,
    "year": 2024,
    "processed_text": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence The rapid advancement of large language models (LLMs) has paved the way for\nthe development of highly capable autonomous agents. However, existing\nmulti-agent frameworks often struggle with integrating diverse capable\nthird-party agents due to reliance on agents defined within their own\necosystems. They also face challenges in simulating distributed environments,\nas most frameworks are limited to single-device setups. Furthermore, these\nframeworks often rely on hard-coded communication pipelines, limiting their\nadaptability to dynamic task requirements. Inspired by the concept of the\nInternet, we propose the Internet of Agents (IoA), a novel framework that\naddresses these limitations by providing a flexible and scalable platform for\nLLM-based multi-agent collaboration. IoA introduces an agent integration\nprotocol, an instant-messaging-like architecture design, and dynamic mechanisms\nfor agent teaming and conversation flow control. Through extensive experiments\non general assistant tasks, embodied AI tasks, and retrieval-augmented\ngeneration benchmarks, we demonstrate that IoA consistently outperforms\nstate-of-the-art baselines, showcasing its ability to facilitate effective\ncollaboration among heterogeneous agents. IoA represents a step towards linking\ndiverse agents in an Internet-like environment, where agents can seamlessly\ncollaborate to achieve greater intelligence and capabilities. Our codebase has\nbeen released at \\url{https://github.com/OpenBMB/IoA}.",
    "bm25_score": 11.121990358449501
  },
  {
    "entry_id": "http://arxiv.org/abs/2502.05957v1",
    "updated": "2025-02-09T16:53:56+00:00",
    "published": "2025-02-09T16:53:56+00:00",
    "title": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents",
    "authors": "['Jiabin Tang' 'Tianyu Fan' 'Chao Huang']",
    "summary": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities\nin task automation and intelligent decision-making, driving the widespread\nadoption of agent development frameworks such as LangChain and AutoGen.\nHowever, these frameworks predominantly serve developers with extensive\ntechnical expertise - a significant limitation considering that only 0.03 % of\nthe global population possesses the necessary programming skills. This stark\naccessibility gap raises a fundamental question: Can we enable everyone,\nregardless of technical background, to build their own LLM agents using natural\nlanguage alone? To address this challenge, we introduce MetaChain-a\nFully-Automated and highly Self-Developing framework that enables users to\ncreate and deploy LLM agents through Natural Language Alone. Operating as an\nautonomous Agent Operating System, MetaChain comprises four key components: i)\nAgentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing\nFile System, and iv) Self-Play Agent Customization module. This lightweight yet\npowerful system enables efficient and dynamic creation and modification of\ntools, agents, and workflows without coding requirements or manual\nintervention. Beyond its code-free agent development capabilities, MetaChain\nalso serves as a versatile multi-agent system for General AI Assistants.\nComprehensive evaluations on the GAIA benchmark demonstrate MetaChain's\neffectiveness in generalist multi-agent tasks, surpassing existing\nstate-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented\nGeneration (RAG)-related capabilities have shown consistently superior\nperformance compared to many alternative LLM-based solutions.",
    "comment": "Code: https://github.com/HKUDS/MetaChain",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL']",
    "links": "['http://arxiv.org/abs/2502.05957v1' 'http://arxiv.org/pdf/2502.05957v1']",
    "pdf_url": "http://arxiv.org/pdf/2502.05957v1",
    "arxiv_id": "2502.05957",
    "row_id": 10299,
    "year": 2025,
    "processed_text": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents Large Language Model (LLM) Agents have demonstrated remarkable capabilities\nin task automation and intelligent decision-making, driving the widespread\nadoption of agent development frameworks such as LangChain and AutoGen.\nHowever, these frameworks predominantly serve developers with extensive\ntechnical expertise - a significant limitation considering that only 0.03 % of\nthe global population possesses the necessary programming skills. This stark\naccessibility gap raises a fundamental question: Can we enable everyone,\nregardless of technical background, to build their own LLM agents using natural\nlanguage alone? To address this challenge, we introduce MetaChain-a\nFully-Automated and highly Self-Developing framework that enables users to\ncreate and deploy LLM agents through Natural Language Alone. Operating as an\nautonomous Agent Operating System, MetaChain comprises four key components: i)\nAgentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing\nFile System, and iv) Self-Play Agent Customization module. This lightweight yet\npowerful system enables efficient and dynamic creation and modification of\ntools, agents, and workflows without coding requirements or manual\nintervention. Beyond its code-free agent development capabilities, MetaChain\nalso serves as a versatile multi-agent system for General AI Assistants.\nComprehensive evaluations on the GAIA benchmark demonstrate MetaChain's\neffectiveness in generalist multi-agent tasks, surpassing existing\nstate-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented\nGeneration (RAG)-related capabilities have shown consistently superior\nperformance compared to many alternative LLM-based solutions.",
    "bm25_score": 11.09532298829803
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.15538v1",
    "updated": "2024-02-23T06:25:20+00:00",
    "published": "2024-02-23T06:25:20+00:00",
    "title": "AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System",
    "authors": "['Zhiwei Liu' 'Weiran Yao' 'Jianguo Zhang' 'Liangwei Yang' 'Zuxin Liu'\n 'Juntao Tan' 'Prafulla K. Choubey' 'Tian Lan' 'Jason Wu' 'Huan Wang'\n 'Shelby Heinecke' 'Caiming Xiong' 'Silvio Savarese']",
    "summary": "The booming success of LLMs initiates rapid development in LLM agents. Though\nthe foundation of an LLM agent is the generative model, it is critical to\ndevise the optimal reasoning strategies and agent architectures. Accordingly,\nLLM agent research advances from the simple chain-of-thought prompting to more\ncomplex ReAct and Reflection reasoning strategy; agent architecture also\nevolves from single agent generation to multi-agent conversation, as well as\nmulti-LLM multi-agent group chat. However, with the existing intricate\nframeworks and libraries, creating and evaluating new reasoning strategies and\nagent architectures has become a complex challenge, which hinders research\ninvestigation into LLM agents. Thus, we open-source a new AI agent library,\nAgentLite, which simplifies this process by offering a lightweight,\nuser-friendly platform for innovating LLM agent reasoning, architectures, and\napplications with ease. AgentLite is a task-oriented framework designed to\nenhance the ability of agents to break down tasks and facilitate the\ndevelopment of multi-agent systems. Furthermore, we introduce multiple\npractical applications developed with AgentLite to demonstrate its convenience\nand flexibility. Get started now at:\n\\url{https://github.com/SalesforceAIResearch/AgentLite}.",
    "comment": "preprint. Library is available at\n  https://github.com/SalesforceAIResearch/AgentLite",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": "['cs.MA' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2402.15538v1' 'http://arxiv.org/pdf/2402.15538v1']",
    "pdf_url": "http://arxiv.org/pdf/2402.15538v1",
    "arxiv_id": "2402.15538",
    "row_id": 322495,
    "year": 2024,
    "processed_text": "AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System The booming success of LLMs initiates rapid development in LLM agents. Though\nthe foundation of an LLM agent is the generative model, it is critical to\ndevise the optimal reasoning strategies and agent architectures. Accordingly,\nLLM agent research advances from the simple chain-of-thought prompting to more\ncomplex ReAct and Reflection reasoning strategy; agent architecture also\nevolves from single agent generation to multi-agent conversation, as well as\nmulti-LLM multi-agent group chat. However, with the existing intricate\nframeworks and libraries, creating and evaluating new reasoning strategies and\nagent architectures has become a complex challenge, which hinders research\ninvestigation into LLM agents. Thus, we open-source a new AI agent library,\nAgentLite, which simplifies this process by offering a lightweight,\nuser-friendly platform for innovating LLM agent reasoning, architectures, and\napplications with ease. AgentLite is a task-oriented framework designed to\nenhance the ability of agents to break down tasks and facilitate the\ndevelopment of multi-agent systems. Furthermore, we introduce multiple\npractical applications developed with AgentLite to demonstrate its convenience\nand flexibility. Get started now at:\n\\url{https://github.com/SalesforceAIResearch/AgentLite}.",
    "bm25_score": 10.996366474450303
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.16416v1",
    "updated": "2025-03-20T17:59:23+00:00",
    "published": "2025-03-20T17:59:23+00:00",
    "title": "Survey on Evaluation of LLM-based Agents",
    "authors": "['Asaf Yehudai' 'Lilach Eden' 'Alan Li' 'Guy Uziel' 'Yilun Zhao'\n 'Roy Bar-Haim' 'Arman Cohan' 'Michal Shmueli-Scheuer']",
    "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2503.16416v1' 'http://arxiv.org/pdf/2503.16416v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.16416v1",
    "arxiv_id": "2503.16416",
    "row_id": 11259,
    "year": 2025,
    "processed_text": "Survey on Evaluation of LLM-based Agents The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
    "bm25_score": 10.898638923203542
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.24354v1",
    "updated": "2025-05-30T08:46:23+00:00",
    "published": "2025-05-30T08:46:23+00:00",
    "title": "Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research",
    "authors": "['Qianqian Zhang' 'Jiajia Liao' 'Heting Ying' 'Yibo Ma' 'Haozhan Shen'\n 'Jingcheng Li' 'Peng Liu' 'Lu Zhang' 'Chunxin Fang' 'Kyusong Lee'\n 'Ruochen Xu' 'Tiancheng Zhao']",
    "summary": "Language agents powered by large language models (LLMs) have demonstrated\nremarkable capabilities in understanding, reasoning, and executing complex\ntasks. However, developing robust agents presents significant challenges:\nsubstantial engineering overhead, lack of standardized components, and\ninsufficient evaluation frameworks for fair comparison. We introduce Agent\nGraph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and\nextensible framework that addresses these challenges through three key\ncontributions: (1) a modular architecture with a graph-based workflow engine,\nefficient memory management, and clean component abstraction; (2) a\ncomprehensive suite of reusable agent algorithms implementing state-of-the-art\nreasoning approaches; and (3) a rigorous evaluation framework enabling\nsystematic comparison across multiple dimensions. Through extensive experiments\non mathematical reasoning and multimodal tasks, we evaluate various agent\nalgorithms across different LLMs, revealing important insights about their\nrelative strengths and applicability. Our results demonstrate that while\nsophisticated reasoning approaches can enhance agent capabilities, simpler\nmethods like Chain-of-Thought often exhibit robust performance with\nsignificantly lower computational overhead. AGORA not only simplifies language\nagent development but also establishes a foundation for reproducible agent\nresearch through standardized evaluation protocols.",
    "comment": "Accepted by ACL 2025 Demo",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL']",
    "links": "['http://arxiv.org/abs/2505.24354v1' 'http://arxiv.org/pdf/2505.24354v1']",
    "pdf_url": "http://arxiv.org/pdf/2505.24354v1",
    "arxiv_id": "2505.24354",
    "row_id": 461745,
    "year": 2025,
    "processed_text": "Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research Language agents powered by large language models (LLMs) have demonstrated\nremarkable capabilities in understanding, reasoning, and executing complex\ntasks. However, developing robust agents presents significant challenges:\nsubstantial engineering overhead, lack of standardized components, and\ninsufficient evaluation frameworks for fair comparison. We introduce Agent\nGraph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and\nextensible framework that addresses these challenges through three key\ncontributions: (1) a modular architecture with a graph-based workflow engine,\nefficient memory management, and clean component abstraction; (2) a\ncomprehensive suite of reusable agent algorithms implementing state-of-the-art\nreasoning approaches; and (3) a rigorous evaluation framework enabling\nsystematic comparison across multiple dimensions. Through extensive experiments\non mathematical reasoning and multimodal tasks, we evaluate various agent\nalgorithms across different LLMs, revealing important insights about their\nrelative strengths and applicability. Our results demonstrate that while\nsophisticated reasoning approaches can enhance agent capabilities, simpler\nmethods like Chain-of-Thought often exhibit robust performance with\nsignificantly lower computational overhead. AGORA not only simplifies language\nagent development but also establishes a foundation for reproducible agent\nresearch through standardized evaluation protocols.",
    "bm25_score": 10.832757924626353
  },
  {
    "entry_id": "http://arxiv.org/abs/2209.07758v2",
    "updated": "2023-10-10T18:53:29+00:00",
    "published": "2022-09-16T07:35:20+00:00",
    "title": "Game-theoretic Objective Space Planning",
    "authors": "['Hongrui Zheng' 'Zhijun Zhuang' 'Johannes Betz' 'Rahul Mangharam']",
    "summary": "Generating competitive strategies and performing continuous motion planning\nsimultaneously in an adversarial setting is a challenging problem. In addition,\nunderstanding the intent of other agents is crucial to deploying autonomous\nsystems in adversarial multi-agent environments. Existing approaches either\ndiscretize agent action by grouping similar control inputs, sacrificing\nperformance in motion planning, or plan in uninterpretable latent spaces,\nproducing hard-to-understand agent behaviors. Furthermore, the most popular\npolicy optimization frameworks do not recognize the long-term effect of actions\nand become myopic. This paper proposes an agent action discretization method\nvia abstraction that provides clear intentions of agent actions, an efficient\noffline pipeline of agent population synthesis, and a planning strategy using\ncounterfactual regret minimization with function approximation. Finally, we\nexperimentally validate our findings on scaled autonomous vehicles in a\nhead-to-head racing setting. We demonstrate that using the proposed framework\nsignificantly improves learning, improves the win rate against different\nopponents, and the improvements can be transferred to unseen opponents in an\nunseen environment.",
    "comment": "Submitted to 2024 International Conference on Autonomous Agents and\n  Multi-Agent Systems (AAMAS 2024)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": "['cs.RO' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2209.07758v2' 'http://arxiv.org/pdf/2209.07758v2']",
    "pdf_url": "http://arxiv.org/pdf/2209.07758v2",
    "arxiv_id": "2209.07758",
    "row_id": 373355,
    "year": 2023,
    "processed_text": "Game-theoretic Objective Space Planning Generating competitive strategies and performing continuous motion planning\nsimultaneously in an adversarial setting is a challenging problem. In addition,\nunderstanding the intent of other agents is crucial to deploying autonomous\nsystems in adversarial multi-agent environments. Existing approaches either\ndiscretize agent action by grouping similar control inputs, sacrificing\nperformance in motion planning, or plan in uninterpretable latent spaces,\nproducing hard-to-understand agent behaviors. Furthermore, the most popular\npolicy optimization frameworks do not recognize the long-term effect of actions\nand become myopic. This paper proposes an agent action discretization method\nvia abstraction that provides clear intentions of agent actions, an efficient\noffline pipeline of agent population synthesis, and a planning strategy using\ncounterfactual regret minimization with function approximation. Finally, we\nexperimentally validate our findings on scaled autonomous vehicles in a\nhead-to-head racing setting. We demonstrate that using the proposed framework\nsignificantly improves learning, improves the win rate against different\nopponents, and the improvements can be transferred to unseen opponents in an\nunseen environment.",
    "bm25_score": 10.602105100416164
  },
  {
    "entry_id": "http://arxiv.org/abs/2504.01963v1",
    "updated": "2025-03-13T06:17:50+00:00",
    "published": "2025-03-13T06:17:50+00:00",
    "title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
    "authors": "['R. M. Aratchige' 'W. M. K. S. Ilmini']",
    "summary": "This survey investigates foundational technologies essential for developing\neffective Large Language Model (LLM)-based multi-agent systems. Aiming to\nanswer how best to optimize these systems for collaborative, dynamic\nenvironments, we focus on four critical areas: Architecture, Memory, Planning,\nand Technologies/Frameworks. By analyzing recent advancements and their\nlimitations - such as scalability, real-time response challenges, and agent\ncoordination constraints, we provide a detailed view of the technological\nlandscape. Frameworks like the Mixture of Agents architecture and the ReAct\nplanning model exemplify current innovations, showcasing improvements in role\nassignment and decision-making. This review synthesizes key strengths and\npersistent challenges, offering practical recommendations to enhance system\nscalability, agent collaboration, and adaptability. Our findings provide a\nroadmap for future research, supporting the creation of robust, efficient\nmulti-agent systems that advance both individual agent performance and\ncollective system resilience.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": "['cs.MA' 'cs.AI' 'cs.CL']",
    "links": "['http://arxiv.org/abs/2504.01963v1' 'http://arxiv.org/pdf/2504.01963v1']",
    "pdf_url": "http://arxiv.org/pdf/2504.01963v1",
    "arxiv_id": "2504.01963",
    "row_id": 325360,
    "year": 2025,
    "processed_text": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems This survey investigates foundational technologies essential for developing\neffective Large Language Model (LLM)-based multi-agent systems. Aiming to\nanswer how best to optimize these systems for collaborative, dynamic\nenvironments, we focus on four critical areas: Architecture, Memory, Planning,\nand Technologies/Frameworks. By analyzing recent advancements and their\nlimitations - such as scalability, real-time response challenges, and agent\ncoordination constraints, we provide a detailed view of the technological\nlandscape. Frameworks like the Mixture of Agents architecture and the ReAct\nplanning model exemplify current innovations, showcasing improvements in role\nassignment and decision-making. This review synthesizes key strengths and\npersistent challenges, offering practical recommendations to enhance system\nscalability, agent collaboration, and adaptability. Our findings provide a\nroadmap for future research, supporting the creation of robust, efficient\nmulti-agent systems that advance both individual agent performance and\ncollective system resilience.",
    "bm25_score": 10.534991352979834
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.14228v2",
    "updated": "2024-07-11T14:18:35+00:00",
    "published": "2024-06-20T11:49:23+00:00",
    "title": "EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms",
    "authors": "['Siyu Yuan' 'Kaitao Song' 'Jiangjie Chen' 'Xu Tan' 'Dongsheng Li'\n 'Deqing Yang']",
    "summary": "The rise of powerful large language models (LLMs) has spurred a new trend in\nbuilding LLM-based autonomous agents for solving complex tasks, especially\nmulti-agent systems. Despite the remarkable progress, we notice that existing\nworks are heavily dependent on human-designed frameworks, which greatly limits\nthe functional scope and scalability of agent systems. How to automatically\nextend the specialized agent to multi-agent systems to improve task-solving\ncapability still remains a significant challenge. In this paper, we introduce\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\nframeworks as the initial individual and then apply a series of evolutionary\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\nagents with diverse agent settings. EvoAgent can be generalized to any\nLLM-based agent framework, and can automatically extend the existing agent\nframework to multi-agent systems without any extra human designs. Experimental\nresults across various tasks have shown that EvoAgent can automatically\ngenerate multiple expert agents and significantly enhance the task-solving\ncapabilities of LLM-based agents.",
    "comment": "Work in process",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2406.14228v2' 'http://arxiv.org/pdf/2406.14228v2']",
    "pdf_url": "http://arxiv.org/pdf/2406.14228v2",
    "arxiv_id": "2406.14228",
    "row_id": 312,
    "year": 2024,
    "processed_text": "EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms The rise of powerful large language models (LLMs) has spurred a new trend in\nbuilding LLM-based autonomous agents for solving complex tasks, especially\nmulti-agent systems. Despite the remarkable progress, we notice that existing\nworks are heavily dependent on human-designed frameworks, which greatly limits\nthe functional scope and scalability of agent systems. How to automatically\nextend the specialized agent to multi-agent systems to improve task-solving\ncapability still remains a significant challenge. In this paper, we introduce\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\nframeworks as the initial individual and then apply a series of evolutionary\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\nagents with diverse agent settings. EvoAgent can be generalized to any\nLLM-based agent framework, and can automatically extend the existing agent\nframework to multi-agent systems without any extra human designs. Experimental\nresults across various tasks have shown that EvoAgent can automatically\ngenerate multiple expert agents and significantly enhance the task-solving\ncapabilities of LLM-based agents.",
    "bm25_score": 10.431963270110765
  },
  {
    "entry_id": "http://arxiv.org/abs/2501.09316v1",
    "updated": "2025-01-16T06:14:58+00:00",
    "published": "2025-01-16T06:14:58+00:00",
    "title": "SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs",
    "authors": "['Anbang Ye' 'Qianran Ma' 'Jia Chen' 'Muqi Li' 'Tong Li' 'Fujiao Liu'\n 'Siqi Mai' 'Meichen Lu' 'Haitao Bao' 'Yang You']",
    "summary": "Despite significant advancements in general-purpose AI agents, several\nchallenges still hinder their practical application in real-world scenarios.\nFirst, the limited planning capabilities of Large Language Models (LLM)\nrestrict AI agents from effectively solving complex tasks that require\nlong-horizon planning. Second, general-purpose AI agents struggle to\nefficiently utilize domain-specific knowledge and human expertise. In this\npaper, we introduce the Standard Operational Procedure-guided Agent\n(SOP-agent), a novel framework for constructing domain-specific agents through\npseudocode-style Standard Operational Procedures (SOPs) written in natural\nlanguage. Formally, we represent a SOP as a decision graph, which is traversed\nto guide the agent in completing tasks specified by the SOP. We conduct\nextensive experiments across tasks in multiple domains, including\ndecision-making, search and reasoning, code generation, data cleaning, and\ngrounded customer service. The SOP-agent demonstrates excellent versatility,\nachieving performance superior to general-purpose agent frameworks and\ncomparable to domain-specific agent systems. Additionally, we introduce the\nGrounded Customer Service Benchmark, the first benchmark designed to evaluate\nthe grounded decision-making capabilities of AI agents in customer service\nscenarios based on SOPs.",
    "comment": "35 pages, 5 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI']",
    "links": "['http://arxiv.org/abs/2501.09316v1' 'http://arxiv.org/pdf/2501.09316v1']",
    "pdf_url": "http://arxiv.org/pdf/2501.09316v1",
    "arxiv_id": "2501.09316",
    "row_id": 10067,
    "year": 2025,
    "processed_text": "SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs Despite significant advancements in general-purpose AI agents, several\nchallenges still hinder their practical application in real-world scenarios.\nFirst, the limited planning capabilities of Large Language Models (LLM)\nrestrict AI agents from effectively solving complex tasks that require\nlong-horizon planning. Second, general-purpose AI agents struggle to\nefficiently utilize domain-specific knowledge and human expertise. In this\npaper, we introduce the Standard Operational Procedure-guided Agent\n(SOP-agent), a novel framework for constructing domain-specific agents through\npseudocode-style Standard Operational Procedures (SOPs) written in natural\nlanguage. Formally, we represent a SOP as a decision graph, which is traversed\nto guide the agent in completing tasks specified by the SOP. We conduct\nextensive experiments across tasks in multiple domains, including\ndecision-making, search and reasoning, code generation, data cleaning, and\ngrounded customer service. The SOP-agent demonstrates excellent versatility,\nachieving performance superior to general-purpose agent frameworks and\ncomparable to domain-specific agent systems. Additionally, we introduce the\nGrounded Customer Service Benchmark, the first benchmark designed to evaluate\nthe grounded decision-making capabilities of AI agents in customer service\nscenarios based on SOPs.",
    "bm25_score": 10.337814722813487
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.11807v2",
    "updated": "2025-05-27T01:30:17+00:00",
    "published": "2025-05-17T03:28:24+00:00",
    "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic",
    "authors": "['Yufei Xiang' 'Yiqun Shen' 'Yeqin Zhang' 'Cam-Tu Nguyen']",
    "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines.",
    "comment": "17 pages, Published in EMNLP 2024 (Proceedings of the 2024 Conference\n  on Empirical Methods in Natural Language Processing)",
    "journal_ref": "Proceedings of the 2024 Conference on Empirical Methods in Natural\n  Language Processing, pages 4650-4666, ACL Anthology, 2024",
    "doi": "10.18653/v1/2024.emnlp-main.268",
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI']",
    "links": "['http://dx.doi.org/10.18653/v1/2024.emnlp-main.268'\n 'http://arxiv.org/abs/2505.11807v2' 'http://arxiv.org/pdf/2505.11807v2']",
    "pdf_url": "http://arxiv.org/pdf/2505.11807v2",
    "arxiv_id": "2505.11807",
    "row_id": 463196,
    "year": 2025,
    "processed_text": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines.",
    "bm25_score": 10.279503188448029
  },
  {
    "entry_id": "http://arxiv.org/abs/2501.13946v1",
    "updated": "2025-01-19T11:19:25+00:00",
    "published": "2025-01-19T11:19:25+00:00",
    "title": "Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks",
    "authors": "['Diego Gosmar' 'Deborah A. Dahl']",
    "summary": "Hallucinations remain a significant challenge in current Generative AI\nmodels, undermining trust in AI systems and their reliability. This study\ninvestigates how orchestrating multiple specialized Artificial Intelligent\nAgents can help mitigate such hallucinations, with a focus on systems\nleveraging Natural Language Processing (NLP) to facilitate seamless agent\ninteractions. To achieve this, we design a pipeline that introduces over three\nhundred prompts, purposefully crafted to induce hallucinations, into a\nfront-end agent. The outputs are then systematically reviewed and refined by\nsecond- and third-level agents, each employing distinct large language models\nand tailored strategies to detect unverified claims, incorporate explicit\ndisclaimers, and clarify speculative content. Additionally, we introduce a set\nof novel Key Performance Indicators (KPIs) specifically designed to evaluate\nhallucination score levels. A dedicated fourth-level AI agent is employed to\nevaluate these KPIs, providing detailed assessments and ensuring accurate\nquantification of shifts in hallucination-related behaviors. A core component\nof this investigation is the use of the OVON (Open Voice Network) framework,\nwhich relies on universal NLP-based interfaces to transfer contextual\ninformation among agents. Through structured JSON messages, each agent\ncommunicates its assessment of the hallucination likelihood and the reasons\nunderlying questionable content, thereby enabling the subsequent stage to\nrefine the text without losing context. The results demonstrate that employing\nmultiple specialized agents capable of interoperating with each other through\nNLP-based agentic frameworks can yield promising outcomes in hallucination\nmitigation, ultimately bolstering trust within the AI community.",
    "comment": "18 pages, 6 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI' 'cs.MA']",
    "links": "['http://arxiv.org/abs/2501.13946v1' 'http://arxiv.org/pdf/2501.13946v1']",
    "pdf_url": "http://arxiv.org/pdf/2501.13946v1",
    "arxiv_id": "2501.13946",
    "row_id": 56401,
    "year": 2025,
    "processed_text": "Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks Hallucinations remain a significant challenge in current Generative AI\nmodels, undermining trust in AI systems and their reliability. This study\ninvestigates how orchestrating multiple specialized Artificial Intelligent\nAgents can help mitigate such hallucinations, with a focus on systems\nleveraging Natural Language Processing (NLP) to facilitate seamless agent\ninteractions. To achieve this, we design a pipeline that introduces over three\nhundred prompts, purposefully crafted to induce hallucinations, into a\nfront-end agent. The outputs are then systematically reviewed and refined by\nsecond- and third-level agents, each employing distinct large language models\nand tailored strategies to detect unverified claims, incorporate explicit\ndisclaimers, and clarify speculative content. Additionally, we introduce a set\nof novel Key Performance Indicators (KPIs) specifically designed to evaluate\nhallucination score levels. A dedicated fourth-level AI agent is employed to\nevaluate these KPIs, providing detailed assessments and ensuring accurate\nquantification of shifts in hallucination-related behaviors. A core component\nof this investigation is the use of the OVON (Open Voice Network) framework,\nwhich relies on universal NLP-based interfaces to transfer contextual\ninformation among agents. Through structured JSON messages, each agent\ncommunicates its assessment of the hallucination likelihood and the reasons\nunderlying questionable content, thereby enabling the subsequent stage to\nrefine the text without losing context. The results demonstrate that employing\nmultiple specialized agents capable of interoperating with each other through\nNLP-based agentic frameworks can yield promising outcomes in hallucination\nmitigation, ultimately bolstering trust within the AI community.",
    "bm25_score": 10.249796725034319
  },
  {
    "entry_id": "http://arxiv.org/abs/2410.20007v1",
    "updated": "2024-10-25T23:32:48+00:00",
    "published": "2024-10-25T23:32:48+00:00",
    "title": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models",
    "authors": "['Danqing Wang' 'Zhuorui Ye' 'Fei Fang' 'Lei Li']",
    "summary": "Enhancing the reasoning capabilities of large language models (LLMs) is\ncrucial for enabling them to tackle complex, multi-step problems. Multi-agent\nframeworks have shown great potential in enhancing LLMs' reasoning\ncapabilities. However, the lack of effective cooperation between LLM agents\nhinders their performance, especially for multi-step reasoning tasks. This\npaper proposes a novel cooperative multi-agent reasoning framework (CoPlanner)\nby separating reasoning steps and assigning distinct duties to different\nagents. CoPlanner consists of two LLM agents: a planning agent and a reasoning\nagent. The planning agent provides high-level strategic hints, while the\nreasoning agent follows these hints and infers answers. By training the\nplanning agent's policy through the interactive reasoning process via Proximal\nPolicy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the\nprevious best method by 9.94\\% on LogiQA and 3.09\\% on BBH. Our results\ndemonstrate that the guidance from the planning agent and the effective\ncooperation between the agents contribute to the superior performance of\nCoPlanner in tackling multi-step reasoning problems.",
    "comment": "Working in progress",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL']",
    "links": "['http://arxiv.org/abs/2410.20007v1' 'http://arxiv.org/pdf/2410.20007v1']",
    "pdf_url": "http://arxiv.org/pdf/2410.20007v1",
    "arxiv_id": "2410.20007",
    "row_id": 7378,
    "year": 2024,
    "processed_text": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models Enhancing the reasoning capabilities of large language models (LLMs) is\ncrucial for enabling them to tackle complex, multi-step problems. Multi-agent\nframeworks have shown great potential in enhancing LLMs' reasoning\ncapabilities. However, the lack of effective cooperation between LLM agents\nhinders their performance, especially for multi-step reasoning tasks. This\npaper proposes a novel cooperative multi-agent reasoning framework (CoPlanner)\nby separating reasoning steps and assigning distinct duties to different\nagents. CoPlanner consists of two LLM agents: a planning agent and a reasoning\nagent. The planning agent provides high-level strategic hints, while the\nreasoning agent follows these hints and infers answers. By training the\nplanning agent's policy through the interactive reasoning process via Proximal\nPolicy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the\nprevious best method by 9.94\\% on LogiQA and 3.09\\% on BBH. Our results\ndemonstrate that the guidance from the planning agent and the effective\ncooperation between the agents contribute to the superior performance of\nCoPlanner in tackling multi-step reasoning problems.",
    "bm25_score": 10.190369555336375
  }
]