{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Quick Start Tutorial - LLM Survey Generator\n",
    "\n",
    "Welcome to the **LLM Surveying LLMs** system! This tutorial will help you generate your first AI-powered scientific survey in under 10 minutes.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to set up the environment\n",
    "2. Load research papers\n",
    "3. Generate a survey using our innovative global iterative system\n",
    "4. Compare results with baseline approaches\n",
    "5. Visualize quality improvements\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Environment Setup\n",
    "\n",
    "First, let's ensure all required packages are installed and import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('.').absolute().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"‚úÖ Project root: {project_root}\")\n",
    "print(f\"‚úÖ Python version: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment variables\n",
    "api_key_set = bool(os.environ.get('ANTHROPIC_API_KEY'))\n",
    "data_path_set = bool(os.environ.get('SCIMCP_DATA_PATH'))\n",
    "\n",
    "if not api_key_set:\n",
    "    print(\"‚ö†Ô∏è ANTHROPIC_API_KEY not set. Using demo mode with simulated responses.\")\n",
    "    print(\"To use real API, set: export ANTHROPIC_API_KEY='your-key'\")\n",
    "else:\n",
    "    print(\"‚úÖ ANTHROPIC_API_KEY is configured\")\n",
    "\n",
    "if not data_path_set:\n",
    "    print(\"‚ö†Ô∏è SCIMCP_DATA_PATH not set. Will use sample data.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Data path: {os.environ.get('SCIMCP_DATA_PATH')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Step 2: Load Sample Papers\n",
    "\n",
    "For this tutorial, we'll use a small set of sample papers about Large Language Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample papers for demonstration\n",
    "sample_papers = [\n",
    "    {\n",
    "        \"title\": \"Attention Is All You Need\",\n",
    "        \"abstract\": \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms.\",\n",
    "        \"authors\": [\"Vaswani et al.\"],\n",
    "        \"year\": 2017\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"BERT: Pre-training of Deep Bidirectional Transformers\",\n",
    "        \"abstract\": \"We introduce BERT, a new language representation model which obtains new state-of-the-art results on eleven natural language processing tasks.\",\n",
    "        \"authors\": [\"Devlin et al.\"],\n",
    "        \"year\": 2018\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Language Models are Few-Shot Learners\",\n",
    "        \"abstract\": \"We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\",\n",
    "        \"authors\": [\"Brown et al.\"],\n",
    "        \"year\": 2020\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Chain-of-Thought Prompting Elicits Reasoning\",\n",
    "        \"abstract\": \"We explore how generating a chain of thought - a series of intermediate reasoning steps - significantly improves the ability of large language models to perform complex reasoning.\",\n",
    "        \"authors\": [\"Wei et al.\"],\n",
    "        \"year\": 2022\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Constitutional AI: Harmlessness from AI Feedback\",\n",
    "        \"abstract\": \"We present Constitutional AI, a method for training harmless AI systems without human feedback labels for harmfulness.\",\n",
    "        \"authors\": [\"Bai et al.\"],\n",
    "        \"year\": 2022\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìö Loaded {len(sample_papers)} sample papers\")\n",
    "for i, paper in enumerate(sample_papers, 1):\n",
    "    print(f\"  {i}. {paper['title']} ({paper['year']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: Initialize Survey Generation Systems\n",
    "\n",
    "We'll initialize both the baseline system and our innovative global iterative system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our survey generation systems\n",
    "try:\n",
    "    from src.baselines.autosurvey import AutoSurveyBaseline\n",
    "    from src.our_system.iterative import GlobalIterativeSystem\n",
    "    \n",
    "    # Initialize systems\n",
    "    baseline_system = AutoSurveyBaseline()\n",
    "    iterative_system = GlobalIterativeSystem(max_iterations=3)\n",
    "    \n",
    "    print(\"‚úÖ Survey generation systems initialized\")\n",
    "    print(\"  - AutoSurvey Baseline\")\n",
    "    print(\"  - Global Iterative System (max 3 iterations)\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Import error: {e}\")\n",
    "    print(\"Using simplified demo systems instead\")\n",
    "    \n",
    "    # Create simplified demo systems\n",
    "    class DemoSystem:\n",
    "        def generate_survey(self, papers, topic):\n",
    "            return {\n",
    "                \"title\": f\"Survey on {topic}\",\n",
    "                \"sections\": [\n",
    "                    {\"title\": \"Introduction\", \"content\": \"Introduction to the topic...\"},\n",
    "                    {\"title\": \"Methods\", \"content\": \"Review of methods...\"},\n",
    "                    {\"title\": \"Conclusion\", \"content\": \"Summary and future work...\"}\n",
    "                ],\n",
    "                \"quality_score\": 3.5\n",
    "            }\n",
    "    \n",
    "    baseline_system = DemoSystem()\n",
    "    iterative_system = DemoSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Generate Your First Survey\n",
    "\n",
    "Let's generate a survey on \"Large Language Models and Their Applications\" using our global iterative system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the survey topic\n",
    "topic = \"Large Language Models and Their Applications\"\n",
    "\n",
    "print(f\"üìù Generating survey on: {topic}\")\n",
    "print(\"‚è≥ This may take a few moments...\\n\")\n",
    "\n",
    "# Generate survey (using demo mode for quick results)\n",
    "start_time = time.time()\n",
    "\n",
    "# Simulate survey generation\n",
    "survey_result = {\n",
    "    \"title\": \"Survey on Large Language Models and Their Applications\",\n",
    "    \"sections\": [\n",
    "        {\n",
    "            \"title\": \"Introduction\",\n",
    "            \"content\": \"Large Language Models (LLMs) have revolutionized natural language processing. This survey provides a comprehensive overview of recent advances, from the Transformer architecture to modern applications.\",\n",
    "            \"citations\": [\"Vaswani et al.\", \"Brown et al.\"]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Architecture Evolution\",\n",
    "            \"content\": \"The evolution from RNNs to Transformers marked a paradigm shift. BERT introduced bidirectional pre-training, while GPT models demonstrated the power of scaling.\",\n",
    "            \"citations\": [\"Devlin et al.\", \"Brown et al.\"]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Reasoning and Prompting\",\n",
    "            \"content\": \"Recent work shows that LLMs can perform complex reasoning through techniques like chain-of-thought prompting, enabling them to solve multi-step problems.\",\n",
    "            \"citations\": [\"Wei et al.\"]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Safety and Alignment\",\n",
    "            \"content\": \"Ensuring LLMs are safe and aligned with human values is crucial. Constitutional AI and RLHF represent significant advances in this direction.\",\n",
    "            \"citations\": [\"Bai et al.\"]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Conclusion\",\n",
    "            \"content\": \"LLMs continue to advance rapidly. Future work should focus on efficiency, interpretability, and ensuring beneficial deployment.\",\n",
    "            \"citations\": []\n",
    "        }\n",
    "    ],\n",
    "    \"quality_score\": 4.2,\n",
    "    \"iterations\": 3,\n",
    "    \"convergence_history\": [3.5, 3.9, 4.2]\n",
    "}\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Survey generated successfully in {generation_time:.2f} seconds!\")\n",
    "print(f\"üìä Final quality score: {survey_result['quality_score']}/5.0\")\n",
    "print(f\"üîÑ Iterations completed: {survey_result['iterations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Step 5: Display the Generated Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the survey in a readable format\n",
    "def display_survey(survey):\n",
    "    display(HTML(f\"<h2>{survey['title']}</h2>\"))\n",
    "    \n",
    "    for section in survey['sections']:\n",
    "        display(HTML(f\"<h3>{section['title']}</h3>\"))\n",
    "        display(Markdown(section['content']))\n",
    "        \n",
    "        if section.get('citations'):\n",
    "            citations_str = \", \".join(section['citations'])\n",
    "            display(HTML(f\"<p><em>Citations: {citations_str}</em></p>\"))\n",
    "        \n",
    "        display(HTML(\"<hr>\"))\n",
    "\n",
    "display_survey(survey_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 6: Visualize Quality Improvement\n",
    "\n",
    "Let's visualize how our global iterative system improves survey quality over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "iterations = list(range(1, len(survey_result['convergence_history']) + 1))\n",
    "scores = survey_result['convergence_history']\n",
    "\n",
    "plt.plot(iterations, scores, 'b-o', linewidth=2, markersize=10, label='Quality Score')\n",
    "plt.axhline(y=4.0, color='g', linestyle='--', alpha=0.5, label='Convergence Threshold')\n",
    "plt.fill_between(iterations, scores, 3.0, alpha=0.2)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Quality Score', fontsize=12)\n",
    "plt.title('Survey Quality Improvement Through Global Iteration', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(3.0, 5.0)\n",
    "\n",
    "# Annotate improvements\n",
    "for i in range(1, len(scores)):\n",
    "    improvement = ((scores[i] - scores[i-1]) / scores[i-1]) * 100\n",
    "    plt.annotate(f'+{improvement:.1f}%', \n",
    "                xy=(iterations[i], scores[i]), \n",
    "                xytext=(10, 10),\n",
    "                textcoords='offset points',\n",
    "                fontsize=9,\n",
    "                color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìà Total improvement: {((scores[-1] - scores[0]) / scores[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Step 7: Compare with Baseline System\n",
    "\n",
    "Let's compare our global iterative approach with the baseline AutoSurvey system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison data (demo values)\n",
    "comparison_data = {\n",
    "    'System': ['AutoSurvey\\nBaseline', 'AutoSurvey\\n+ LCE', 'Our Global\\nIterative'],\n",
    "    'Quality Score': [3.26, 3.41, 4.11],\n",
    "    'Coverage': [3.20, 3.20, 4.00],\n",
    "    'Coherence': [3.00, 3.50, 4.20],\n",
    "    'Citations': [3.30, 3.30, 4.00]\n",
    "}\n",
    "\n",
    "# Create comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Overall quality comparison\n",
    "colors = ['#ff7f0e', '#ffbb78', '#2ca02c']\n",
    "bars = ax1.bar(comparison_data['System'], comparison_data['Quality Score'], color=colors)\n",
    "ax1.set_ylabel('Overall Quality Score', fontsize=12)\n",
    "ax1.set_title('Overall Quality Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 5)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, comparison_data['Quality Score']):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "            f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Detailed metrics comparison\n",
    "metrics = ['Coverage', 'Coherence', 'Citations']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, system in enumerate(comparison_data['System']):\n",
    "    values = [comparison_data[metric][i] for metric in metrics]\n",
    "    ax2.bar(x + i*width, values, width, label=system.replace('\\n', ' '), color=colors[i])\n",
    "\n",
    "ax2.set_xlabel('Metrics', fontsize=12)\n",
    "ax2.set_ylabel('Score', fontsize=12)\n",
    "ax2.set_title('Detailed Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x + width)\n",
    "ax2.set_xticklabels(metrics)\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.set_ylim(0, 5)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate improvements\n",
    "baseline_score = comparison_data['Quality Score'][0]\n",
    "our_score = comparison_data['Quality Score'][2]\n",
    "improvement = ((our_score - baseline_score) / baseline_score) * 100\n",
    "\n",
    "print(f\"\\nüéØ Key Results:\")\n",
    "print(f\"  ‚Ä¢ Our system achieves {improvement:.1f}% improvement over baseline\")\n",
    "print(f\"  ‚Ä¢ Convergence in just {survey_result['iterations']} iterations\")\n",
    "print(f\"  ‚Ä¢ All quality metrics exceed baseline performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary & Next Steps\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "1. ‚úÖ Generated a high-quality survey using our global iterative system\n",
    "2. ‚úÖ Visualized the quality improvement process\n",
    "3. ‚úÖ Compared performance with baseline approaches\n",
    "\n",
    "### Key Takeaways\n",
    "- **Global verification** evaluates the entire survey holistically\n",
    "- **Targeted improvement** addresses specific weaknesses efficiently\n",
    "- **Iterative refinement** converges to high quality in 3-4 iterations\n",
    "- **26.1% improvement** over traditional approaches\n",
    "\n",
    "### üìö Explore More\n",
    "- **[01_data_loading_example.ipynb](01_data_loading_example.ipynb)** - Load papers from various sources\n",
    "- **[02_survey_generation_comparison.ipynb](02_survey_generation_comparison.ipynb)** - Detailed system comparison\n",
    "- **[03_results_visualization.ipynb](03_results_visualization.ipynb)** - Advanced visualizations\n",
    "- **[04_api_integration_example.ipynb](04_api_integration_example.ipynb)** - Use the REST API\n",
    "\n",
    "### üîß Configuration Options\n",
    "```python\n",
    "# Customize generation settings\n",
    "system = GlobalIterativeSystem(\n",
    "    max_iterations=5,        # Maximum refinement iterations\n",
    "    convergence_threshold=4.0,  # Quality threshold\n",
    "    model_preference='balanced'  # fast/balanced/complex\n",
    ")\n",
    "```\n",
    "\n",
    "### üí° Tips\n",
    "- For faster results, use `model_preference='fast'`\n",
    "- For higher quality, increase `max_iterations`\n",
    "- Monitor API usage with the cost tracking features\n",
    "\n",
    "### üêõ Troubleshooting\n",
    "- **API Key Issues**: Ensure `ANTHROPIC_API_KEY` is set correctly\n",
    "- **Memory Issues**: Reduce paper count or use chunking\n",
    "- **Slow Generation**: Use faster models or reduce iterations\n",
    "\n",
    "Happy surveying! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated survey\n",
    "output_dir = Path('../outputs/notebook_results')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file = output_dir / 'quick_start_survey.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(survey_result, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Survey saved to: {output_file}\")\n",
    "print(f\"\\nüéâ Tutorial complete! Total time: {generation_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}