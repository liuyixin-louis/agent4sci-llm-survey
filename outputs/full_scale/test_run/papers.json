[
  {
    "entry_id": "http://arxiv.org/abs/2502.01390v1",
    "updated": "2025-02-03T14:23:22+00:00",
    "published": "2025-02-03T14:23:22+00:00",
    "title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant",
    "authors": "['Gaole He' 'Gianluca Demartini' 'Ujwal Gadiraju']",
    "summary": "Since the explosion in popularity of ChatGPT, large language models (LLMs)\nhave continued to impact our everyday lives. Equipped with external tools that\nare designed for a specific purpose (e.g., for flight booking or an alarm\nclock), LLM agents exercise an increasing capability to assist humans in their\ndaily work. Although LLM agents have shown a promising blueprint as daily\nassistants, there is a limited understanding of how they can provide daily\nassistance based on planning and sequential decision making capabilities. We\ndraw inspiration from recent work that has highlighted the value of\n'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks.\nWe conducted an empirical study (N = 248) of LLM agents as daily assistants in\nsix commonly occurring tasks with different levels of risk typically associated\nwith them (e.g., flight ticket booking and credit card payments). To ensure\nuser agency and control over the LLM agent, we adopted LLM agents in a\nplan-then-execute manner, wherein the agents conducted step-wise planning and\nstep-by-step execution in a simulation environment. We analyzed how user\ninvolvement at each stage affects their trust and collaborative team\nperformance. Our findings demonstrate that LLM agents can be a double-edged\nsword -- (1) they can work well when a high-quality plan and necessary user\ninvolvement in execution are available, and (2) users can easily mistrust the\nLLM agents with plans that seem plausible. We synthesized key insights for\nusing LLM agents as daily assistants to calibrate user trust and achieve better\noverall task outcomes. Our work has important implications for the future\ndesign of daily assistants and human-AI collaboration with LLM agents.",
    "comment": "conditionally accepted to CHI 2025",
    "journal_ref": null,
    "doi": "10.1145/3706598.3713218",
    "primary_category": "cs.HC",
    "categories": "['cs.HC' 'cs.CL']",
    "links": "['http://dx.doi.org/10.1145/3706598.3713218'\n 'http://arxiv.org/abs/2502.01390v1' 'http://arxiv.org/pdf/2502.01390v1']",
    "pdf_url": "http://arxiv.org/pdf/2502.01390v1",
    "arxiv_id": "2502.01390",
    "row_id": 230264,
    "year": 2025,
    "processed_text": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant Since the explosion in popularity of ChatGPT, large language models (LLMs)\nhave continued to impact our everyday lives. Equipped with external tools that\nare designed for a specific purpose (e.g., for flight booking or an alarm\nclock), LLM agents exercise an increasing capability to assist humans in their\ndaily work. Although LLM agents have shown a promising blueprint as daily\nassistants, there is a limited understanding of how they can provide daily\nassistance based on planning and sequential decision making capabilities. We\ndraw inspiration from recent work that has highlighted the value of\n'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks.\nWe conducted an empirical study (N = 248) of LLM agents as daily assistants in\nsix commonly occurring tasks with different levels of risk typically associated\nwith them (e.g., flight ticket booking and credit card payments). To ensure\nuser agency and control over the LLM agent, we adopted LLM agents in a\nplan-then-execute manner, wherein the agents conducted step-wise planning and\nstep-by-step execution in a simulation environment. We analyzed how user\ninvolvement at each stage affects their trust and collaborative team\nperformance. Our findings demonstrate that LLM agents can be a double-edged\nsword -- (1) they can work well when a high-quality plan and necessary user\ninvolvement in execution are available, and (2) users can easily mistrust the\nLLM agents with plans that seem plausible. We synthesized key insights for\nusing LLM agents as daily assistants to calibrate user trust and achieve better\noverall task outcomes. Our work has important implications for the future\ndesign of daily assistants and human-AI collaboration with LLM agents.",
    "bm25_score": 10.851289472377129
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04559v4",
    "updated": "2024-11-01T16:10:41+00:00",
    "published": "2024-02-07T03:37:19+00:00",
    "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
    "authors": "['Chengxing Xie' 'Canyu Chen' 'Feiran Jia' 'Ziyu Ye' 'Shiyang Lai'\n 'Kai Shu' 'Jindong Gu' 'Adel Bibi' 'Ziniu Hu' 'David Jurgens'\n 'James Evans' 'Philip Torr' 'Bernard Ghanem' 'Guohao Li']",
    "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
    "comment": "Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.HC']",
    "links": "['http://arxiv.org/abs/2402.04559v4' 'http://arxiv.org/pdf/2402.04559v4']",
    "pdf_url": "http://arxiv.org/pdf/2402.04559v4",
    "arxiv_id": "2402.04559",
    "row_id": 2020,
    "year": 2024,
    "processed_text": "Can Large Language Model Agents Simulate Human Trust Behavior? Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
    "bm25_score": 10.84366068597188
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.01637v1",
    "updated": "2024-06-02T16:25:26+00:00",
    "published": "2024-06-02T16:25:26+00:00",
    "title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities",
    "authors": "['Richard Fang' 'Rohan Bindu' 'Akul Gupta' 'Qiusi Zhan' 'Daniel Kang']",
    "summary": "LLM agents have become increasingly sophisticated, especially in the realm of\ncybersecurity. Researchers have shown that LLM agents can exploit real-world\nvulnerabilities when given a description of the vulnerability and toy\ncapture-the-flag problems. However, these agents still perform poorly on\nreal-world vulnerabilities that are unknown to the agent ahead of time\n(zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world,\nzero-day vulnerabilities. Prior agents struggle with exploring many different\nvulnerabilities and long-range planning when used alone. To resolve this, we\nintroduce HPTSA, a system of agents with a planning agent that can launch\nsubagents. The planning agent explores the system and determines which\nsubagents to call, resolving long-term planning issues when trying different\nvulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and\nshow that our team of agents improve over prior work by up to 4.5$\\times$.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": "['cs.MA' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2406.01637v1' 'http://arxiv.org/pdf/2406.01637v1']",
    "pdf_url": "http://arxiv.org/pdf/2406.01637v1",
    "arxiv_id": "2406.01637",
    "row_id": 322366,
    "year": 2024,
    "processed_text": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities LLM agents have become increasingly sophisticated, especially in the realm of\ncybersecurity. Researchers have shown that LLM agents can exploit real-world\nvulnerabilities when given a description of the vulnerability and toy\ncapture-the-flag problems. However, these agents still perform poorly on\nreal-world vulnerabilities that are unknown to the agent ahead of time\n(zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world,\nzero-day vulnerabilities. Prior agents struggle with exploring many different\nvulnerabilities and long-range planning when used alone. To resolve this, we\nintroduce HPTSA, a system of agents with a planning agent that can launch\nsubagents. The planning agent explores the system and determines which\nsubagents to call, resolving long-term planning issues when trying different\nvulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and\nshow that our team of agents improve over prior work by up to 4.5$\\times$.",
    "bm25_score": 10.8220419454651
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.19213v1",
    "updated": "2025-03-24T23:39:44+00:00",
    "published": "2025-03-24T23:39:44+00:00",
    "title": "A Survey of Large Language Model Agents for Question Answering",
    "authors": "['Murong Yue']",
    "summary": "This paper surveys the development of large language model (LLM)-based agents\nfor question answering (QA). Traditional agents face significant limitations,\nincluding substantial data requirements and difficulty in generalizing to new\nenvironments. LLM-based agents address these challenges by leveraging LLMs as\ntheir core reasoning engine. These agents achieve superior QA results compared\nto traditional QA pipelines and naive LLM QA systems by enabling interaction\nwith external environments. We systematically review the design of LLM agents\nin the context of QA tasks, organizing our discussion across key stages:\nplanning, question understanding, information retrieval, and answer generation.\nAdditionally, this paper identifies ongoing challenges and explores future\nresearch directions to enhance the performance of LLM agent QA systems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI' 'cs.HC']",
    "links": "['http://arxiv.org/abs/2503.19213v1' 'http://arxiv.org/pdf/2503.19213v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.19213v1",
    "arxiv_id": "2503.19213",
    "row_id": 57360,
    "year": 2025,
    "processed_text": "A Survey of Large Language Model Agents for Question Answering This paper surveys the development of large language model (LLM)-based agents\nfor question answering (QA). Traditional agents face significant limitations,\nincluding substantial data requirements and difficulty in generalizing to new\nenvironments. LLM-based agents address these challenges by leveraging LLMs as\ntheir core reasoning engine. These agents achieve superior QA results compared\nto traditional QA pipelines and naive LLM QA systems by enabling interaction\nwith external environments. We systematically review the design of LLM agents\nin the context of QA tasks, organizing our discussion across key stages:\nplanning, question understanding, information retrieval, and answer generation.\nAdditionally, this paper identifies ongoing challenges and explores future\nresearch directions to enhance the performance of LLM agent QA systems.",
    "bm25_score": 10.509595567029404
  },
  {
    "entry_id": "http://arxiv.org/abs/2403.12482v2",
    "updated": "2024-05-23T06:29:00+00:00",
    "published": "2024-03-19T06:39:47+00:00",
    "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
    "authors": "['Xudong Guo' 'Kaixuan Huang' 'Jiale Liu' 'Wenhui Fan' 'Natalia V\u00e9lez'\n 'Qingyun Wu' 'Huazheng Wang' 'Thomas L. Griffiths' 'Mengdi Wang']",
    "summary": "Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.CY' 'cs.MA']",
    "links": "['http://arxiv.org/abs/2403.12482v2' 'http://arxiv.org/pdf/2403.12482v2']",
    "pdf_url": "http://arxiv.org/pdf/2403.12482v2",
    "arxiv_id": "2403.12482",
    "row_id": 1490,
    "year": 2024,
    "processed_text": "Embodied LLM Agents Learn to Cooperate in Organized Teams Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
    "bm25_score": 10.465627248418183
  }
]