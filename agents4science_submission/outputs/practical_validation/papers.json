[
  {
    "entry_id": "http://arxiv.org/abs/2502.01390v1",
    "updated": "2025-02-03T14:23:22+00:00",
    "published": "2025-02-03T14:23:22+00:00",
    "title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant",
    "authors": "['Gaole He' 'Gianluca Demartini' 'Ujwal Gadiraju']",
    "summary": "Since the explosion in popularity of ChatGPT, large language models (LLMs)\nhave continued to impact our everyday lives. Equipped with external tools that\nare designed for a specific purpose (e.g., for flight booking or an alarm\nclock), LLM agents exercise an increasing capability to assist humans in their\ndaily work. Although LLM agents have shown a promising blueprint as daily\nassistants, there is a limited understanding of how they can provide daily\nassistance based on planning and sequential decision making capabilities. We\ndraw inspiration from recent work that has highlighted the value of\n'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks.\nWe conducted an empirical study (N = 248) of LLM agents as daily assistants in\nsix commonly occurring tasks with different levels of risk typically associated\nwith them (e.g., flight ticket booking and credit card payments). To ensure\nuser agency and control over the LLM agent, we adopted LLM agents in a\nplan-then-execute manner, wherein the agents conducted step-wise planning and\nstep-by-step execution in a simulation environment. We analyzed how user\ninvolvement at each stage affects their trust and collaborative team\nperformance. Our findings demonstrate that LLM agents can be a double-edged\nsword -- (1) they can work well when a high-quality plan and necessary user\ninvolvement in execution are available, and (2) users can easily mistrust the\nLLM agents with plans that seem plausible. We synthesized key insights for\nusing LLM agents as daily assistants to calibrate user trust and achieve better\noverall task outcomes. Our work has important implications for the future\ndesign of daily assistants and human-AI collaboration with LLM agents.",
    "comment": "conditionally accepted to CHI 2025",
    "journal_ref": null,
    "doi": "10.1145/3706598.3713218",
    "primary_category": "cs.HC",
    "categories": "['cs.HC' 'cs.CL']",
    "links": "['http://dx.doi.org/10.1145/3706598.3713218'\n 'http://arxiv.org/abs/2502.01390v1' 'http://arxiv.org/pdf/2502.01390v1']",
    "pdf_url": "http://arxiv.org/pdf/2502.01390v1",
    "arxiv_id": "2502.01390",
    "row_id": 230264,
    "year": 2025,
    "processed_text": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant Since the explosion in popularity of ChatGPT, large language models (LLMs)\nhave continued to impact our everyday lives. Equipped with external tools that\nare designed for a specific purpose (e.g., for flight booking or an alarm\nclock), LLM agents exercise an increasing capability to assist humans in their\ndaily work. Although LLM agents have shown a promising blueprint as daily\nassistants, there is a limited understanding of how they can provide daily\nassistance based on planning and sequential decision making capabilities. We\ndraw inspiration from recent work that has highlighted the value of\n'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks.\nWe conducted an empirical study (N = 248) of LLM agents as daily assistants in\nsix commonly occurring tasks with different levels of risk typically associated\nwith them (e.g., flight ticket booking and credit card payments). To ensure\nuser agency and control over the LLM agent, we adopted LLM agents in a\nplan-then-execute manner, wherein the agents conducted step-wise planning and\nstep-by-step execution in a simulation environment. We analyzed how user\ninvolvement at each stage affects their trust and collaborative team\nperformance. Our findings demonstrate that LLM agents can be a double-edged\nsword -- (1) they can work well when a high-quality plan and necessary user\ninvolvement in execution are available, and (2) users can easily mistrust the\nLLM agents with plans that seem plausible. We synthesized key insights for\nusing LLM agents as daily assistants to calibrate user trust and achieve better\noverall task outcomes. Our work has important implications for the future\ndesign of daily assistants and human-AI collaboration with LLM agents.",
    "bm25_score": 10.851289472377129
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04559v4",
    "updated": "2024-11-01T16:10:41+00:00",
    "published": "2024-02-07T03:37:19+00:00",
    "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
    "authors": "['Chengxing Xie' 'Canyu Chen' 'Feiran Jia' 'Ziyu Ye' 'Shiyang Lai'\n 'Kai Shu' 'Jindong Gu' 'Adel Bibi' 'Ziniu Hu' 'David Jurgens'\n 'James Evans' 'Philip Torr' 'Bernard Ghanem' 'Guohao Li']",
    "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
    "comment": "Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.HC']",
    "links": "['http://arxiv.org/abs/2402.04559v4' 'http://arxiv.org/pdf/2402.04559v4']",
    "pdf_url": "http://arxiv.org/pdf/2402.04559v4",
    "arxiv_id": "2402.04559",
    "row_id": 2020,
    "year": 2024,
    "processed_text": "Can Large Language Model Agents Simulate Human Trust Behavior? Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
    "bm25_score": 10.84366068597188
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.01637v1",
    "updated": "2024-06-02T16:25:26+00:00",
    "published": "2024-06-02T16:25:26+00:00",
    "title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities",
    "authors": "['Richard Fang' 'Rohan Bindu' 'Akul Gupta' 'Qiusi Zhan' 'Daniel Kang']",
    "summary": "LLM agents have become increasingly sophisticated, especially in the realm of\ncybersecurity. Researchers have shown that LLM agents can exploit real-world\nvulnerabilities when given a description of the vulnerability and toy\ncapture-the-flag problems. However, these agents still perform poorly on\nreal-world vulnerabilities that are unknown to the agent ahead of time\n(zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world,\nzero-day vulnerabilities. Prior agents struggle with exploring many different\nvulnerabilities and long-range planning when used alone. To resolve this, we\nintroduce HPTSA, a system of agents with a planning agent that can launch\nsubagents. The planning agent explores the system and determines which\nsubagents to call, resolving long-term planning issues when trying different\nvulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and\nshow that our team of agents improve over prior work by up to 4.5$\\times$.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": "['cs.MA' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2406.01637v1' 'http://arxiv.org/pdf/2406.01637v1']",
    "pdf_url": "http://arxiv.org/pdf/2406.01637v1",
    "arxiv_id": "2406.01637",
    "row_id": 322366,
    "year": 2024,
    "processed_text": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities LLM agents have become increasingly sophisticated, especially in the realm of\ncybersecurity. Researchers have shown that LLM agents can exploit real-world\nvulnerabilities when given a description of the vulnerability and toy\ncapture-the-flag problems. However, these agents still perform poorly on\nreal-world vulnerabilities that are unknown to the agent ahead of time\n(zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world,\nzero-day vulnerabilities. Prior agents struggle with exploring many different\nvulnerabilities and long-range planning when used alone. To resolve this, we\nintroduce HPTSA, a system of agents with a planning agent that can launch\nsubagents. The planning agent explores the system and determines which\nsubagents to call, resolving long-term planning issues when trying different\nvulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and\nshow that our team of agents improve over prior work by up to 4.5$\\times$.",
    "bm25_score": 10.8220419454651
  },
  {
    "entry_id": "http://arxiv.org/abs/2503.19213v1",
    "updated": "2025-03-24T23:39:44+00:00",
    "published": "2025-03-24T23:39:44+00:00",
    "title": "A Survey of Large Language Model Agents for Question Answering",
    "authors": "['Murong Yue']",
    "summary": "This paper surveys the development of large language model (LLM)-based agents\nfor question answering (QA). Traditional agents face significant limitations,\nincluding substantial data requirements and difficulty in generalizing to new\nenvironments. LLM-based agents address these challenges by leveraging LLMs as\ntheir core reasoning engine. These agents achieve superior QA results compared\nto traditional QA pipelines and naive LLM QA systems by enabling interaction\nwith external environments. We systematically review the design of LLM agents\nin the context of QA tasks, organizing our discussion across key stages:\nplanning, question understanding, information retrieval, and answer generation.\nAdditionally, this paper identifies ongoing challenges and explores future\nresearch directions to enhance the performance of LLM agent QA systems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI' 'cs.HC']",
    "links": "['http://arxiv.org/abs/2503.19213v1' 'http://arxiv.org/pdf/2503.19213v1']",
    "pdf_url": "http://arxiv.org/pdf/2503.19213v1",
    "arxiv_id": "2503.19213",
    "row_id": 57360,
    "year": 2025,
    "processed_text": "A Survey of Large Language Model Agents for Question Answering This paper surveys the development of large language model (LLM)-based agents\nfor question answering (QA). Traditional agents face significant limitations,\nincluding substantial data requirements and difficulty in generalizing to new\nenvironments. LLM-based agents address these challenges by leveraging LLMs as\ntheir core reasoning engine. These agents achieve superior QA results compared\nto traditional QA pipelines and naive LLM QA systems by enabling interaction\nwith external environments. We systematically review the design of LLM agents\nin the context of QA tasks, organizing our discussion across key stages:\nplanning, question understanding, information retrieval, and answer generation.\nAdditionally, this paper identifies ongoing challenges and explores future\nresearch directions to enhance the performance of LLM agent QA systems.",
    "bm25_score": 10.509595567029404
  },
  {
    "entry_id": "http://arxiv.org/abs/2403.12482v2",
    "updated": "2024-05-23T06:29:00+00:00",
    "published": "2024-03-19T06:39:47+00:00",
    "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
    "authors": "['Xudong Guo' 'Kaixuan Huang' 'Jiale Liu' 'Wenhui Fan' 'Natalia V\u00e9lez'\n 'Qingyun Wu' 'Huazheng Wang' 'Thomas L. Griffiths' 'Mengdi Wang']",
    "summary": "Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.CY' 'cs.MA']",
    "links": "['http://arxiv.org/abs/2403.12482v2' 'http://arxiv.org/pdf/2403.12482v2']",
    "pdf_url": "http://arxiv.org/pdf/2403.12482v2",
    "arxiv_id": "2403.12482",
    "row_id": 1490,
    "year": 2024,
    "processed_text": "Embodied LLM Agents Learn to Cooperate in Organized Teams Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
    "bm25_score": 10.465627248418183
  },
  {
    "entry_id": "http://arxiv.org/abs/2409.10372v3",
    "updated": "2024-10-30T16:45:15+00:00",
    "published": "2024-09-16T15:15:51+00:00",
    "title": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation",
    "authors": "['Qiliang Chen' 'Sepehr Ilami' 'Nunzio Lore' 'Babak Heydari']",
    "summary": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.CY' 'cs.GT']",
    "links": "['http://arxiv.org/abs/2409.10372v3' 'http://arxiv.org/pdf/2409.10372v3']",
    "pdf_url": "http://arxiv.org/pdf/2409.10372v3",
    "arxiv_id": "2409.10372",
    "row_id": 8152,
    "year": 2024,
    "processed_text": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
    "bm25_score": 10.429110429674427
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06596v1",
    "updated": "2024-02-09T18:19:25+00:00",
    "published": "2024-02-09T18:19:25+00:00",
    "title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment",
    "authors": "['Mingzhe Xing' 'Rongkai Zhang' 'Hui Xue' 'Qi Chen' 'Fan Yang' 'Zhen Xiao']",
    "summary": "Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.HC' 'cs.SE']",
    "links": "['http://arxiv.org/abs/2402.06596v1' 'http://arxiv.org/pdf/2402.06596v1']",
    "pdf_url": "http://arxiv.org/pdf/2402.06596v1",
    "arxiv_id": "2402.06596",
    "row_id": 1976,
    "year": 2024,
    "processed_text": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.",
    "bm25_score": 10.423962205197373
  },
  {
    "entry_id": "http://arxiv.org/abs/2406.03007v1",
    "updated": "2024-06-05T07:14:28+00:00",
    "published": "2024-06-05T07:14:28+00:00",
    "title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents",
    "authors": "['Yifei Wang' 'Dizhan Xue' 'Shengjie Zhang' 'Shengsheng Qian']",
    "summary": "With the prosperity of large language models (LLMs), powerful LLM-based\nintelligent agents have been developed to provide customized services with a\nset of user-defined tools. State-of-the-art methods for constructing LLM agents\nadopt trained LLMs and further fine-tune them on data for the agent task.\nHowever, we show that such methods are vulnerable to our proposed backdoor\nattacks named BadAgent on various agent tasks, where a backdoor can be embedded\nby fine-tuning on the backdoor data. At test time, the attacker can manipulate\nthe deployed LLM agents to execute harmful operations by showing the trigger in\nthe agent input or environment. To our surprise, our proposed attack methods\nare extremely robust even after fine-tuning on trustworthy data. Though\nbackdoor attacks have been studied extensively in natural language processing,\nto the best of our knowledge, we could be the first to study them on LLM agents\nthat are more dangerous due to the permission to use external tools. Our work\ndemonstrates the clear risk of constructing LLM agents based on untrusted LLMs\nor data. Our code is public at https://github.com/DPamK/BadAgent",
    "comment": "Accepted by ACL 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": "['cs.CL' 'cs.AI' 'cs.CR' 'cs.LG']",
    "links": "['http://arxiv.org/abs/2406.03007v1' 'http://arxiv.org/pdf/2406.03007v1']",
    "pdf_url": "http://arxiv.org/pdf/2406.03007v1",
    "arxiv_id": "2406.03007",
    "row_id": 32069,
    "year": 2024,
    "processed_text": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents With the prosperity of large language models (LLMs), powerful LLM-based\nintelligent agents have been developed to provide customized services with a\nset of user-defined tools. State-of-the-art methods for constructing LLM agents\nadopt trained LLMs and further fine-tune them on data for the agent task.\nHowever, we show that such methods are vulnerable to our proposed backdoor\nattacks named BadAgent on various agent tasks, where a backdoor can be embedded\nby fine-tuning on the backdoor data. At test time, the attacker can manipulate\nthe deployed LLM agents to execute harmful operations by showing the trigger in\nthe agent input or environment. To our surprise, our proposed attack methods\nare extremely robust even after fine-tuning on trustworthy data. Though\nbackdoor attacks have been studied extensively in natural language processing,\nto the best of our knowledge, we could be the first to study them on LLM agents\nthat are more dangerous due to the permission to use external tools. Our work\ndemonstrates the clear risk of constructing LLM agents based on untrusted LLMs\nor data. Our code is public at https://github.com/DPamK/BadAgent",
    "bm25_score": 10.423872763811385
  },
  {
    "entry_id": "http://arxiv.org/abs/2410.12481v1",
    "updated": "2024-10-16T11:59:27+00:00",
    "published": "2024-10-16T11:59:27+00:00",
    "title": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling",
    "authors": "['Loris Gaven' 'Clement Romac' 'Thomas Carta' 'Sylvain Lamprier'\n 'Olivier Sigaud' 'Pierre-Yves Oudeyer']",
    "summary": "The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": "['cs.LG' 'cs.AI']",
    "links": "['http://arxiv.org/abs/2410.12481v1' 'http://arxiv.org/pdf/2410.12481v1']",
    "pdf_url": "http://arxiv.org/pdf/2410.12481v1",
    "arxiv_id": "2410.12481",
    "row_id": 294884,
    "year": 2024,
    "processed_text": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments.",
    "bm25_score": 10.365938331371575
  },
  {
    "entry_id": "http://arxiv.org/abs/2505.03961v1",
    "updated": "2025-05-06T20:23:25+00:00",
    "published": "2025-05-06T20:23:25+00:00",
    "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete",
    "authors": "['Gerrit Gro\u00dfmann' 'Larisa Ivanova' 'Sai Leela Poduru'\n 'Mohaddeseh Tabrizian' 'Islam Mesabah' 'David A. Selby'\n 'Sebastian J. Vollmer']",
    "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment.",
    "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": "['cs.AI' 'cs.CL' 'cs.MA' 'I.2.11; I.2.7; I.6; J.4']",
    "links": "['http://arxiv.org/abs/2505.03961v1' 'http://arxiv.org/pdf/2505.03961v1']",
    "pdf_url": "http://arxiv.org/pdf/2505.03961v1",
    "arxiv_id": "2505.03961",
    "row_id": 11854,
    "year": 2025,
    "processed_text": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment.",
    "bm25_score": 10.31770003957593
  }
]